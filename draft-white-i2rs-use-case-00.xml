<?xml version="1.0" encoding="us-ascii"?>

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
<!-- One method to get references from the online citation libraries.
     There has to be one entity for each item to be referenced. 
     An alternate method (rfc include) is described in the references. -->
     
<!ENTITY RFC2119 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml">
<!ENTITY RFC3746 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3746.xml">
<!ENTITY I-D.atlas-irs-problem-statement SYSTEM "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.atlas-irs-problem-statement.xml">
<!ENTITY I-D.ward-irs-framework SYSTEM "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.ward-irs-framework.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<?rfc strict="no" ?>
<?rfc toc="yes" ?>
<?rfc symrefs="yes" ?>
<?rfc sortrefs="yes"?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>
<rfc category="info" docName="draft-white-i2rs-use-case-00" ipr="trust200902">
  
  <front>
    <title abbrev="IRS Use Cases">Use Cases for an Interface to the Routing System</title>
    <author fullname="Russ White" initials="R" surname="White">
      <organization>Verisign</organization>
      <address>
        <postal>
          <street>12061 Bluemont Way</street>
          <!-- Reorder these if your country does things differently -->
          <city>Reston</city>
          <region>VA</region>
          <code>20190</code>
          <country>USA</country>
        </postal>
        <email>riwhite@verisign.com</email>
        <!-- uri and facsimile elements may also be added -->
      </address>
    </author>
	<author fullname="Susan Hares" initials="S" surname="Hares">
      <organization>Hickory Hill Consulting</organization>
      <address>
        <postal>
          <street>7453 Hickory Hill</street>
          <!-- Reorder these if your country does things differently -->
          <city>Saline</city>
          <region>MI</region>
          <code>48176</code>
          <country>USA</country>
        </postal>
        <email>shares@ndzh.com</email>
        <!-- uri and facsimile elements may also be added -->
      </address>
    </author>
    <author fullname="Rex E. Fernando" initials="R" surname="Fernando">
      <organization>Cisco Systems</organization>
      <address>
	<postal>
	  <street>170 W Tasman Dr</street>
	  <city>San Jose</city>
	  <region>CA</region>
	  <code>95134</code>
	  <country>USA</country>
	</postal>
	<email>rex@cisco.com</email>
      </address>
    </author>
    <date month="February" year="2013" />
    <area>Routing</area>
    <workgroup>Routing Area Working Group</workgroup>
    <abstract>
      <t>Programmatic interfaces to provide control over individual forwarding devices in a network promise to reduce operational costs while improving scaling, control, and visibility into the operation of large scale networks. To this end, several programmatic interfaces have been proposed. OpenFlow, for instance, provides a mechanism to replace the dynamic control plane processes on individual forwarding devices throughout a network with off box processes that interact with the forwarding tables on each device. Another example is NETCONF, which provides a fast and flexible mechanism to interact with device configuration and policy.</t>
	  <t>There is, however, no proposal which provides an interface to all aspects of the routing systemas a system. Such a system would not interact with the forwarding system on individual devices, but rather with the control plane processes already used to discover the best path to any given destination through the network, as well as interact with the routing information base (RIB), which feeds the forwarding table the information needed to actually switch traffic at a local level.</t>
	  <t>This document describes a set of use cases such a system could fulfill. It is designed to provide underlying support for the framework, policy, and other drafts describing the Interface to the Routing System (IRS).</t>
    </abstract>
  </front>
  <middle>
    <section title="Introduction" toc="default">
      <t> </t>
      <t>The Interface to the Routing System Framework [IRS] desribes a mechanism where the distributed control plane can be augmented by an outside control plane through an open, accessible interface, including the Routing Information Base (RIB), in individual devices. This represents a "halfway point" beteween completely replacing the traditional distributed control plane and directly configuring devices to distribute policy or modifications to routing through off-board processes. This draft proposes a set of use cases that explain where the work described in [IRS] will be useful. The goal is to inform not only the community's understanding of where IRS fits in the larger scheme of SDN proposals, but also to inform the requirements, framework, and specification of IRS to provide the best fit for the purposes which make the most sense for this type of programmatic interface. </t>
	  <t>Towards this end the authors have searched for a number of different use cases representing not only complex modifications of the control plane, including interaction with applications and network conditions, but also simpler use cases. The array of use cases presented here should provide the reader with a solid understanding of the power of an SDN solution that will augment, rather than replace, traditional distributed control planes.</t>
	  <t>Each use case is presented in its own section.</t>
    </section>
    <section title="Optimized Exit Control" toc="default">
  	  <t> </t>
	  <t>At edges where traffic exits along two or more possible paths, it is often desirable to choose a path based on more information the dynamic control plane provides. For instance, a network operator may want to take into account factos such as:</t>
      <t>
	  <list style="symbols">
	  <t>Cost per unit of data sent, indluding time of day variations, surcharges over a specific amount of data transmitted, and surcharges for transmitting data to specific types of destinations.</t>
		<t>Urgency of data traffic or flow.</t>
		<t>Exit point performance, including historical jitter, delay, and available bandwidth, possibly on a per destination basis.</t>
		<t>Availability of a specific destination through a given link at the per destination basis (more specific than the routing protocol provides).</t>
	  </list>
	  </t>
      <t>A number of possible solutions have been proposed or deployed in the past. For instance, the necessary metrics could be added to [BGP], or any other routing protocol, to provide the necessary information, and fine-tuned algorithms could be developed and deployed. Massive changes to well known and understood distributed control plane protocols to resolve a single use case, however, are not likely to be productive for the community as a whole. It's often difficult to justify the added complexity in the database and algorithms of routing protools to solve what is considered a point case.</t>
	  <t>Another alternative has been the development of specific appliances designed to monitor the information necessary to provide an optimal edge decision, and then to use some automated configuration mechanism to transmit the decision to the edge routers. An example is illustrated in the figure below.</t>
      <figure title="" suppress-title="false" align="center" alt="" width="" height="">
          <artwork xml:space="preserve" name="" type="" align="center" alt="" width="" height=""><![CDATA[
     |-----------------R1-----------|
     |                 |            |
Internal Network   Controller   External Network
     |                 |            |
     |-----------------R2-----------|]]>
          </artwork>
      </figure>
      <t>The controller in this network must: </t>
	  <t>
	  <list style="symbols">
	    <t>Discover the topology of the network from R1 and R2.</t>
		<t>Compare the current traffic flow information to policies set administratively by the network operator.</t>
		<t>Monitor the flow of traffic from the perspective of R1 and R2.</t>
		<t>Inject forwarding information to directly impact the traffic flow at the edge devices, or modify the policy of the existing distributed (dynamic) control plane already running in the network.</t>
      </list>
	  </t>
	  <t>Many of these steps is challenging for currently available solutions.</t>
	  <t>To discover the topology at the edge rotuers, the controllers can either participate in the control plane, or walk the local routing table using a network management protocol. Neither of these options are optimal in this case because the controlling process cannot interact dynamically with the local topology information in near real time through such mechanisms.</t>
	  <t>Injecting forwarding information directly into the RIB on the individual devices in this network is possible today through the configuration of static routes through some external mechanism, such as SNMP, NETCONF, or by direct external interaction with the devices' CLI. None of these options are attractive because:</t>
	  <t>
	  <list style="symbols">
	    <t>They modify the actual configuration of the device (unlike a dynamic routing process).</t>
		<t>They are too persistent (routes installed through static configuration persist across device reboots).</t>
	    <t>The controller cannot interact with the routing table in parallel with other routing processes. For instance, when a routing process attempts to install a new route in the routing table, there is often a callback or other notification to the other routing processes running on the same device; this notification provides important information the controller can take into account in its view of the current state of the routing table, and the state of the device's routing table. Interface level events also often trigger notifications from the RIB to local routing processes; these notifications would be invaluble for the controller to modify injected routing state in reaction to network topology events.</t>
		<t>Routes installed through the an off box controller through the CLI or XML interface are difficult to redistribute into other protocols to draw traffic to a specific exit point, and it can be difficult to fine tune how these injected routes interact with routes learned through other routing processes.</t>
      </list>
	  </t> 
	  <t>IRS can resolve these issues by providing an open interface to the local RIB on each device, allowing the controller to interact with the RIB just as a local routing process would. This would allow the controlling process to see the topology information in the RIB dynamically, receiving near real time updates for route removals, installs, and other events, and without relying on static configuration to inject forwarding information each device can use.</t>
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
	  <list style="symbols">
        <t>IRS should provide the ability to read the local RIB of each forwarding device, including the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of each installed route, a route preference, and an identifier indicating the installing process.</t> 
	    <t>The ability to monitor the available routes installed in the RIB of each forwarding device, including near real time notification of route installation and removal. This information must include the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of the installed route, and an identifier indicating the installing process.</t>
	    <t>The ability to install destination based routes in the local RIB of each forwarding device. This must include the ability to supply the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), a route preference, a route metric, a next hop, an outbound interface, and a route process identifier.</t>
		<t>The ability to interact with various policies configured on the forwarding devices, in order to inform the policies implemented by the dynamic routing processes. This interaction SHOULD be through existing configuration mechanisms, such as NETCONF, and SHOULD be recorded in the configuration of the local device so operators are aware of the full policy implemented in the network from the running configuration.</t>
		<t>The ability to interact with traffic flow and other network traffic level measurement protocols and systems, in order to determine path performance, top talkers, and other information required to make an informed path decision based on locally configured policy.</t>
	  </list>
	  </t>
    </section>
    <section title="Distributed Reaction to Network Based Attacks" toc="default">
	  <t> </t>
      <t>Quickly modifying the control plane to reroute traffic for one destination while leaving a standard configuration in place (filters, metrics, and other policy mechanisms) is a challenge --but this is precisely the challenge of a network engineer attempting to deal with a network incursion. The ability to redirect specific flows of information or specific classes of traffic into, through, and back out of traffic analyzers on the fly is crucial in these situations. The following network diagram provides an illustration of the problem.</t>
      <figure title="" suppress-title="false" align="center" alt="" width="" height="">
          <artwork xml:space="preserve" name="" type="" align="center" alt="" width="" height=""><![CDATA[
Valid Source---\  /--R2--------------------\
                R1                          R3---Valid Destination
Attack Source--/  \--Monitoring Device-----/]]></artwork>
      </figure>
	  <t>Modifying the cost of the link between R1 and R2 to draw the attack traffic through the monitoring device in the distributed control plane will, of necessity, also draw the valid traffic through the monitoring device. Drawing valid traffic through a monitoring device introduces delay, jitter, and other quality of service issues, as well as posing a problem for the monitoring device itself in terms of traffic load and management.</t>
	  <t>An IRS controller could stand between the detection of the attack and the control plane to facilitate the rapid modification of control and forwarding planes to either block the traffic or redirect it to analysis devices connected to the network.</t> 
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
	  <list style="symbols">
	    <t>The ability to monitor the available routes installed in the RIB of each forwarding device, including near real time notification of route installation and removal. This information must include the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of the installed route, and an identifier indicating the installing process.</t>
	    <t>The ability to install source and destination based routes in the local RIB of each forwarding device. This must include the ability to supply the destination prefix (NLRI), the source prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), a route preference, a route metric, a next hop, an outbound interface, and a route process identifier.</t>
	    <t>The ability to install a route to a null destination, effectively filtering traffic to this destination.</t>
		<t>The ability to interact with various policies configured on the forwarding devices, in order to inform the policies implemented by the dynamic routing processes. This interaction SHOULD be through existing configuration mechanisms, such as NETCONF, and SHOULD be recorded in the configuration of the local device so operators are aware of the full policy implemented in the network from the running configuration.</t>
		<t>The ability to interact with traffic flow and other network traffic level measurement protocols and systems, in order to determine path performance, top talkers, and other information required to make an informed path decision based on locally configured policy.</t>
	  </list>
	  </t>
    </section>
    <section title="Remote Service Routing" toc="default">
  	  <t> </t>
	  <t>In hub and spoke overlay networks, there is always an issue with balancing between the information held in the spoke routing table, optimal routing through the network underlying the overlay, and mobility. Most solutions in this space use some form of centralized route server that acts as a directory of all reachable destinations and next hops, a protocol by which spoke devices and this route server communicate, and caches at the remote sites.</t>
      <t>An IRS solution would use the same elements, but with a different control plane. Remote sites would register (or advertise through some standard routing protocol, such as BGP), the reachable destinations at each site, along with the address of the router (or other device) used to reach that destination. These would, as always, be stored in a route server (or several redundant route servers) at a central location.</t>
      <t>When a remote site sends a set of packets to the central location that are eventually destined to some other remote site, the central location can forward this traffic, but at the same time simply directly insert the correct routing information into the remote site's routing table. If the location of the destination changes, the route server can directly modify the routing information at the remote site as needed.</t>
      <t>An interesting aspect of this solution is that no new and specialized protocols are needed between the remote sites and the centralized route server(s). Normal routing protocols can be used to notify the centralized route server(s) of modifications in reachability information, and the route server(s) can respond as needed, based on local algorithms optimized for a particular application or network. For instance, short lived flows might be allowed to simply pass through the hub site with no reaction, while longer lived flows might warrant a specific route to be installed in the remote router. Algorithms can also be developed that would optimize traffic flow through the overlay, and also to remove routing entries from remote devices when they are no longer needed based on far greater intelligence than simple non-use for some period of time. </t>
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
	  <list style="symbols">
        <t>The ability to read the local RIB of each forwarding device, including the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of each installed route, a route preference, and an identifier indicating the installing process.</t> 
	    <t>The ability to monitor the available routes installed in the RIB of each forwarding device, including near real time notification of route installation and removal. This information must include the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of the installed route, and an identifier indicating the installing process.</t>
	    <t>The ability to install destination based routes in the local RIB of each forwarding device. This must include the ability to supply the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), a route preference, a route metric, a next hop, an outbound interface, and a route process identifier.</t>
	  </list>
	  </t>
	</section>
    <section title="Within Data Center Routing" toc="default">
      <t> </t>
	  <t> Data Centers have evolved into massive topologies with thousands of server racks and millions of hosts. Data Centers use BGP with ECMP, ISIS (with multiple LAGs), or other protocols to tie the data center together. Data centers are currently designed around a three or four tier structure with: server, top-of-rack switches, aggregation switches, and router interfacing the data center to the Internet. Microsoft's usage of BGP in the data center, described in [Lapukh-BGP], examines many of these elements of data center design.</t>
      <t>One key element of these Data Center routing infrastructures is the ability to quickly read topology information and excute configuration from a centralized location. Key to this environment is the tight feedback loop between learning about topology changes or loading changes, and instantiating new routing policy. Without IRS, may Data Centers are using extra physical topologies or logical topologies to work around the features. </t>
      <t>For example, Microsoft's network uses BGP because the topology state could be read from BGP impementations in a consistent fashion. Microsoft might have chosen a different routing protocol (such as ISIS) if the routing protocol state had been easier to obtain. Microsoft chose BGP for the data center because routers had a good BGP interface with topology information.</t>
      <t>An IRS solution would use the same in the elements, but with a different control plane. The IRS enable control plane could provide the Data Center 4 tier infrastructure the quick access to topology and data flow information needed for traffic flow optimization. Changes to the Data Center infrastructure done via the IRS could have a tight feedback loop.</t>t
      <t>Again, this solution would reduce the need for new and specialized protocols while giving the Data Center the control it desire.  The IRS routing interface could be extended to virtual routers. </t>
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
	  <list style="symbols">
        <t>The ability to read the local RIB of each forwarding device, including the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of each installed route, a route preference, and an identifier indicating the installing process.</t>
	    <t>The ability to monitor the available routes installed in the RIB of each forwarding device, including near real time notification of route installation and removal. This information must include the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of the installed route, and an identifier indicating the installing process.</t>
	    <t>The ability to install destination based routes in the local RIB of each forwarding device. This must include the ability to supply the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), a route preference, a route metric, a next hop, an outbound interface, and a route process identifier.</t>
	    <t>The ability to read the tables of other local protocol processes running on the device. This reading action SHOULD be supported through an import/export interface which can present the information in a consistent manner across all protocol implementations, rather than using a protocol specific model for each type of available process.</t>
	    <t>The ability to inject information directly into the local tables of other protocol processes running on the forwarding device. This injection SHOULD be supported through an import/export interface which can inject routing information in a consistent manner across all protocol implementations, rather than using a protocol specific model for each type of available process.</t>
		<t>The ability to interact with various policies configured on the forwarding devices, in order to inform the policies implemented by the dynamic routing processes. This interaction SHOULD be through existing configuration mechanisms, such as NETCONF, and SHOULD be recorded in the configuration of the local device so operators are aware of the full policy implemented in the network from the running configuration.</t>
		<t>The ability to interact with traffic flow and other network traffic level measurement protocols and systems, in order to determine path performance, top talkers, and other information required to make an informed path decision based on locally configured policy.</t>
	  </list>
	  </t>
    </section>
    <section title="Temporary Overlays between Data Centers" toc="default">
	  <t> </t>
      <t>Data Centers within one organization may operate as one single entity even though the Data Centers are geographically distributed fashion. Applications are load balanced within Data Centers and between data centers to take advantage of cost economics in power, storage, and server availability for compute resources. Applications are also transfer to alternate data centers in case of failures within a data center. To reduce time during failure, Data Centers often replicate user storage between two or more data centers. During the tranfer of stored information prior to a Data Center to Data Center move, the Data Center controllers need to dynamically aquire a large amount of inter-data center bandwidth through an overlay network, often during off hours.</t>
	  <t>IRS could provide the connection between the overlay network configuration, local policies, and the control plane to dynamically bring a large bandwidth inter-data center overlay or channel into use, and then to remove it from use when the data transfer is completed.</t>
	  <t> Similarly, during a fail-over, a control process within data centers interacts with a group host process and the network to seamless move the processing to another data center. During the fail-over case, additional process state may need to be moved as well to restart the system. The difference between these data-to-data center moves is immediate and urgent need to move systems. If an application (such as medical or banking services) pays to have this type of fail-over, it is likely the service will pay for preemption on network bandwidth. IRS can allow the Data Center network and the Network connecting the data center to prempt other best-effort traffic to send this priority data flow. After the high priority data flow has finished, networks can return to their previous condition</t> 
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
	  <list style="symbols">
        <t>The ability to read the local RIB of each forwarding device, including the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of each installed route, a route preference, and an identifier indicating the installing process.</t> 
	    <t>The ability to monitor the available routes installed in the RIB of each forwarding device, including near real time notification of route installation and removal. This information must include the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), the metric of the installed route, and an identifier indicating the installing process.</t>
	    <t>The ability to install destination based routes in the local RIB of each forwarding device. This must include the ability to supply the destination prefix (NLRI), a table identifier (if the forwarding device has multiple forwarding instances), a route preference, a route metric, a next hop, an outbound interface, and a route process identifier.</t>
		<t>The ability to interact with various policies configured on the forwarding devices, in order to inform the policies implemented by the dynamic routing processes. This interaction SHOULD be through existing configuration mechanisms, such as NETCONF, and SHOULD be recorded in the configuration of the local device so operators are aware of the full policy implemented in the network from the running configuration.</t>
		<t>The ability to interact with policies and configurations on the forwarding devices using time based processing, either through timed auto-rollback or some other mechanism. This interaction SHOULD be through existing configuration mechanisms, such as NETCONF, and SHOULD be recorded in the configuration of the local device so operators are aware of the full policy implemented in the network from the running configuration.</t>
		<t>The ability to interact with traffic flow and other network traffic level measurement protocols and systems, in order to determine path performance, top talkers, and other information required to make an informed path decision based on locally configured policy.</t>
	  </list>
	  </t>
    </section>
    <section title="Central membership computation for MPLS based VPNs" toc="default">
  	  <t> </t>
      <t>MPLS based VPNs use route target extended communities to express membership information. Every PE router holds incoming BGP NLRI and processes them to determine membership and then import the NLRI into the appropriate MPLS/VPN routing tables. This consumes resources, both memory and compute on each of the PE devices.</t>
	  <t>An alternative approach is to monitor routing updates on every PE from the attached CEs and then compute membership in a central manner. Once computed the routes are pushed to the VPN RIBs of the participating PEs.</t>
	  <t>This centralization of membership control has a few advantages.</t>
      <t>
  	  <list style="symbols">
	    <t>The membership mechanism (route-targets) need not be configured in each of the PEs and can be expressed once centrally.</t>
		<t>No resources in the PEs need to be spent to categorize routes into the VRF tables that they belong and to filter out unwanted state.</t>
		<t>Doing it centrally means the availability of almost unlimited compute capacity to compute membership and hence can be done in a scaleable manner.</t>
		<t>More sophisticated routing policies and filters can be applied during the central import/export process than can be expressed and performed using the traditional route target mechanism.</t>
		<t>Routes can be selectively pushed only to the participating PE's further reducing the memory load on the individual routers in the network. This further obviates for a distributed mechanisms such as rt constraints to reduce unnecessary path state in the routers.</t>
      </list>
      </t>
      <t> Note that centrally compution of membership can be applied to other scenarios as well such as VPLS, MVPNs, MAC VPNs etc. Depending on the scenario, what gets monitored from the CE might vary. Central computation will especially help VPLS where multi-homing and load balancing using distributed techniques has particularly been a challenge.</t>
	  <t>Also note that one of the biggest promises of central route computation is simplification and reduction of computation and memory load on all devices in the network. This use case is just one example that illustrates these benefits of central computation very well.</t>
      <t>Summary of IRS Capabilities and Interactions:</t>
      <t>
      <list style="symbols">
        <t>The ability to read the loc-RIB-In BGP table that gets all the routes that the CE has provided to a PE router.</t>
		<t>The ability to install destination based routes in the local RIB of the PE devices. This must include the ability to supply the destination prefix (NLRI), a table identifier, a route preference, a route metric, a next-hop tunnel through which traffic would be carried</t>
      </list>
      </t>
   </section>
  </middle>

  <back>
    <references title="Normative References">
      &RFC2119;
    </references>
 
  </back>
</rfc>

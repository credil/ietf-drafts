<?xml version="1.0" encoding="US-ASCII"?>
<!-- This template is for creating an Internet Draft using xml2rfc,
    which is available here: http://xml.resource.org. -->
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
<!-- One method to get references from the online citation libraries.
    There has to be one entity for each item to be referenced. 
    An alternate method (rfc include) is described in the references. -->
<!--
<!ENTITY RFC2119 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml">
<!ENTITY RFC2629 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2629.xml">
<!ENTITY RFC3552 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3552.xml">
<!ENTITY RFC5226 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5226.xml">
<!ENTITY RFC6350 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.6350.xml">
<!ENTITY RFC6501 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.6501.xml">
-->
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs), 
    please see http://xml.resource.org/authoring/README.html. -->
<!-- Below are generally applicable Processing Instructions (PIs) that most I-Ds might want to use.
    (Here they are set differently than their defaults in xml2rfc v1.32) -->
<?rfc strict="yes" ?>
<!-- give errors regarding ID-nits and DTD validation -->
<!-- control the table of contents (ToC) -->
<?rfc toc="yes"?>
<!-- generate a ToC -->
<?rfc tocdepth="4"?>
<!-- the number of levels of subsections in ToC. default: 3 -->
<!-- control references -->
<?rfc symrefs="yes"?>
<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc sortrefs="yes" ?>
<!-- sort the reference entries alphabetically -->
<!-- control vertical white space 
    (using these PIs as follows is recommended by the RFC Editor) -->
<?rfc compact="yes" ?>
<!-- do not start each main section on a new page -->
<?rfc subcompact="no" ?>
<!-- keep one blank line between list items -->
<!-- end of list of popular I-D processing instructions -->
<rfc category="info" docName="draft-groves-clue-multi-content-00" ipr="trust200902">
 <!-- category values: std, bcp, info, exp, and historic
    ipr values: trust200902, noModificationTrust200902, noDerivativesTrust200902,
       or pre5378Trust200902
    you can add the attributes updates="NNNN" and obsoletes="NNNN" 
    they will automatically be output with "(if approved)" -->

 <!-- ***** FRONT MATTER ***** -->

 <front>
   <!-- The abbreviated title is used in the page header - it is only necessary if the 
        full title is longer than 39 characters -->

   <title abbrev="Abbreviated Title">Describing Captures in CLUE and relation to multipoint conferencing</title>

   <!-- add 'role="editor"' below for the editors if appropriate -->

   <!-- Another author who claims to be an editor -->

   <author fullname="Christian Groves" initials="C" role="editor"
           surname="Groves">
     <organization>Huawei</organization>

     <address>
       <postal>
         <street></street>

         <!-- Reorder these if your country does things differently -->

         <city>Melbourne</city>

         <region></region>

         <code></code>

         <country>Australia</country>
       </postal>

       <email>Christian.Groves@nteczone.com</email>

       <!-- uri and facsimile elements may also be added -->
     </address>
   </author>
      <author fullname="Weiwei Yang" initials="W" 
           surname="Yang">
     <organization>Huawei</organization>

     <address>
       <postal>
         <street></street>

         <!-- Reorder these if your country does things differently -->

         <city></city>

         <region></region>

         <code></code>

         <country>P.R.China</country>
       </postal>

       <email>tommy@huawei.com</email>

       <!-- uri and facsimile elements may also be added -->
     </address>
   </author>

      <author fullname="Roni Even" initials="R"
           surname="Even">
     <organization>Huawei</organization>

     <address>
       <postal>
         <street></street>

         <!-- Reorder these if your country does things differently -->

         <city>Tel Aviv</city>

         <region></region>

         <code></code>

         <country>Isreal</country>
       </postal>

       <email>roni.even@mail01.huawei.com</email>

       <!-- uri and facsimile elements may also be added -->
     </address>
   </author>

   <date year="2013" />

   <!-- If the month and year are both specified and are the current ones, xml2rfc will fill 
        in the current day for you. If only the current year is specified, xml2rfc will fill 
	 in the current day and month for you. If the year is not the current one, it is 
	 necessary to specify at least a month (xml2rfc assumes day="1" if not specified for the 
	 purpose of calculating the expiry date).  With drafts it is normally sufficient to 
	 specify just the year. -->

   <!-- Meta-data Declarations -->

   <area>Real-time Applications and Infrastructure Area</area>

   <workgroup>CLUE</workgroup>

   <!-- WG name at the upperleft corner of the doc,
        IETF is fine for individual submissions.  
	 If this element is not present, the default is "Network Working Group",
        which is used by the RFC Editor as a nod to the history of the IETF. -->

   <keyword>template</keyword>

   <!-- Keywords will be incorporated into HTML output
        files in a meta tag but they have no effect on text or nroff
        output. If you submit your draft to the RFC Editor, the
        keywords will be used for the search engine. -->

   <abstract>
     <t>In a multipoint Telepresence conference, there are more than two sites participating.  Additional complexity is required to enable media streams from each participant to show up on the displays of the other participants.  Common policies to address the multipoint case include "site-switch" and "segment-switch". The document will discuss these policies as well as the "composed" policy and how they work in the multipoint case.</t>
	 <t>The current CLUE framework document contains the "composed" and "switched" attributes to describe situations where a capture is mix or composition of streams or where the capture represents a dynamic subset of streams. "Composed" and "switched" are capture level attributes. In addition to these attributes the framework defines an attribute "Scene-switch-policy" on a capture scene entry (CSE) level which indicates how the captures are switched.</t>
	 <t>This draft discusses composition/switching in CLUE and makes a number of proposals to better define and support these capabilities.</t>
   </abstract>
 </front>

 <middle>
   <section title="Introduction">
		<t>One major objective for Telepresence is to be able to preserve the "Being there" user experience.  However, in multi-site conferences it is often (in fact usually) not possible to simultaneously provide full size video, eye contact, common perception of gestures and gaze by all participants.  Several policies can be used for stream distribution and display: all provide good results but they all make different compromises.</t>
		<t>The policies are described in <xref target="I-D.ietf-clue-telepresence-use-cases"/>. <xref target="RFC6501"/> has the following requirement:<list hangIndent="8" style="hanging">
        	<t hangText="REQMT-14:">The solution MUST support mechanisms to make possible for either or both site switching or segment switching.  [Edt: This needs rewording.  Deferred until layout discussion is resolved.]</t>
		</list></t>
		<t>The policies described in the use case draft include the site-switch, segment-switch and composed policies.</t>
		<t>Site switch is described in the CLUE use case "One common policy is called site switching.  Let's say the speaker is at site A and everyone else is at a "remote" site.  When the room at site A shown, all the camera images from site A are forwarded to the remote sites.  Therefore at each receiving remote site, all the screens display camera images from site A.  This can be used to   preserve full size image display, and also provide full visual  context of the displayed far end, site A.  In site switching, there is a fixed relation between the cameras in each room and the displays   in remote rooms.  The room or participants being shown is switched from time to time based on who is speaking or by manual control, e.g., from site A to site B."</t>
		<t>These policies are mirrored in the framework document through a number of attributes.</t>
		<t>Currently in the CLUE framework document <xref target="I-D.ietf-clue-framework"/> there are two media capture attributes: Composed and Switched.</t>
		<t>Composed is defined as:<list hangIndent="8" style="hanging">
        	<t>A field with a Boolean value which indicates whether or not the Media Capture is a mix (audio) or composition (video) of streams.</t>
			<t>This attribute is useful for a media consumer to avoid nesting a composed video capture into another composed capture or rendering. This attribute is not intended to describe the layout a media provider uses when composing video streams.</t>
		</list></t>
		<t>Switched is defined as:<list hangIndent="8" style="hanging">
        	<t>A field with a Boolean value which indicates whether or not the Media Capture represents the (dynamic) most appropriate subset of a 'whole'.  What is 'most appropriate' is up to the provider and could be the active speaker, a lecturer or a VIP.</t>
		</list></t>
		<t>There is also a Capture Scene Entry (CSE) attribute "scene switch policy" defined as:<list hangIndent="8" style="hanging">
        	<t>A media provider uses this scene-switch-policy attribute to indicate its support for different switching policies.</t>
		</list></t>
	</section>
	 
	<section title="Issues"> 
		<t>This section discusses a number of issues in the current framework around the support of switched/composed captures and media streams when considering multipoint conferencing. Some issues are more required functions and some are related to the current description in the framework document.</t>
		
		<section title="Role of an MCU in a multipoint conference">
		<t>In a multipoint conference there is a central control point (MCU). The MCU will have the CLUE advertisements from all the conference participants and will prepare and send advertisements to all the conference participants.  The MCU will also have more information about the conference, participants and media which it receives at conference creation and via call signalling. This data is not stable since each user who joins or leaves the conference causes a change is conference state. An MCU supporting SIP may utilise the Conference event package, XCON and CCMP to maintain and distribute conference state.</t>
		<t><xref target="RFC4575"/> defines a conference event package. Using the event framework notifications are sent about changes in the membership of this conference and optionally about changes in the state of additional conference components. The conference information is composed of the conference description, host information, conference state, users that has endpoints where each endpoint includes the media description.</t>
		<t><xref target="RFC6501"/> extends the conference event package and tries to be signalling protocol agnostic.  RFC6501 adds new elements but also provides values for some of the elements defined in RFC4575, for example it defines roles ( like "administrator", "moderator", "user", "participant", "observer", and "none").</t>
		<t><xref target="RFC6503"/> Centralized Conferencing Manipulation Protocol (CCMP) allows authenticated and authorized users to create, manipulate, and delete conference objects.  Operations on conferences include adding and removing participants, changing their roles, as well as adding and removing media streams and associated endpoints.</t>
		<t>CCMP implements the client-server model within the XCON framework, with the conferencing client and conference server acting as client and server, respectively.  CCMP uses HTTP as the protocol to transfer requests and responses, which contain the domain-specific XML-encoded data objects defined in <xref target="RFC6501"/> "Conference Information Data Model for Centralized Conferencing (XCON)".</t>
		<t>The XCON data model and CCMP provides a generic way to create and control conferences. CCMP is not SIP specific but SIP endpoint will subscribe to the conference event package to get information about changes in the conference state.</t>
		<t>Therefore when a MCU implements the above protocols there will be an interaction between any CLUE states and those within a conferencing framework. For example: if an endpoint leaves a conference this will mean that an MCU may need to indicate via CLUE to the other endpoints that those captures are no longer available and it would also need to indicate via the Conferencing framework that the endpoint is longer part of the conference. </t>
		<t>The question is how do these concepts relate as the Conferencing framework does not have the concept of captures or scenes? Other aspects overlap, for example:<list hangIndent="8" style="hanging">
        	<t>The conference framework has "available media" , CLUE has encodings to indicate codec.</t>
			<t>The conference framework has "users", CLUE has no concept of users although it has capture attributes that relate to the users in a capture.</t>
		</list></t>
		<t>It is noted point to point calls may not implement the conferencing framework. It is desirable that CLUE procedures be the same whether an endpoint is communicating with a peer endpoint or an MCU.</t>
		</section>
		
		
		<section title="Relation to scene">
			<t>One of the early justifications for switching / composition was the ability to switch between sites. When looking at the CLUE framework there is no concept of "site" in the CLUE hierarchy. The closest concept is an "endpoint" but this has no identity within the CLUE syntax. The highest level is the "clueInfo" that includes captureScenes and an endpoint may have multiple capture scenes.</t>
			<t>If the switched and composed attributes are specified at a capture level it is not clear what the correlation is between the capture and the endpoint / scenes, particularly when the attributes are described in the context of sites. A scene may be composed of multiple captures. Where an MCU is involved in a conference with multiple endpoints, multiple capture scenes are involved. It becomes difficult to map all the scenes and capture information from the source endpoints into one capture scene sent to an endpoint. Discussion of switching, composition et al. needs to be described in terms of the CLUE concepts.</t>
			<t>When considering the SIP conferencing framework it can be seen that there are complications with interworking with the scene concept. There may be multiple media of the same type e.g room view and presentation but they are easily identified. This also needs to be considered.</t>
		</section>
		
		<section title="Description of the contents of a switched/composed capture">
			<t>When considering switching and composition whilst this may be represented by one capture and one resulting media stream there may be multiple original source captures. Each of these source captures would have had their own set of attributes. A media capture with the composed attribute allows the description of the capture as whole but not a description of the constituent parts.  In the case of a MCU taking multiple input media captures and compositing them into one output capture the CLUE related characteristics of these inputs are lost in the current solution. Alternate methods such as the RFC6501 layout field etc. may need to be investigated.</t>
			<t>Consider the case where MCUs receive CLUE advertisements from various endpoints. Having a single capture with a switched attribute makes it difficult to fully express what the content is when it is from multiple endpoints. It may be possible to specify lists of capture attribute values when sending an advertisement from the MCU, i.e. role=speaker,audience but it becomes difficult to relate multiple attributes, i.e. (role=speaker,language=English),(role=audience,language=french). </t>
			<t>One capture could represent source captures from multiple locations. A consumer may wish to examine the inputs to a switched capture , i.e. choose which of the original endpoints it wants to see/hear. In order to do this the original capture information would need to be conveyed in a manner that minimises overhead for the MCU.</t>
			<t>By being able to link multiple source captures to one mixed (switched/composed) capture in a CLUE advertisement allows a fuller description of the content of the capture.</t>
		</section>
		
		<section title="Attribute interaction">
			<t>Today the "composed" and "switched" attributes appear at a media capture level. If "switched" is specified for multiple captures in a capture scene it's not clear from the framework what the switching policy is. For example: If a CSE contains three VCs each with "switched" does the switch occurs between these captures? Does the switch occur internal to each capture?</t>
			<t>The "scene-switch-policy" CSE attribute has been defined to indicate switch policy but there doesn't appear to be a description of whether this only relates to captures marked with "switch" and/or "composed"?  If a CSE marked with "scene-switch-policy" contains non-switched, non-composed captures what does this mean?</t>
			<t>What are the interactions between the two properties? E.g. Are "switched" and "composed" attributes mutually exclusive or not?  Is switched capture with a scene switch policy of "segment-switched" a "composed" capture?</t>
			<t>These issues need to be clarified in the framework.</t>
		</section>
		
		<section title="Policy">
			<t>The "Scene-switch-policy" attribute allows the indication of whether switched captures are "site" or "segment" switched. However there is no indication of what the switch or the composition "trigger" policy is. Content could be provided based on a round robin view, loudest speaker etc. Where an advertising endpoint supports different algorithms it would be advantageous for a consumer to know and select an applicable policy.</t>
		</section>
		
		<section title="Media stream composition and encodings">
			<t>Whether single or multiple streams are used for switched captures is not clear from the capture description. For example:</t>
			<t>There are 3 endpoints (A,B,C) each with 3 video captures.(VCa1,VCa2,VCa3, etc.). A MCU wants to indicate to endpoint C that it can offer a switched view of endpoints A and B.</t>
			<t>It could send an Advertisement with CSE (VCa1,VCa2,VCa3, VCb1,VCb2,VCb3),scene-switch-policy=site-switch.</t>
			<t>Normally such a configuration (without the switch policy) would relate to 6 different media streams. Switching introduces several possibilities.</t>
			<t>For site switching:<list hangIndent="8" style="hanging">
        	<t hangText="a)">There could one media stream with the contents of all the 6 captures. The MCU always send a composed image with the VCs from the applicable end point.</t>
			<t hangText="b)">There could be two media streams each containing the VCs from one endpoint, the MCU chooses which stream to send.</t>
			<t hangText="c)">There could be 6 media streams. The MCU chooses which 3 streams to send.</t>
		</list></t>
			<t>For segment switching this is further complicated because the MCU may choose to send media related to endpoint A or B.  There no text describing any limitation so the MCU may send 1 VC or 5.</t>
			<t>Utilising CLUE "encodings" may be a way to describe how the switch is taking place in terms of media provided but such description is missing from the framework. One could assume that an individual encoding be assigned to multiple media captures (i.e. multiple VCs to indicate they are encoded in the same stream) but again this is problematic as the framework indicates that "An Individual Encoding can be assigned to at most one Capture Encoding at any given time."</t>
			<t>This could do with further clarification in the framework.</t>
		</section>
		
		<section title="Relation of switched captures to simultaneous transmission sets">
			<t>Simultaneous Transmission Set is defined as "a set of Media Captures that can be transmitted simultaneously from a Media Provider." It's not clear how this definition would relate to switched or composed streams. The captures may not be able to be sent at the same time but may form a timeslot on a particular stream. They may be provided together but not at precisely the same time.</t>
			<t>The current version of the framework in section 6.3 it indicates that: <list hangIndent="8" style="hanging">
        	<t>"It is a syntax conformance requirement that the simultaneous transmission sets must allow all the media captures in any particular Capture Scene Entry to be used simultaneously."</t>
			</list></t>
			<t>If switching or composition is specified at a capture level only it is evident that simultaneity constraints do not come into play. However if multiple captures are used in a single media stream I.e. associated with the CSE then these may be subject to a simultaneous transmission set description.</t>
			<t>It is also noted that there is a similar issue for encoding group. See section 8/[Framework]:<list hangIndent="8" style="hanging">
        	<t>"It is a protocol conformance requirement that the Encoding Groups must allow all the Captures in a particular Capture Scene Entry to be used simultaneously."</t>
			</list></t>
			<t>If "switching" is used then there is no need to send the encodings at the same time.</t>
			<t>This needs to be clarified.</t>
		</section>
		
		<section title="Conveying spatial information for switched/composed captures">
			<t>CLUE currently allows the ability to signal spatial information related to a media capture. It is unclear in the current draft how this would work with switching/composition.  In section 6.1 / <xref target="I-D.ietf-clue-telepresence-use-cases"/> it does say:<list hangIndent="8" style="hanging">
        	<t>"For a switched capture that switches between different sections within a larger area, the area of capture should use coordinates for the larger potential area."</t>
			</list></t>
			<t>This describes a single capture not when there are multiple switched captures. It appears to focus on segment switching rather than site switching and does not appear to cover "composed" (if it is related).</t>
			<t>An advertiser may or may not want to use common spatial attributes for captures associated with a switched captures. For example: it may be beneficial for the Advertiser in a composed image to indicate that different captures have a different capture area in a virtual space.</t>
			<t>This should be given consideration in the framework.</t>
		</section>
		
		<section title="Consumer selection">
			<t>In section 6.2.2 of version 9 <xref target="I-D.ietf-clue-framework"/> it indicates that an Advertiser may provide multiple values for the "scene-switch-policy" and that the Consumer may choose and return the value it requires.</t>
			<t>In version 9 of the framework there was no mechanism in CLUE for a Consumer to choose and return individual values from capture scene, CSE or media capture attributes.</t>
			<t>In version 10 of the framework the text was updated to indicate that the consumer could choose values from a list. It is not clear that this capability is needed as the procedure only relates to the "scene-switch-policy". The switching policy may be better specified by other means.</t>
		</section>	
	</section>
	 
	<section title="Proposal">
		<t>As has been discussed above there are a number of issues with regards to the support of switched/composed captures/streams in CLUE particularly when considering MCUs. The authors believe that there is no single action that can address the above issues. Several options are discussed below. The options are not mutually exclusive.<list hangIndent="8" style="hanging">
        	<t hangText="1)">Introduce syntax to CLUE to better describe source captures</t>
			<t hangText="2)">Introduce updates to the XCON conferencing framework (e.g. Conference package, XCON etc.) to introduce CLUE concepts. </t>
			<t hangText="3)">Update CLUE to better describe the current suite of attributes with the understanding these provide limited information with respect to source information.</t>
		</list></t>
		
		<section title="CLUE Syntax Updates">
			<t>The authors believe that there are a number of requirements for this:<list hangIndent="8" style="hanging">
        	<t hangText="-">It should be possible to advertise the individual captures that make up a single switched/composed media stream before receiving the actual media stream.</t>
			<t hangText="-">It should be possible to describe the relationship between captures that make up a single switched/composed media stream.</t>
			<t hangText="-">It should be possible to describe this using CLUE semantics rather than with terms such as "site" or "segment" which need their own definition.</t>
		</list></t>
			<t>The authors also believe that whether media is composed, segment switched, site switched the common element is that the media stream contains multiple captures from potentially multiple sources.</t>
			<t><xref target="I-D.ietf-clue-framework"/> does have the "Scene-switch-policy" attribute at a CSE level but as described in section 2 it is not sufficient for several reasons. E.g. it is not possible assign an encoding to a CSE, a CSE cannot reference captures from multiple scenes and there is a relationship with STSs that needs to be considered.</t>
			<t>In order to be able to fully express and support media stream with multiple captures the authors propose a new type of capture, the "multiple content capture" (MCC). The MCC is essentially the same as audio or video captures in that it may have its own attributes the main difference is that it can also include other captures. It indicates that the MCC capture is composed of other captures. This composition may be positional (i.e. segments/tiling) or time composition (switched) etc. and specified by a policy attribute. The MCC can be assigned an encoding. For example:
			<list hangIndent="8" style="hanging">
        		<t>MCC1(VC1,VC2,VC3),[POLICY]</t>
			</list></t>
			<t>This would indicate that MCC1 is composed of 3 video captures according to the policy. </t>
			<t>One further difference is that a MCC may reference individual captures from multiple scenes. For example:<list hangIndent="8" style="hanging">
        		<t>CS#1(VC1,VC2)</t>
				<t>CS#2(VC3,VC4)</t>
				<t>CS#3(MCC1(VC1,VC3))</t>
			</list></t>
			<t>This would indicate that scene #3 contains a MCC that is composed from individual encodings VC1 and VC3. This allows the consumer to associate any capture scene properties from the original scene with the multiple content capture.</t>
			<t>The MCC would be able to be utilised by both normal endpoints and MCUs. For example: it would allow an endpoint to construct a mixed video stream that is a virtual scene with a composition of presentation video and individual captures.</t>
			<t>This proposal does not consider any relation to the SIP conferencing framework.</t>
			<t>The sections below provide more detail on the proposal.</t>
			
			<section title="Definitions">
				<t>Multiple content capture: Media capture for audio or video that indicates the capture contains multiple audio or video captures. Individual media captures may or may not be present in the resultant capture encoding depending on time or space. Denoted as MCCn in the example cases in this document.</t>
			</section>
			
			<section title="Multiple Content Capture Details">
				<t>The MCC indicates that multiple captures are contained in one media capture by referencing the applicable individual media captures. Only one capture type (i.e. audio, video, etc.) is allowed in each MCC instance. The MCC contains a reference to the media captures as well attributes associated with the MCC itself. The MCC may reference individual captures from other capture scenes. If an MCC is used in a CSE that CSE may also reference captures from other Capture Scenes.</t>
				<t>Note: Different Capture Scenes are not spatially related.</t>
				<t>Each instance of the MCC has its own captureID i.e. MCC1. This allows all the individual captures contained in the MCC to be referenced by a single ID.</t>
				<t>The example below shows the use of a MultipleContent capture:</t>
				<figure align="left">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	CaptureScene1 [VC1 {attributes},
                   VC2 {attributes},
                   VC3 {attributes},
				   MCC1(VC1,VC2,VC3){attributes}]
           ]]></artwork>
		   		</figure>
				<t>This indicates that MCC1 is a single capture that contains the captures VC1, VC2 and VC3 according to any MCC1 attributes.</t>
				<t>One or more MCCs may also specified in a CSE. This allows an Advertiser to indicate that several MCC captures are used to represent a capture scene.</t>
				<t>Note: Section 6.1/<xref target="I-D.ietf-clue-framework"/> indicates that "A Media Capture is associated with exactly one Capture Scene". For MCC this could be further clarified to indicate that "A Media Capture is defined in a capture scene and is given an advertisement unique identity. The identity may be referenced outside the Capture Scene that defines it through a multiple content capture (MCC).</t>				
			</section>
			
			
			<section title="MCC Attributes">
				<t>Attributes may be associated with the MCC instance and the individual captures that the MCC references. A provider should avoid providing conflicting attribute values between the MCC and individual captures.  Where there is conflict the attributes of the MCC override any that may be present in the individual captures.</t>
				<t>There are two MCC specific attributes "MaxCaptures" and "Policy" which are used to give more information regarding when the individual captures appears and what policy is used to determine this.</t>
				<t>The spatial related attributes can be further used to determine how the individual captures "appear" within a stream. For example a virtual scene could be constructed for the MCC capture with two video captures with a "MaxCaptures" attribute of 2 and an "area of capture" attribute provided with an overall area. Each of the individual captures could then also include an "area of capture" attribute with a sub-set of the overall area. The consumer would then know the relative position of the content in the composed stream. For example: The above capture scene may indicate that VC1 has an x-axis capture area 1-5, VC2 6-10 and VC3 11-15. The MCC capture may indicate an x-axis capture area 1-15.</t>
			</section>
			
			<section title="MCC Attributes">
				<t>MaxCaptures:{integer}</t>
				<t>This field is only associated with MCCs and indicates the maximum number of individual captures that may appear in a capture encoding at a time. It may be used to derive how the individual captures within the MCC are composed with regards to space and time. Individual content in the capture may be switched in time so that only one of the individual captures/CSEs are shown (MaxCaptures:1). The individual captures may be composed so that they are all shown in the MCC (MaxCaptures:n).</t>
				<t>For example:<list hangIndent="8" style="hanging">
        			<t>MCC1(VC1,VC2,VC3),MaxCaptures:1</t>
				</list></t>
				<t>This would indicate that the Advertiser in the capture encoding would switch (or compose depending on policy) between VC1, VC2 or VC3 as there may be only a maximum of one capture at a time.</t>
			</section>
			
			<section title="Composition policy">	
				<t>TBD - This attribute is to address what algorithm the endpoint/MCU uses to determine what appears in the MCC captures. E.g. loudest, round robin.</t>
			</section>		
			
			
			<section title="Synchronisation">	
				<t>Note: The {scene-switch-policy} attribute has values that indicates  "site-switch" or "segment" switch. The distinction between these is that "site-switch" indicates that when there is mixed content that captures related to an endpoint appear together.  "segment-switch" indicates that different endpoints captures could appear together. An issue is that a Consumer has no concept of "endpoints" only "capture scenes".  Also as highlighted a Consumer has no method to return parameters for CSEs.</t>
				<t>The use of MCCs enables the Advertiser to communicate to the Consumer that captures originate from different captures scenes. In cases where multiple MCCs represent a scene (i.e. multiple MCCs in a CSE) an Advertiser may wish to indicate that captures from one capture scene are present in the capture encodings of specified MCCs at the same time.  Having an attribute at capture level removes the need for CSE level attributes which are problematic for consumers.</t>
				<t>Synch-id: { integer}</t>
				<t>This MCC attribute indicates how the individual captures in multiple MCC captures are synchronised. To indicate that the capture encodings associated with MCCs contain captures from the source at the same time the Advertiser should set the same SynchID on each of the concerned MCCs.  It is the provider that determines what the source for the captures is. For example when the provider is in an MCU it may determine that each separate CLUE endpoint is a remote source of media.</t>
				<t>For example:</t>
				<figure align="left" anchor="figure1" title="Synchronisation Example">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	CaptureScene1[Description=AustralianConfRoom,
                  VC1(left),VC2(middle),VC3(right),
				  CSE1(VC1,VC2,VC3)]
	CaptureScene2[Description=ChinaConfRoom,
	              VC4(left),VC5(middle),VC6(right),
                  CSE2(VC4,VC5,VC6)]
	CaptureScene3[MCC1(VC1,VC4){Sync-id:1}{encodinggroup1},
                  MCC2(VC2,VC5){Sync-id:1}{encodinggroup2},
                  MCC3(VC3,VC6){encodinggroup3},
                  CSE3(MCC1,MCC2,MCC3)]
           ]]></artwork>	
		   </figure>
		   		<t>The above advertisement would indicate MCC1,MCC2,MCC3 make up a capture scene. There would be three capture encodings. Because MCC1 and MCC2 have the same Sync-id, each encoding1 and encoding2 would together have content from only capture scene 1 or only capture scene 2 at a particular point in time. Encoding3 would not be synchronised with encoding1 or encoding2.</t>
				<t>Without this attribute it is assumed that multiple MCCs may provide different sources at any particular point in time.</t>
			</section>		
			
			
			<section title="MCC and encodings">	
				<t>MCCs shall be assigned an encoding group and thus become a capture encoding.  The captures referenced by the MCC do not need to be assigned to an encoding group. This means that all the individual captures referenced by the MCC will appear in the capture encoding according to any MCC attributes. This allows an Advertiser to specify capture attributes associated with the individual captures without the need to provide an individual capture encoding for each of the inputs.</t>
				<t>If an encoding group is assigned to an individual capture referenced by the MCC it indicates that this capture may also have an individual capture encoding.</t>
				<t>For example:</t>
				<figure align="left">
       <preamble></preamble>
<artwork align="left"><![CDATA[
    CaptureScene1 [VC1 {encoding group1},
                   VC2 ]
			       MCC1(VC1,VC2){encoding group3}] 
           ]]></artwork>	
		   </figure>
		   		<t>This would indicate that VC1 may be sent as its own capture encoding from encoding group1 or that it may be sent as part of a capture encoding from encoding group3 along with VC2.</t>
				<t>Note: The section 8/<xref target="I-D.ietf-clue-framework"/> indicates that every capture is associated with an encoding group. To utilise MCCs this requirement has to be relaxed.</t>				
			</section>
			
			
			<section title="MCCs and STSs">	
				<t>The MCC can be used in simultaneous sets, therefore providing a means to indicate whether several multiple content captures can be provided at the same time. Captures within a MCC can be provided together but not necessarily at the same time. Therefore by specifying a MCC in an STS it does not indicate that all the referenced individual captures may be present at a time. The MaxCaptures attributes indicates the maximum number of captures that may be present.</t>
				<t>An MCC instance of is limited to one media type e.g. video, audio, text.</t>
				<t>Note: This gets around the problem where the framework says that all captures (even switched ones) within a CSE have to be allowed in a STS to be sent at the same time.</t>
			</section>			
			
			
			<section title="Consumer Behaviour">	
				<t>On receipt of an advertisement with an MCC the Consumer treats the MCC as per other individual captures with the following differences:<list hangIndent="8" style="hanging">
        			<t hangText="-">The Consumer would understand that the MCC is a capture that includes the referenced individual captures and that these individual captures would be delivered as part of the MCC's capture encoding.</t>
					<t hangText="-">The Consumer may utilise any of the attributes associated with the referenced individual captures and any capture scene attributes from where the individual capture was defined to choose the captures.</t>
					<t hangText="-">The Consumer may or may not want to receive all the indicated captures. Therefore it can choose to receive a sub-set of captures indicated by the MCC.</t>
				</list></t>
				<t>For example if the Consumer receives:<list hangIndent="8" style="hanging">
        			<t>MCC1(VC1,VC2,VC3){attributes}</t>
				</list></t>		
				<t>A Consumer should choose all the captures within a MCCs however if the consumer determines that it doesn't want VC3 it can return MCC1(VC1,VC2). If it wants all the individual capture then it returns just a reference to the MCC (i.e. MCC1).</t>
				<t>Note: The ability to return a subset of capture is for consistency with the current framework. It says that a Consumer should choose all the captures from a CSE but it allows it to select a subset (if the STS is provided). The intent was to provide equivalent functionality for a MCC.</t>
				
			</section>
			
			
			<section title="MCU Behaviour">
				<t>The use of MCCs allows the MCU to easily construct outgoing Advertisements. The following sections provide several examples.</t>
				<section title="Single content captures and multiple contents capture in the same Advertisement">
					<t>Four endpoints are involved in a CLUE session. To formulate an Advertisement to endpoint 4 the following Advertisements received from endpoint 1 to 3 and used by the MCU. Note: The IDs overlap in the incoming advertisements. The MCU is responsible for making these unique in the outgoing advertisement.</t>
					<figure align="left" anchor="figure2" title="MCU case: Received advertisements">
       <preamble></preamble>
<artwork align="left"><![CDATA[
    Endpoint 1 CaptureScene1[Description=AustralianConfRoom,
                             VC1(role=audience)]
    Endpoint 2 CaptureScene1[Description=ChinaConfRoom,
                             VC1(role=speaker),VC2(role=audience),
						     CSE1(VC1,VC2)]
    Endpoint 3 CaptureScene1[Description=USAConfRoom,
                             VC1(role=audience)]
           ]]></artwork>
		   </figure>
					<t>Note: Endpoint 2 above indicates that it sends two streams.</t>
					<t>If the MCU wanted to provide a multiple content capture containing the audience of the 3 endpoints and the speaker it could construct the following advertisement:</t>
					<figure align="left" anchor="figure3" title="MCU case: MCC with multiple audience and speaker">
       <preamble></preamble>
<artwork align="left"><![CDATA[
    CaptureScene1[Description=AustralianConfRoom,
	              VC1(role=audience)]
    CaptureScene2[Description=ChinaConfRoom,
	              VC2(role=speaker),VC3(role=audience),
                  CSE1(VC2,VC3)]
    CaptureScene3[Description=USAConfRoom,
	              VC4(role=audience)]
    CaptureScene4[MCC1(VC1,VC2,VC3,VC4){encodinggroup1}]
           ]]></artwork>
		   </figure>						
					<t>Alternatively if the MCU wanted to provide the speaker as one stream and the audiences as another it could assign an encoding group to VC2 in Capture Scene 2 and provide a CSE in Capture Scene 4:</t>
					<figure align="left" anchor="figure4" title="MCU case: MCC with audience and separate speaker">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	CaptureScene1[Description=AustralianConfRoom,
	              VC1(role=audience)]
	CaptureScene2[Description=ChinaConfRoom,
	              VC2(role=speaker){encodinggroup2},
				  VC3(role=audience),
				  CSE1(VC2,VC3)]
	CaptureScene3[Description=USAConfRoom,
	              VC4(role=audience)]
	CaptureScene4[MCC1(VC1,VC3,VC4){encodinggroup1},
				  CSE2(MCC1,VC2)]
           ]]></artwork>
		   </figure>	
					<t>Therefore a Consumer could choose whether or not to have a separate "role=speaker" stream and could choose which endpoints to see. If it wanted the second stream but not the Australian conference room it could indicate the following captures in the Configure message:</t>
					<figure align="left" anchor="figure5" title="MCU case: Consumer Response">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	MCC1(VC3,VC4),VC2
           ]]></artwork>
		   </figure>				
				</section>
		
			
			<section title="Several multiple content captures in the same Advertisement">
				<t>Multiple MCCs can be used where multiple streams are used to carry media from multiple endpoints.  For example: </t>
				<t>A conference has three endpoints D,E and F, each end point has three video captures covering the left, middle and right regions of each conference room. The MCU receives the following advertisements from D and E:</t>
	<figure align="left" anchor="figure6" title="MCU case: Multiple captures from multiple endpoints">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	Endpoint D CaptureScene1[Description=AustralianConfRoom,
                             VC1(left){encodinggroup1},
                             VC2(middle){encodinggroup2},
                             VC3(right){encodinggroup3},
                             CSE1(VC1,VC2,VC3)]
	Endpoint E CaptureScene1[Description=ChinaConfRoom,
                             VC1(left){encodinggroup1},
                             VC2(middle){encodinggroup2},
                             VC3(right){encodinggroup3},
                             CSE1(VC1,VC2,VC3)]
           ]]></artwork>
		   </figure>
		   		<t>Note: The Advertisement uses the same identities. There is no co-ordination between endpoints so it is likely there would be identity overlap between received advertisements.</t>
				<t>The MCU wants to offer Endpoint F three capture encodings. Each capture encoding would contain a capture from either Endpoint D or Endpoint E depending on the policy. The MCU would send the following:</t>
<figure align="left" anchor="figure7" title="MCU case: Multiple MCCs for multiple captures">
       <preamble></preamble>
<artwork align="left"><![CDATA[
	CaptureScene1[Description=AustralianConfRoom,
				  VC1(left),VC2(middle),VC3(right),
				  CSE1(VC1,VC2,VC3)]
	CaptureScene2[Description=ChinaConfRoom,
	              VC4(left),VC5(middle),VC6(right),
				  CSE2(VC4,VC5,VC6)]
	CaptureScene3[MCC1(VC1,VC4){encodinggroup1},
				  MCC2(VC2,VC5){encodinggroup2},
                  MCC3(VC3,VC6){encodinggroup3},
				  CSE3(MCC1,MCC2,MCC3)]
           ]]></artwork>
		   </figure>
		   		<t>Note: The identities from Endpoint E have been renumbered so that they are unique in the outgoing advertisement.</t>
			</section>	
		</section>
	  </section>	
			
			<section title="Multipoint Conferencing Framework Updates">
				<t>The CLUE protocol extends the EP description defined in the signalling protocol (SDP for SIP) by providing more information about the available media.  If we look at XCON it uses the information available from the signalling protocol but instead of using SDP to distribute the participants information and to control the multipoint conference. This is done using a data structure defined in XML using the CCMP protocol over HTML (note that CCMP can be used also over CLUE channel if required).  XCON provide a hierarchy the starts from conference information that includes users having endpoints that have media.</t>
				<t>The role is part of the user structure while the mixing mode is part of the conference level information specifying the mixing mode per each of the media available in the conference.</t>
				<t>CLUE on the other end does not have such structure it start from what is probably, in XCON terms, an end points that has media structured by Scenes that has media.  There is no user or conference level information though the "role" proposal tries to add the user information (note that use information is different from the role in the call or the conference).</t>
				<t>The XCON structure looks better when looking at a multipoint conference.  Yet it does not make sense to have such a data model for the point to point calls. Therefore only going with this option means that capture attribute information will not be available for point to point calls.</t>
			</section>	
				
			
			<section title="Existing Parameter Updates">
				<t>As discussed in section 2 the existing CLUE attributes surrounding switching and composition have a number of open issues. This section proposes changes to the text describing the attributes to better describe their usage and interaction. It is also assumed that by using these attributes there is no attempt to describe the any component source capture information.</t>
				<section title="Composed">
					<t>The current CLUE framework describes the "Composed" attribute as:<list hangIndent="8" style="hanging">
        			<t>A boolean field which indicates whether or not the Media Capture is a mix (audio) or composition (video) of streams.</t>
					<t>This attribute is useful for a media consumer to avoid nesting a composed video capture into another composed capture or rendering. This attribute is not intended to describe the layout a media provider uses when composing video streams.</t>
					</list></t> 
					
					<t>It is proposed to update the description:<list hangIndent="8" style="hanging">
        				<t>A boolean field which indicates whether or not the Media Capture has been composed from a mix of audio sources or several video sources. The sources may be local to the provider (i.e. video capture device) or remote to the provider (i.e. a media stream received by the provider from a remote endpoint). This attribute is useful for a media consumer to avoid nesting a composed video capture into another composed capture or rendering. </t>
						<t>This attribute does not imply anything with regards to the attributes of the source audio or video except that the composed capture will be contained in a capture encoding from a single source. This attribute is not intended to describe the layout a media provider uses when composing video streams.</t>
						<t>The "composed" attribute may be used in conjunction with a "switched" attribute when one or more of the dynamic sources is a composition.</t>
					</list></t> 
				</section>	
			
			
			
			<section title="Switched">
				<t>The current CLUE framework describes the "Switched" attribute as:<list hangIndent="8" style="hanging">
        			<t>A boolean field which indicates whether or not the Media Capture represents the (dynamic) most appropriate subset of a 'whole'. What is 'most appropriate' is up to the provider and could be the active speaker, a lecturer or a VIP.</t>
					</list></t>  
					
					<t>It is proposed to update the description:<list hangIndent="8" style="hanging">
        				<t>A boolean field which indicates whether the Media Capture represents a dynamic representation of the capture scene that contains the capture. It applies to both audio and video captures.</t>
						<t>A dynamic representation is one that provides alternate capture sub-areas within the overall area of capture associated with the capture over time in a single capture encoding from one source. What capture area is contained in the capture encoding at a particular time is dependent on the provider policy. For example: a provider may encode the active speaker or lecturer based on volume level. It is not possible for consumers to associate attributes with a particular capture sub-area nor to indicate which sub-capture area they require.</t>
					</list></t>  
				</section>
				
				
			<section title="Scene-switch-policy">
				<t>The current CLUE framework describes the "Scene Switch Policy" attribute as:<list hangIndent="8" style="hanging">
        				<t>Scene-switch-policy: {site-switch, segment-switch} </t>
						<t>A media provider uses this scene-switch-policy attribute to indicate its support for different switching policies.  In the provider's Advertisement, this attribute can have multiple values, which means the provider supports each of the indicated policies.</t>
						<t>The consumer, when it requests media captures from this Capture Scene Entry, should also include this attribute but with only the single value (from among the values indicated by the provider) indicating the Consumer's choice for which policy it wants the provider to use.  The Consumer must choose the same value for all the Media Captures in the Capture Scene Entry.  If the provider does not support any of these policies, it should omit this attribute.</t>
						<t>The "site-switch" policy means all captures are switched at the same time to keep captures from the same endpoint site together.  Let's say the speaker is at site A and everyone else is at a "remote" site.</t>
						<t>When the room at site A shown, all the camera images from site A are forwarded to the remote sites.  Therefore at each receiving remote site, all the screens display camera images from site A. This can be used to preserve full size image display, and also provide full visual context of the displayed far end, site A. In site switching, there is a fixed relation between the cameras in each room and the displays in remote rooms.  The room or participants being shown is switched from time to time based on who is speaking or by manual control.</t>
						<t>The "segment-switch" policy means different captures can switch at different times, and can be coming from different endpoints.  Still using site A as where the speaker is, and "remote" to refer to all the other sites, in segment switching, rather than sending all the images from site A, only the image containing the speaker at site A is shown.  The camera images of the current speaker and previous speakers (if any) are forwarded to the other sites in the conference.</t>
						<t>Therefore the screens in each site are usually displaying images from different remote sites - the current speaker at site A and the  previous ones.  This strategy can be used to preserve full size image display, and also capture the non-verbal communication  between the speakers.  In segment switching, the display depends on the activity in the remote rooms - generally, but not necessarily based on audio / speech detection.</t>
					</list></t>   
					<t>Firstly it is proposed to rename this attribute to "Capture Source Synchronisation" in order to remove any confusion with the switch attribute and also to remove the association with a scene as the any information regarding source scenes is lost. This is due to that the CSE represents the current scene. No change in functionality is intended by the renaming. It is proposed to describe it as follows:<list hangIndent="8" style="hanging">
        				<t>Capture Source Synchronisation: {source-synch,asynch}</t>
						<t>By setting this attribute against a CSE it indicates that each of the media captures specified within the CSE results in a capture encoding that contains media related to different remote sources. For example if CSE1 contains VC1,VC2,VC3 then there will be three capture encodings sent from the provider each displaying captures from different remote sources. It is the provider that determines what the source for the captures is. For example when the provider is in an MCU it may determine that each separate CLUE endpoint is a remote source of media. Likewise it is the provider that determines how many remote sources are involved. However it is assumed that each capture within the CSE will contain the same number and set of sources.</t>
						<t>"Source-synch" indicates that each capture encoding related to the captures within the CSE contains media related to one remote source at the same point in time.</t>
						<t>"Asynch" indicates that that each capture encoding may contain media related to any remote source at any point in time.</t>
						<t>If a provider supports both synchronisation methods it should send separate CSEs containing separate captures, each CSE with a separate capture source synchronisation label.</t>
						<t>A provider when setting attributes against captures within a Capture Source Synchronisation marked CSE should consider that the media related to the remote sources may have its own separate characteristics. For example: each source may have its own capture area therefore this needs to be taken into account in the providers advertisement. </t>
						<t>The "Switched" attribute may be used with a capture in a "Capture Source Synchronisation" marked CSE. This indicates that one or more of the remote sources associated with the capture has dynamic media that may change within its own time frame. i.e. the media from a remote source may change without an impact on the other captures.</t>
						<t>The "Composed" attribute may be used with captures in the "Capture Source Synchronisation" marked CSE. This indicates the capture encoding contains a composition or multiple sources from one remote endpoint at a particular point in time.</t>
					</list></t>  
					<t>Furthermore it is assumed that if the current set of parameters is maintained that the indication of the mechanism for the trigger of switching sources (e.g. loudest source, round robin) is not possible because the Consumer only chooses captures and not sources. If it's purely up to the provider then this information would be superfluous. It is proposed to capture this:<list hangIndent="8" style="hanging">
        				<t>The trigger (or policy) that decides when a source is present is up to the provider. The ability to provide detailed information about sources is for further study.</t>
					</list></t>  
		</section>	
		
		<section title="MCU behaviour">
			<t>When a CLUE endpoint is acting as a MCU it implies the need for an advertisement aggregation function. That is the endpoint receives CLUE advertisements from multiple endpoints uses this information, its media processing capabilities and any policy information to form advertisements to the other endpoints.</t>
			<t>Contributor's note: TBD I think there needs to be a discussion here about that source information is lost. How individual attributes are affected. i.e. it may be possible to simply aggregate language information but not so simple when there's different spatial information. Also need to consider capture encodings.</t>
		
		</section>
	  </section>
	</section>
		
    <section anchor="Acknowledgements" title="Acknowledgements">
     <t>This template was derived from an initial version written by Pekka
     Savola and contributed by him to the xml2rfc project.</t>
	</section>
	
   <!-- Possibly a 'Contributors' section ... -->

   <section anchor="IANA" title="IANA Considerations">
     <t>It is not expected that the proposed changes present the need for any IANA registrations.</t>
   </section>

   <section anchor="Security" title="Security Considerations">
     <t>It is not expected that the proposed changes present any addition security issues to the current framework.</t>
	</section>
	
 </middle>

 <!--  *****BACK MATTER ***** -->

 <back>
   <!-- References split into informative and normative -->

   <!-- There are 2 ways to insert reference entries from the citation libraries:
    1. define an ENTITY at the top, and use "ampersand character"RFC2629; here (as shown)
    2. simply use a PI "less than character"?rfc include="reference.RFC.2119.xml"?> here
       (for I-Ds: include="reference.I-D.narten-iana-considerations-rfc2434bis.xml")
    Both are cited textually in the same manner: by using xref elements.
    If you use the PI option, xml2rfc will, by default, try to find included files in the same
    directory as the including file. You can also define the XML_LIBRARY environment variable
    with a value containing a set of directories to search.  These can be either in the local
    filing system or remote ones accessed by http (http://domain/dir/... ).-->

   <references title="Normative References">
     <!--?rfc include="http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml"?-->

	 <?rfc include="reference.RFC.2119.xml"?>

   </references>

   <references title="Informative References">
     <!-- Here we use entities that we defined at the beginning. -->

	 <?rfc include="reference.RFC.2629.xml"?>
	 
	 <?rfc include="reference.RFC.4575.xml"?>

	 <?rfc include="reference.RFC.6501.xml"?>
	 
	 <?rfc include="reference.RFC.6503.xml"?>
	 
	 <?rfc include="reference.I-D.ietf-clue-framework.xml"?>
	 
	 <?rfc include="reference.I-D.ietf-clue-telepresence-use-cases.xml"?>
	 
	 <?rfc include="reference.I-D.ietf-clue-telepresence-requirements.xml"?>
	 
	 <?rfc include="reference.I-D.groves-clue-capture-attr.xml"?>
	 
	 
<!--	  <reference anchor="I-D.ietf-clue-framework">
       <front>
         <title>Framework for Telepresence Multi-Streams</title>

         <author initials="A" surname="Romanow">
           <organization></organization>
         </author>
		 <author initials="M" surname="Duckworth">
           <organization></organization>
         </author>
		 <author initials="A" surname="Pepperell">
           <organization></organization>
         </author>
		  <author initials="B" surname="Baldino">
           <organization></organization>
         </author>
         <date year="draft-ietf-clue-framework-09 (work in progress)" />
       </front>
     </reference> -->
	</references>

   <!-- Change Log

v00 2006-03-15  EBD   Initial version

v01 2006-04-03  EBD   Moved PI location back to position 1 -
                     v3.1 of XMLmind is better with them at this location.
v02 2007-03-07  AH    removed extraneous nested_list attribute,
                     other minor corrections
v03 2007-03-09  EBD   Added comments on null IANA sections and fixed heading capitalization.
                     Modified comments around figure to reflect non-implementation of
                     figure indent control.  Put in reference using anchor="DOMINATION".
                     Fixed up the date specification comments to reflect current truth.
v04 2007-03-09 AH     Major changes: shortened discussion of PIs,
                     added discussion of rfc include.
v05 2007-03-10 EBD    Added preamble to C program example to tell about ABNF and alternative 
                     images. Removed meta-characters from comments (causes problems).

v06 2010-04-01 TT     Changed ipr attribute values to latest ones. Changed date to
                     year only, to be consistent with the comments. Updated the 
                     IANA guidelines reference from the I-D to the finished RFC.  -->
 </back>
</rfc>

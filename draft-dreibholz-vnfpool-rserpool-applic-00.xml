<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">

<?rfc toc="yes"?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>
<?rfc strict="no" ?>
<?rfc symrefs="no" ?>
<!-- <?rfc sortrefs="yes" ?>  -->

<rfc category="info" ipr="trust200902" docName="draft-dreibholz-vnfpool-rserpool-applic-00.txt">

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>


<front>

<title abbrev="RSerPool for VNFPOOL">The Applicability of Reliable Server Pooling&nbsp;(RSerPool) for Virtual Network Function Resource Pooling&nbsp;(VNFPOOL)</title>

<!-- ************** THOMAS DREIBHOLZ *************** -->
<author initials="T." surname="Dreibholz" fullname="Thomas Dreibholz">
<organization abbrev="Simula Research Laboratory">Simula Research Laboratory, Network Systems Group</organization>
<address>
<postal>
   <street>Martin Linges vei 17</street>
   <city>1364 Fornebu</city>
   <region>Akershus</region>
   <country>Norway</country>
</postal>
<phone>+47-6782-8200</phone>
<facsimile>+47-6782-8201</facsimile>
<email>dreibh@simula.no</email>
<uri>http://www.iem.uni-due.de/~dreibh/</uri>
</address>
</author>

<!-- ************** MICHAEL TÃœXEN *************** -->
<author initials='M.' surname='Tuexen' fullname='Michael Tuexen'>
<organization abbrev='Muenster Univ. of Appl. Sciences'>
              Muenster University of Applied Sciences</organization>
<address>
    <postal>
        <street>Stegerwaldstrasse 39</street>
        <city>48565 Steinfurt</city>
        <region>Nordrhein-Westfalen</region>
        <country>Germany</country>
    </postal>
    <email>tuexen@fh-muenster.de</email>
    <uri>https://www.fh-muenster.de/fb2/personen/professoren/tuexen/</uri>
</address>
</author>

<!-- ************** MELINDA SHORE *************** -->
<author initials='M.' surname='Shore' fullname='Melinda Shore'>
<organization abbrev='No Mountain Software'>
              No Mountain Software</organization>
<address>
    <postal>
        <street>PO Box 16271</street>
        <city>Two Rivers</city>
        <code>99716</code>
        <region>Alaska</region>
        <country>U.S.A.</country>
    </postal>
    <phone>+1-907-322-9522</phone>
    <email>melinda.shore@nomountain.net</email>
</address>
</author>

<!-- ************** NING ZONG *************** -->
<author initials='N.' surname='Zong' fullname='Ning Zong'>
<organization abbrev='Huawei Technologies'>
              Huawei Technologies</organization>
<address>
    <postal>
        <street>101 Software Avenue</street>
        <city>Nanjing</city>
        <code>210012</code>
        <region>Jiangsu</region>
        <country>China</country>
    </postal>
    <email>zongning@huawei.com</email>
</address>
</author>

<date />
<keyword>Internet-Draft</keyword>

<abstract>
<t>This draft describes the application of Reliable Server Pooling&nbsp;(RSerPool) for Virtual Network Function Resource Pooling&nbsp;(VNFPOOL).</t>
</abstract>

</front>


<middle>


<section title="Introduction">

<!--
<section title="Terminology">
<t>TBD</t>
</section>
-->

<section title="Abbreviations">
<t>
<list style="symbols">
 <t>PE:       Pool Element</t>
 <t>PR:       Pool Registrar</t>
 <t>PU:       Pool User</t>
 <t>RSerPool: Reliable Server Pooling</t>
 <t>SCTP:     Stream Control Transmission Protocol</t>
 <t>VNFPOOL:  Virtual Network Function Resource Pooling</t>
</list>
</t>
</section>


<!--<section title="Conventions">
<t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL"
in this document are to be interpreted as described in
<xref target="RFC2119"/>.</t>
</section>-->

</section>


<section title="Virtual Network Function Resource Pooling">
<t>
 Virtualised Network Function&nbsp;(VNF) (e.g.&nbsp;vFW, vLB) -- as introduced in more detail in <xref target="I-D.zong-vnfpool-problem-statement" /> -- provides the same function as the equivalent network function (e.g.&nbsp;FW, LB), but is deployed as software instances running on general purpose servers via virtualisation platform. The main features of VNF include the following aspects:
 <list style="numbers">
  <t>A service consists of a sequence of topologically distributed VNF instances where the data connections are preferably directly established between the instances.</t>
  <t>There are potentially more factors that cause VNF instance transition or even failure; VNF pool refers to a group of VNF instances providing same network function.</t>
 </list>
</t>
<t>
 Virtualisation technology allows network function virtualisation operators to build a reliable VNF by pooling the underlying resources, such as CPU, storage, networking, etc. to form a cluster of VNF instances. VNF pool refers to a cluster or group of VNF instances providing same network function. Each VNF pool has a Pool Manager&nbsp;(PM) to manage the VNF instance such as instance selection, monitoring, etc. There will be a redundancy mechanism for a reliable PM to achieve reliable VNF. More details on VNF pool can be found in <xref target="I-D.zong-vnfpool-problem-statement" />.
</t>
</section>


<section title="Reliable Server Pooling">

<section title="Introduction">

<t>
<figure anchor="fig:rserpool-overview" align="center">
<artwork><![CDATA[

                      +---------------+
                      |   Pool User   |
                      +---------------+
                              ^
                              | ASAP
                              V
                      +---------------+   ENRP   +---------------+
                      |   Registrar   |<-------->|   Registrar   |
                      +---------------+          +---------------+
                              ^
                              | ASAP
                              V
      +------------------------------------------------------------+
      | +--------------+ +--------------+         +--------------+ |
      | | Pool Element | | Pool Element | ... ... | Pool Element | |
      | +--------------+ +--------------+         +--------------+ |
      | Server Pool                                                |
      +------------------------------------------------------------+

]]></artwork>
</figure>
</t>

<t>
 An overview of the RSerPool framework -- which is defined as RFC in <xref target="RFC5351" /> -- is provided in <xref target="fig:rserpool-overview" />. There are three types of components:
 <list style="symbols">
  <t>Pool Element&nbsp;(PE) denotes a server in a pool. PEs in the same pool provide the same service.</t>
  <t>Pool User&nbsp;(PU) denotes a client using the service of a pool.</t>
  <t>Pool Registrar&nbsp;(PR) is the management component for the pools.</t>
 </list>
 The set of all pools within an operation scope (for example: an organisation, a company or a department) is denoted as handlespace. Clearly, a single PR would be a single point of failure. Therefore, PRs also have to be redundant. Within the handlespace, each pool is identified by a unique pool handle&nbsp;(PH).
</t>
</section>


<section title="Registrar Operations">
<t>
 The PRs of an operation scope synchronise their view of the handlespace by using the Endpoint haNdlespace Redundancy Protocol&nbsp;(ENRP, defined as RFCs in <xref target="RFC5353" />, <xref target="RFC5354" />). In contrast to for instance the Domain Name System&nbsp;(DNS), an operation scope is restricted to a single administrative domain. That is, all of its components are under the control of the same authority (for example: a company). This property leads to small management overhead, which also allows for RSerPool usage on devices having only limited memory and CPU resources (for example: telecommunications equipment). Nevertheless, PEs may be distributed globally to continue their service even in case of localised disasters (like for example an earthquake). Each PR in the operation scope is identified by a PR ID, which is a randomly chosen 32-bit number.
</t>
</section>


<section title="Pool Element Operations">
<t>
 Within their operation scope, the PEs may choose an arbitrary PR to register into a pool by using the Aggregate Server Access Protocol&nbsp;(ASAP, defined as RFCs in <xref target="RFC5352" />, <xref target="RFC5354" />). The registration is performed by using an ASAP_REGISTRATION message. Within its pool, a PE is characterised by its PE ID, which is a randomly chosen 32-bit number. Upon registration at a PR, the chosen PR becomes the Home-PR&nbsp;(PR-H) of the newly registered PE. A PR-H is responsible for monitoring the availability of its PEs by ASAP_ENDPOINT_KEEP_ALIVE messages (to be acknowledged by a PE via an ASAP_ENDPOINT_KEEP_ALIVE_ACK message within a configured timeout). The PR-H propagates the information about its PEs to the other PRs of the operation scope via ENRP_UPDATE messages.
</t>
<t>
 PEs re-register regularly in an interval denoted as registration lifetime and for information updates. Similar to the registration, a re-registration is performed by using another ASAP_REGISTRATION message. PEs may intentionally deregister from the pool by using an ASAP_DEREGISTRATION message. Also like for the registration, the PR-H makes the deregistration known to the other PRs within the operation scope by using an ENRP_UPDATE message.
</t>
</section>


<section title="Takeover Procedure">
<t>
 As soon as a PE detects the failure of its PR-H (that is: its request is not answered within a given timeout), it simply tries another PR of the operation scope for its registration and deregistration requests. However, as a double safeguard, the remaining PRs also negotiate a takeover of the PEs managed by a dead PR. This ensures that each PE again gets a working PR-H as soon as possible. The PRs of an operation scope monitor the availability of each other PR by using ENRP_PRESENCE messages, which are transmitted regularly. If there is no ENRP_PRESENCE within a given timeout, the peer is assumed to be dead and a so-called takeover procedure (see also <xref target="AINA2009" /> for more details) is initiated for the PEs managed by the dead PR: from all PRs having started this takeover procedure, the PR with the highest PR ID takes over the ownership of these PEs. The PEs are informed about being taken over by their new PR-H via an ASAP_ENDPOINT_KEEP_ALIVE with Home-flag set. The PEs are requested to adopt the sender of this Home-flagged message as their new PR-H.
</t>
</section>


<section title="Pool User Operations">

<section title="Handle Resolution and Response">
<t>
 In order to access the service of a pool given by its PH, a PU requests a PE selection from an arbitrary PR of the operation scope, again by using ASAP. This selection procedure is denoted as handle resolution. Upon reception of a so-called ASAP_HANDLE_RESOLUTION message the PR selects the requested list of PE identities and returns them in an ASAP_HANDLE_RESOLUTION_RESPONSE message.
</t>
</section>

<section title="Pool Member Selection Policies">
<t>
 The pool-specific selection rule is denoted as pool member selection policy or shortly as pool policy. Two classes of load distribution policies are supported: non-adaptive and adaptive strategies (a detailed overview is provided by <xref target="Dre2006" />, <xref target="LCN2005" />, <xref target="IJIIDS2010" />, <xref target="IJHIT2008" />). While adaptive strategies base their selections on the current PE state (which requires up-to-date information), non-adaptive algorithms do not need such data. A basic set of adaptive and non-adaptive pool policies is defined as RFC in <xref target="RFC5356" />.
</t>
<t>
 Defined in <xref target="RFC5356" /> are the non-adaptive policies Round Robin&nbsp;(RR), Random&nbsp;(RAND) and Priority&nbsp;(PRIO) as well as the adaptive policies Least Used&nbsp;(LU) and Least Used with Degradation&nbsp;(LUD).
 While RR/RAND select PEs in turn/randomly, PRIO selects one of the PEs having the highest priority. PRIO can for example be used to realise a master/backup PE setup. Only if there are no master PEs left, a backup PE is selected. Round-robin selection is applied among PEs having the same priority.
 LU selects the least-used PE, according to up-to-date application-specific load information. Round robin selection is applied among multiple least-loaded PEs. LUD, which is evaluated by <xref target="ICDS2008-LUD" />, furthermore introduces a load decrement constant that is added to the actual load each time a PE is selected. It is used to compensate inaccurate load states due to delayed updates. An update resets the load to the actual load value.
</t>
</section>

<section title="Handle Resolution and Response">
<t>
 PE may fail, for example due to hardware or network failures. Since there is a certain latency between the actual failure of a PE and the removal of its entry from the handlespace -- depending on the interval and timeout for the ASAP_ENDPOINT_KEEP_ALIVE monitoring -- the PUs may report unreachable PEs to a PR by using an ASAP_ENDPOINT_UNREACHABLE message. A PR locally counts these reports for each PE and when reaching the threshold MAX-BAD-PE-REPORT (default is 3, as defined in the RFC <xref target="RFC5352" />), the PR may decide to remove the PE from the handlespace. The counter of a PE is reset upon its re-registration. More details on this threshold and guidelines for its configuration can be found in <xref target="IJAIT2009" />.
</t>
</section>

</section>


<section anchor="subsub:Automatic-Configuration" title="Automatic Configuration">
<t>
 RSerPool components need to know the PRs of their operation scope. While it is of course possible to configure a list of PRs into each component, RSerPool also provides an auto-configuration feature: PRs may send so-called announces, that is, ASAP_ANNOUNCE and ENRP_PRESENCE messages which are regularly sent over UDP via IP multicast. Unlike broadcasts, multicast messages can also be transported over routers (at least, this is easily possible within LANs). The announces of the PRs can be heard by the other components, which can maintain a list of currently available PRs. That is, RSerPool components are usually just turned on and everything works automatically.
</t>
</section>


<section anchor="subsub:State-Synchronisation" title="State Synchronisation">
<t>
 RSerPool has been explicitly designed to be application-independent. Therefore, RSerPool has not intended to define special state synchronisation mechanisms for RSerPool-based applications. Such state synchronisation mechanisms are considered as tasks of the applications themselves. However, RSerPool defines two mechanisms to at least support the implementation of more sophisticated strategies: Cookies and Businesss Cards. Details on these mechanisms can also be found in Subsection 3.9.5 of <xref target="Dre2006" />.
</t>

<section title="Cookies">
<t>
 ASAP provides the mechanism of Client-Based State Sharing as introduced in <xref target="LCN2002" />. Whenever useful, the PE may package its state in form of a state cookie and send it -- by an ASAP_COOKIE message -- to the PU. The PU stores the latest state cookie received from the PE. Upon PE failure, this stored cookie is sent in an ASAP_COOKIE_ECHO to the newly chosen PE. This PE may then restore the state. A shared secret known by all PEs of a pool may be used to protect the state from being manipulated or read by the PU.
</t>
<t>
 While Client-Based State Sharing is very simple, it may be inefficient when the state changes too frequently, is too large (the size limit of an ASAP_COOKIE/ASAP_COOKIE_ECHO is 64&nbsp;KiB) or if it must be prevented that a PU sends a state cookie to multiple PEs in order to duplicate its sessions.
</t>
</section>

<section title="Businesss Cards">
<t>
 Depending on the application, there may be constraints restricting the set of PEs usable for failover. The ASAP_BUSINESS_CARD message is used to inform peer components about such constraints.
</t>
<t>
 The first case to use a Business Card is if only a restricted set of PEs in the pool may be used for failover. For example, in a large pool, each PE can share its complete set of session states with a few other PEs only. This keeps the system scalable. That is, a PE in a pool of n servers does not have to synchronise all session states with the other n-1 PEs. In this case, a PE has to tell its PU the set of PE identities being candidates for a failover using an ASAP_BUSINESS_CARD message. A PE may update the list of possible failover candidates at any time by sending another Business Card. The PU has to store the latest list of failover candidates. Of course, if a failover becomes necessary, the PU has to select from this list using the appropriate pool policy -- instead of performing the regular PE selection by handle resolution at a PR. Therefore, some literature also denotes the Business Card by the more expressive term "last will".
</t>
<t>
 In symmetric scenarios, where a PU is also a PE of another pool, the PU has to tell this fact to its PE. This is realised by sending an ASAP_BUSINESS_CARD message to the PE, providing the PH of its pool. Optionally, also specific PE identities for failover may be provided. The format remains the same as explained in the previous paragraph. If the PE detects a failure of its PU, the PE may -- now in the role of a PU -- use the provided PH for a handle resolution to find a new PE or use the provided PE identities to select one. After that, it can perform a failover to that PE.
</t>
</section>

</section>


<section title="Protocol Stack">
<t>
 The protocol stack of a PR provides ENRP and ASAP services to PRs and PEs/PUs respectively. But between PU and PE, ASAP provides a Session Layer protocol in the OSI model. From the perspective of the Application Layer, the PU side establishes a session with a pool. ASAP takes care of selecting a PE of the pool, initiating and maintaining the underlying transport connection and triggering a failover procedure when the PE becomes unavailable.
</t>
<t>
  The Transport Layer protocol is by default SCTP (as defined in <xref target="RFC4960" />) -- except for the UDP-based automatic configuration announces (see <xref target="subsub:Automatic-Configuration" />) -- over possibly multi-homed IPv4 and/or IPv6. SCTP has been chosen due to its support of multi-homing and its reliability features (see also <xref target="Dre2012" />).
</t>
</section>


<section title="Extensions">
<t>
 A couple of extensions to RSerPool are existing:
 Handle Resolution Option defined in <xref target="I-D.dreibholz-rserpool-asap-hropt" /> improves the PE selection by letting the PU tell the PR its required number of PEs to be selected. ENRP Takeover Suggestion introduced in <xref target="I-D.dreibholz-rserpool-enrp-takeover" /> ensures load balancing among PRs. <xref target="I-D.dreibholz-rserpool-delay" /> defines a delay-sensitive pool policy. <xref target="RFC5525" /> defines an SNMP MIB for RSerPool.
</t>
</section>


<section title="Reference Implementation and Deployment">
<t>
 RSPLIB is the Open Source reference implementation of RSerPool. It is currently -- as of October 2013 -- available for Linux, FreeBSD, MacOS and Solaris. It is actively maintained. Particularly, it is also included in Ubuntu Linux as well as in the FreeBSD ports collection. RSPLIB can be downloaded from <xref target="RSerPoolPage" />. Further details on the implementation are available in <xref target="Dre2006" />, <xref target="Globecom2010-Demo" />.
</t>
<t>
  RSerPool with RSPLIB is deployed in a couple of Open Source projects, including the SimProcTC Simulation Processing Tool-Chain for distributing simulation runs in a compute pool (see  <xref target="OMNeTWorkshop2008" /> as well as
  the simulation run distribution project explained in <xref target="Dre2012" /> for a practical example) as well as for service infrastructure management in the NorNet Core research testbed (see <xref target="PAMS2013-NorNet" />).
</t>
</section>


</section>


<section title="Usage of Reliable Server Pooling">
<t>
 **** TO BE DISCUSSED! ****
</t>

<t>
 The following features of RSerPool can be used for VNFPOOL:
 <list style="symbols">
  <t>Pool management.</t>
  <t>PE seclection with pool policies.</t>
  <t>Session management with help of ASAP_BUSINESS_CARD.</t>
 </list>

 The following features have to be added to RSerPool itself:
 <list style="symbols">
  <t>Support of TCP including MPTCP as additional/alternative transport protocols.</t>
  <t>Possibly add some special pool policies?</t>
  <t>See also <xref target="I-D.dreibholz-rserpool-nextgen-ideas" /> for ideas on a next generation of RSerPool.</t>
 </list>

 The following features have to be provided outside of RSerPool:
 <list style="symbols">
  <t>State synchronisation for VNFPOOL.</t>
  <t>Pool Manager functionality as an RSerPool-based service.</t>
 </list>

</t>
</section>


<section title="Security Considerations">
<t>
 Security considerations for RSerPool can be found in <xref target="RFC5355" />. Furthermore, <xref target="IJIIDS2010" /> examines the robustness of RSerPool systems against attacks.
</t>
</section>


<section title="IANA Considerations">
<t>
 This document introduces no additional considerations for IANA.
</t>
</section>


<section title="Testbed Platform">
<t>
 A large-scale and realistic Internet testbed platform with support for Reliable Server Pooling and the underlying SCTP protocol is NorNet. A description of and introduction to NorNet is provided in
 <xref target="PAMS2013-NorNet" />, <xref target="NNUW1-Dreibholz-NorNetCore-Introduction" />, <xref target="NNUW1-Dreibholz-NorNetCore-Tutorial" />. Further information can be found on the project website <xref target="NorNet-Website" /> at http://www.nntb.no.
</t>
</section>


<section title="Acknowledgments">
<t>
   The authors would like to thank
INSERT_NAMES_HERE
   for their friendly support.
</t>
</section>


</middle>


<back>

<references title='Normative References'>
 <!--  <?rfc include="reference.RFC.2119" ?> -->

 <?rfc include="reference.RFC.4960" ?>
 <?rfc include="reference.RFC.5351" ?>
 <?rfc include="reference.RFC.5352" ?>
 <?rfc include="reference.RFC.5353" ?>
 <?rfc include="reference.RFC.5354" ?>
 <?rfc include="reference.RFC.5355" ?>
 <?rfc include="reference.RFC.5356" ?>
 <?rfc include="reference.RFC.5525" ?>

 <?rfc include="reference.I-D.dreibholz-rserpool-asap-hropt" ?>
 <?rfc include="reference.I-D.dreibholz-rserpool-delay" ?>
 <?rfc include="reference.I-D.dreibholz-rserpool-enrp-takeover" ?>
 <?rfc include="reference.I-D.zong-vnfpool-problem-statement" ?>
 <?rfc include="reference.I-D.dreibholz-rserpool-nextgen-ideas" ?>

</references>

<references title='Informative References'>
 <?rfc include="reference.RSerPoolPage" ?>
 <?rfc include="Dre2006" ?>
 <?rfc include="LCN2002" ?>
 <?rfc include="LCN2005" ?>
 <?rfc include="IJHIT2008" ?>
 <?rfc include="ICDS2008-LUD" ?>
 <?rfc include="AINA2009" ?>
 <?rfc include="IJAIT2009" ?>
 <?rfc include="IJIIDS2010" ?>
 <?rfc include="Globecom2010-Demo" ?>
 <?rfc include="OMNeT++Workshop2008" ?>

 <?rfc include="Dre2012" ?>

 <?rfc include="reference.NorNet-Website" ?>
 <?rfc include="PAMS2013-NorNet" ?>
 <?rfc include="NNUW1-Dreibholz-NorNetCore-Introduction" ?>
 <?rfc include="NNUW1-Dreibholz-NorNetCore-Tutorial" ?>
</references>

</back>

</rfc>

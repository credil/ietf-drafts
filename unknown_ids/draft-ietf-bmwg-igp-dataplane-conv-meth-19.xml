<?xml version="1.0" encoding="US-ASCII"?>
<!-- 
# vim sw=4:sta:
-->
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?rfc strict="yes" ?>
<!-- give errors regarding ID-nits and DTD validation -->
<!-- control the table of contents (ToC) -->
<?rfc toc="yes"?>
<!-- generate a ToC -->
<!--<?rfc tocdepth="4"?>-->
<!-- the number of levels of subsections in ToC. default: 3 -->
<!-- control references -->
<?rfc symrefs="yes"?>
<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc sortrefs="yes" ?>
<!-- sort the reference entries alphabetically -->
<!-- control vertical white space 
     (using these PIs as follows is recommended by the RFC Editor) -->
<?rfc compact="yes" ?>
<!-- do not start each main section on a new page -->
<?rfc subcompact="no" ?>
<!-- keep one blank line between list items -->
<!-- end of list of popular I-D processing instructions -->
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<rfc category="info" docName="draft-ietf-bmwg-igp-dataplane-conv-meth-19"
     ipr="pre5378Trust200902">
  <front>
    <title abbrev="IGP Convergence Benchmark Methodology">Benchmarking
    Methodology for Link-State IGP Data Plane Route Convergence</title>

    <author fullname="Scott Poretsky" initials="S" surname="Poretsky">
      <organization>Allot Communications</organization>

      <address>
        <postal>
          <street>67 South Bedford Street, Suite 400</street>

          <city>Burlington</city>

          <region>MA</region>

          <code>01803</code>

          <country>USA</country>
        </postal>

        <phone>+ 1 508 309 2179</phone>

        <email>sporetsky@allot.com</email>
      </address>
    </author>

    <author fullname="Brent Imhoff" initials="B" surname="Imhoff">
      <organization>Juniper Networks</organization>

      <address>
        <postal>
          <street>1194 North Mathilda Ave</street>

          <city>Sunnyvale</city>

          <region>CA</region>

          <code>94089</code>

          <country>USA</country>
        </postal>

        <phone>+ 1 314 378 2571</phone>

        <email>bimhoff@planetspork.com</email>
      </address>
    </author>

    <author fullname="Kris Michielsen" initials="K" surname="Michielsen">
      <organization>Cisco Systems</organization>

      <address>
        <postal>
          <street>6A De Kleetlaan</street>

          <city>Diegem</city>

          <region>BRABANT</region>

          <code>1831</code>

          <country>Belgium</country>
        </postal>

        <email>kmichiel@cisco.com</email>
      </address>
    </author>

    <date month="October" year="2009" />

    <area>Benchmarking Working Group</area>

    <abstract>
      <t>This document describes the methodology for benchmarking Link-State
      Interior Gateway Protocol (IGP) Route Convergence. The methodology is to
      be used for benchmarking IGP convergence time through externally
      observable (black box) data plane measurements. The methodology can be
      applied to any link-state IGP, such as ISIS and OSPF.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction and Scope">
      <t>This document describes the methodology for benchmarking Link-State
      Interior Gateway Protocol (IGP) convergence. The motivation and
      applicability for this benchmarking is described in <xref
      target="Po09a"></xref>. The terminology to be used for this benchmarking
      is described in <xref target="Po09t"></xref>.</t>

      <t>IGP convergence time is measured on the data plane at the Tester by
      observing packet loss through the DUT. All factors contributing to
      convergence time are accounted for by measuring on the data plane, as
      discussed in <xref target="Po09a"></xref>. The test cases in this
      document are black-box tests that emulate the network events that cause
      convergence, as described in <xref target="Po09a"></xref>.</t>

      <t>The methodology described in this document can be applied to IPv4 and
      IPv6 traffic and link-state IGPs such as ISIS <xref
      target="Ca90"></xref><xref target="Ho08"></xref>, OSPF <xref
      target="Mo98"></xref><xref target="Co08"></xref>, and others.</t>
    </section>

    <section title="Existing Definitions">
      <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
      "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
      document are to be interpreted as described in BCP 14, RFC 2119 <xref
      target="Br97"></xref>. RFC 2119 defines the use of these key words to
      help make the intent of standards track documents as clear as possible.
      While this document uses these keywords, this document is not a
      standards track document.</t>

      <t>This document uses much of the terminology defined in <xref
      target="Po09t"></xref> and uses existing terminology defined in other
      BMWG work. Examples include, but are not limited to:</t>

      <texttable anchor="table_terminology" style="none" suppress-title="true">
        <ttcol align="left" width="50%"></ttcol>

        <ttcol align="left"></ttcol>

        <c>Throughput</c>

        <c>[Ref.<xref target="Br91"></xref>, section 3.17]</c>

        <c>Device Under Test (DUT)</c>

        <c>[Ref.<xref target="Ma98"></xref>, section 3.1.1]</c>

        <c>System Under Test (SUT)</c>

        <c>[Ref.<xref target="Ma98"></xref>, section 3.1.2]</c>

        <c>Out-of-order Packet</c>

        <c>[Ref.<xref target="Po06"></xref>, section 3.3.2]</c>

        <c>Duplicate Packet</c>

        <c>[Ref.<xref target="Po06"></xref>, section 3.3.3]</c>

        <c>Stream</c>

        <c>[Ref.<xref target="Po06"></xref>, section 3.3.2]</c>

        <c>Loss Period</c>

        <c>[Ref.<xref target="Ko02"></xref>, section 4]</c>
      </texttable>
    </section>

    <section anchor="test_topologies" title="Test Topologies">
      <section title="Test topology for local changes">
        <t>Figure <xref format="counter" target="local_nlb"></xref> shows the
        test topology to measure IGP convergence time due to local Convergence
        Events such as Local Interface failure (<xref
        target="testcase_local_link_failure"></xref>), layer 2 session failure
        (<xref target="testcase_layer2_session_failure"></xref>), and IGP
        adjacency failure (<xref target="testcase_igp_adj_failure"></xref>).
        This topology is also used to measure IGP convergence time due to the
        route withdrawal (<xref target="testcase_route_withdrawal"></xref>),
        and route cost change (<xref
        target="testcase_route_cost_change"></xref>) Convergence Events. IGP
        adjancencies MUST be established between Tester and DUT, one on the
        Preferred Egress Interface and one on the Next-Best Egress Interface.
        For this purpose the Tester emulates two routers, each establishing
        one adjacency with the DUT. An IGP adjacency SHOULD be established on
        the Ingress Interface between Tester and DUT.</t>

        <figure align="center" anchor="local_nlb"
                title="IGP convergence test topology for local changes">
          <artwork><![CDATA[
---------       Ingress Interface         ----------
|       |<--------------------------------|        |
|       |                                 |        |
|       |    Preferred Egress Interface   |        |
|  DUT  |-------------------------------->| Tester |
|       |                                 |        |
|       |-------------------------------->|        |
|       |    Next-Best Egress Interface   |        |
---------                                 ----------
]]></artwork>
        </figure>
      </section>

      <section title="Test topology for remote changes">
        <t>Figure <xref format="counter" target="remote_nlb"></xref> shows the
        test topology to measure IGP convergence time due to Remote Interface
        failure (<xref target="testcase_remote_link_failure"></xref>). In this
        topology the two routers R1 and R2 are considered System Under Test
        (SUT) and SHOULD be identically configured devices of the same model.
        IGP adjancencies MUST be established between Tester and SUT, one on
        the Preferred Egress Interface and one on the Next-Best Egress
        Interface. For this purpose the Tester emulates one or two routers. An
        IGP adjacency SHOULD be established on the Ingress Interface between
        Tester and SUT. In this topology there is a possibility of a transient
        microloop between R1 and R2 during convergence.</t>

        <figure align="center" anchor="remote_nlb"
                title="IGP convergence test topology for remote changes">
          <artwork><![CDATA[
         ------                      ----------
         |    |  Preferred           |        |
------   | R2 |--------------------->|        |
|    |-->|    |  Egress Interface    |        |
|    |   ------                      |        |
| R1 |                               | Tester |
|    |           Next-Best           |        | 
|    |------------------------------>|        | 
------           Egress Interface    |        |
   ^                                 ----------
   |                                     |
   ---------------------------------------
               Ingress Interface
]]></artwork>
        </figure>
      </section>

      <section title="Test topology for local ECMP changes">
        <t>Figure <xref format="counter" target="local_lb"></xref> shows the
        test topology to measure IGP convergence time due to local Convergence
        Events with members of an Equal Cost Multipath (ECMP) set (<xref
        target="testcase_local_lb_link_failure"></xref>). In this topology,
        the DUT is configured with each egress interface as a member of a
        single ECMP set and the Tester emulates N next-hop routers, one router
        for each member. IGP adjancencies MUST be established between Tester
        and DUT, one on each member of the ECMP set. For this purpose each of
        the N routers emulated by the Tester establishes one adjacency with
        the DUT. An IGP adjacency SHOULD be established on the Ingress
        Interface between Tester and DUT.</t>

        <figure align="center" anchor="local_lb"
                title="IGP convergence test topology for local ECMP change">
          <artwork><![CDATA[
---------       Ingress Interface         ----------
|       |<--------------------------------|        |
|       |                                 |        |
|       |     ECMP set interface 1        |        |
|       |-------------------------------->|        |
|  DUT  |               .                 | Tester |
|       |               .                 |        |
|       |               .                 |        |
|       |-------------------------------->|        |
|       |     ECMP set interface N        |        |
---------                                 ----------
]]></artwork>
        </figure>
      </section>

      <section title="Test topology for remote ECMP changes">
        <t>Figure <xref format="counter" target="remote_lb"></xref> shows the
        test topology to measure IGP convergence time due to remote
        Convergence Events with members of an Equal Cost Multipath (ECMP) set
        (<xref target="testcase_remote_lb_link_failure"></xref>). In this
        topology the two routers R1 and R2 are considered System Under Test
        (SUT) and MUST be identically configured devices of the same model.
        Router R1 is configured with each egress interface as a member of a
        single ECMP set and the Tester emulates N next-hop routers, one router
        for each member. IGP adjancencies MUST be established between Tester
        and SUT, one on each egress interface of SUT. For this purpose each of
        the N routers emulated by the Tester establishes one adjacency with
        the SUT. An IGP adjacency SHOULD be established on the Ingress
        Interface between Tester and SUT. In this topology there is a
        possibility of a transient microloop between R1 and R2 during
        convergence.</t>

        <figure align="center" anchor="remote_lb"
                title="IGP convergence test topology for remote ECMP convergence">
          <artwork><![CDATA[
                          ------     ----------
                          |    |     |        |
------      ECMP set      | R2 |---->|        |
|    |------------------->|    |     |        |
|    |      Interface 1   ------     |        |
|    |                               |        | 
|    |      ECMP set interface 2     |        |
| R1 |------------------------------>| Tester |
|    |               .               |        |
|    |               .               |        |
|    |               .               |        |                             
|    |------------------------------>|        | 
------      ECMP set interface N     |        |
   ^                                 ----------
   |                                     |
   ---------------------------------------
               Ingress Interface
]]></artwork>
        </figure>
      </section>

      <section title="Test topology for Parallel Link changes">
        <t>Figure <xref format="counter" target="local_parallel"></xref> shows
        the test topology to measure IGP convergence time due to local
        Convergence Events with members of a Parallel Link (<xref
        target="testcase_parallel_link_failure"></xref>). In this topology,
        the DUT is configured with each egress interface as a member of a
        Parallel Link and the Tester emulates the single next-hop router. IGP
        adjancencies MUST be established on all N members of the Parallel Link
        between Tester and DUT. For this purpose the router emulated by the
        Tester establishes N adjacencies with the DUT. An IGP adjacency SHOULD
        be established on the Ingress Interface between Tester and DUT.</t>

        <figure align="center" anchor="local_parallel"
                title="IGP convergence test topology for Parallel Link changes">
          <artwork><![CDATA[
---------       Ingress Interface         ----------
|       |<--------------------------------|        |
|       |                                 |        |
|       |     Parallel Link Interface 1   |        |
|       |-------------------------------->|        |
|  DUT  |               .                 | Tester |
|       |               .                 |        |
|       |               .                 |        |
|       |-------------------------------->|        |
|       |     Parallel Link Interface N   |        |
---------                                 ----------
]]></artwork>
        </figure>
      </section>
    </section>

    <section anchor="section_conv_and_loc"
             title="Convergence Time and Loss of Connectivity Period">
      <t>Two concepts will be highlighted in this section: convergence time
      and loss of connectivity period.</t>

      <t>The Route Convergence <xref target="Po09t"></xref> time indicates the
      period in time between the Convergence Event Instant <xref
      target="Po09t"></xref> and the instant in time the DUT is ready to
      forward traffic for a specific route on its Next-Best Egress Interface
      and maintains this state for the duration of the Sustained Convergence
      Validation Time <xref target="Po09t"></xref>. To measure Route
      Convergence time, the Convergence Event Instant and the traffic received
      from the Next-Best Egress Interface need to be observed.</t>

      <t>The Route Loss of Connectivity Period <xref target="Po09t"></xref>
      indicates the time during which traffic to a specific route is lost
      following a Convergence Event until Full Convergence <xref
      target="Po09t"></xref> completes. This Route Loss of Connectivity Period
      can consist of one or more Loss Periods <xref target="Ko02"></xref>. For
      the testcases described in this document it is expected to have a single
      Loss Period. To measure Route Loss of Connectivity Period, the traffic
      received from the Preferred Egress Interface and the traffic received
      from the Next-Best Egress Interface need to be observed.</t>

      <t>The Route Loss of Connectivity Period is most important since that
      has a direct impact on the network user's application performance.</t>

      <t>In general the Route Convergence time is larger than or equal to the
      Route Loss of Connectivity Period. Depending on which Convergence Event
      occurs and how this Convergence Event is applied, traffic for a route
      may still be forwarded over the Preferred Egress Interface after the
      Convergence Event Instant, before converging to the Next-Best Egress
      Interface. In that case the Route Loss of Connectivity Period is shorter
      than the Route Convergence time.</t>

      <t>At least one condition needs to be fulfilled for Route Convergence
      time to be equal to Route Loss of Connectivity Period. The condition is
      that the Convergence Event causes an instantaneous traffic loss for the
      measured route. A fiber cut on the Preferred Egress Interface is an
      example of such a Convergence Event.</t>

      <t>A second condition applies to Route Convergence time measurements
      based on Connectivity Packet Loss <xref target="Po09t"></xref>. This
      second condition is that there is only a single Loss Period during Route
      Convergence. For the testcases described in this document this is
      expected to be the case.</t>

      <section anchor="conv_time_no_instant_loss"
               title="Convergence Events without instant traffic loss">
        <t>To measure convergence time benchmarks for Convergence Events
        caused by a Tester, such as an IGP cost change, the Tester MAY start
        to discard all traffic received from the Preferred Egress Interface at
        the Convergence Event Instant, or MAY separately observe packets
        received from the Preferred Egress Interface prior to the Convergence
        Event Instant. This way these Convergence Events can be treated the
        same as Convergence Events that cause instantaneous traffic loss.</t>

        <t>To measure convergence time benchmarks without instantaneous
        traffic loss (either real or induced by the Tester) at the Convergence
        Event Instant, such as a reversion of a link failure Convergence
        Event, the Tester SHALL only observe packet statistics on the
        Next-Best Egress Interface. If using the Rate-Derived method to
        benchmark convergence times for such Convergence Events, the Tester
        MUST collect a timestamp at the Convergence Event Instant. If using a
        loss-derived method to benchmark convergence times for such
        Convergence Events, the Tester MUST measure the period in time between
        the Start Traffic Instant and the Convergence Event Instant. To
        measure this period in time the Tester can collect timestamps at the
        Start Traffic Instant and the Convergence Event Instant.</t>

        <t>The Convergence Event Instant together with the receive rate
        observations on the Next-Best Egress Interface allow to derive the
        convergence time benchmarks using the Rate-Derived Method <xref
        target="Po09t"></xref>.</t>

        <t>By observing lost packets on the Next-Best Egress Interface only,
        the observed packet loss is the number of lost packets between Traffic
        Start Instant and Convergence Recovery Instant. To measure convergence
        times using a loss-derived method, packet loss between the Convergence
        Event Instant and the Convergence Recovery Instant is needed. The time
        between Traffic Start Instant and Convergence Event Instant must be
        accounted for. An example may clarify this.</t>

        <t>Figure <xref format="counter"
        target="non_instant_loss_example"></xref> illustrates a Convergence
        Event without instantaneous traffic loss for all routes. The top graph
        shows the Forwarding Rate over all routes, the bottom graph shows the
        Forwarding Rate for a single route Rta. Some time after the
        Convergence Event Instant, Forwarding Rate observed on the Preferred
        Egress Interface starts to decrease. In the example, route Rta is the
        first route to experience packet loss at time Ta. Some time later, the
        Forwarding Rate observed on the Next-Best Egress Interface starts to
        increase. In the example, route Rta is the first route to complete
        convergence at time Ta'.</t>

        <figure align="left" anchor="non_instant_loss_example">
          <artwork align="center"><![CDATA[
     ^        
Fwd  |
Rate |-------------                    ............
     |             \                  .
     |              \                .
     |               \              .
     |                \            .
     |.................-.-.-.-.-.-.----------------
     +----+-------+---------------+----------------->
     ^    ^       ^               ^             time
    T0   CEI      Ta              Ta'

     ^        
Fwd  |
Rate |-------------               .................
Rta  |            |               .
     |            |               .
     |.............-.-.-.-.-.-.-.-.----------------
     +----+-------+---------------+----------------->
     ^    ^       ^               ^             time
    T0   CEI      Ta              Ta'

     Preferred Egress Interface: ---
     Next-Best Egress Interface: ...
]]></artwork>

          <postamble>With T0 the Start Traffic Instant; CEI the Convergence
          Event Instant; Ta the time instant traffic loss for route Rta
          starts; Ta' the time instant traffic loss for route Rta
          ends.</postamble>
        </figure>

        <t>If only packets received on the Next-Best Egress Interface are
        observed, the duration of the packet loss period for route Rta can be
        calculated from the received packets as in Equation 1. Since the
        Convergence Event Instant is the start time for convergence time
        measurement, the period in time between T0 and CEI needs to be
        subtracted from the calculated result to become the convergence time,
        as in Equation 2.</t>

        <figure align="center">
          <artwork><![CDATA[
Next-Best Egress Interface packet loss period
    = (packets transmitted
        - packets received from Next-Best Egress Interface) / tx rate
    = Ta' - T0
]]></artwork>

          <postamble>Equation 1</postamble>
        </figure>

        <figure align="center">
          <artwork><![CDATA[
convergence time
    = Next-Best Egress Interface packet loss period - (CEI - T0)
    = Ta' - CEI
]]></artwork>

          <postamble>Equation 2</postamble>
        </figure>
      </section>

      <section title="Loss of Connectivity">
        <t>Route Loss of Connectivity Period SHOULD be measured using the
        Route-Specific Loss-Derived Method. Since the start instant and end
        instant of the Route Loss of Connectivity Period can be different for
        each route, these can not be accurately derived by only observing
        global statistics over all routes. An example may clarify this.</t>

        <t>Following a Convergence Event, route Rta is the first route for
        which packet loss starts, the Route Loss of Connectivity Period for
        route Rta starts at time Ta. Route Rtb is the last route for which
        packet loss starts, the Route Loss of Connectivity Period for route
        Rtb starts at time Tb with Tb&gt;Ta.</t>

        <figure anchor="Fig7"
                title="Example Route Loss Of Connectivity Period">
          <artwork align="center"><![CDATA[
     ^
Fwd  |
Rate |--------                       -----------
     |        \                     /
     |         \                   /
     |          \                 /
     |           \               /
     |            ---------------
     +------------------------------------------>
              ^   ^             ^    ^      time
             Ta   Tb           Ta'   Tb'
                               Tb''  Ta''

]]></artwork>
        </figure>

        <t>If the DUT implementation would be such that Route Rta would be the
        first route for which traffic loss ends at time Ta' with Ta'&gt;Tb.
        Route Rtb would be the last route for which traffic loss ends at time
        Tb' with Tb'&gt;Ta'. By using only observing global traffic statistics
        over all routes, the minimum Route Loss of Connectivity Period would
        be measured as Ta'-Ta. The maximum calculated Route Loss of
        Connectivity Period would be Tb'-Ta. The real minimum and maximum
        Route Loss of Connectivity Periods are Ta'-Ta and Tb'-Tb. Illustrating
        this with the numbers Ta=0, Tb=1, Ta'=3, and Tb'=5, would give a LoC
        Period between 3 and 5 derived from the global traffic statistics,
        versus the real LoC Period between 3 and 4.</t>

        <t>If the DUT implementation would be such that route Rtb would be the
        first for which packet loss ends at time Tb'' and route Rta would be
        the last for which packet loss ends at time Ta'', then the minimum and
        maximum Route Loss of Connectivity Periods derived by observing only
        global traffic statistics would be Tb''-Ta, and Ta''-Ta. The real
        minimum and maximum Route Loss of Connectivity Periods are Tb''-Tb and
        Ta''-Ta. Illustrating this with the numbers Ta=0, Tb=1, Ta''=5,
        Tb''=3, would give a LoC Period between 3 and 5 derived from the
        global traffic statistics, versus the real LoC Period between 2 and
        5.</t>

        <t>The two implementation variations in the above example would result
        in the same derived minimum and maximum Route Loss of Connectivity
        Periods when only observing the global packet statistics, while the
        real Route Loss of Connectivity Periods are different.</t>
      </section>
    </section>

    <section title="Test Considerations">
      <section title="IGP Selection">
        <t>The test cases described in Section <xref format="counter"
        target="test_cases"></xref> MAY be used for link-state IGPs, such as
        ISIS or OSPF. The IGP convergence time test methodology is
        identical.</t>
      </section>

      <section title="Routing Protocol Configuration">
        <t>The obtained results for IGP convergence time may vary if other
        routing protocols are enabled and routes learned via those protocols
        are installed. IGP convergence times SHOULD be benchmarked without
        routes installed from other protocols.</t>
      </section>

      <section title="IGP Topology">
        <t>The Tester emulates a single IGP topology. The DUT establishes IGP
        adjacencies with one or more of the emulated routers in this single
        IGP topology emulated by the Tester. See test topology details in
        <xref target="test_topologies"></xref>. The emulated topology SHOULD
        only be advertised on the DUT egress interfaces.</t>

        <t>The number of IGP routes will impact the measured IGP route
        convergence time. To obtain results similar to those that would be
        observed in an operational network, it is RECOMMENDED that the number
        of installed routes and nodes closely approximate that of the network
        (e.g. thousands of routes with tens or hundreds of nodes).</t>

        <t>The number of areas (for OSPF) and levels (for ISIS) can impact the
        benchmark results.</t>
      </section>

      <section title="Timers">
        <t>There are timers that may impact the measured IGP convergence
        times. The benchmark metrics MAY be measured at any fixed values for
        these timers. To obtain results similar to those that would be
        observed in an operational network, it is RECOMMENDED to configure the
        timers with the values as configured in the operational network.</t>

        <t>Examples of timers that may impact measured IGP convergence time
        include, but are not limited to:</t>

        <t><list style="empty">
            <t>Interface failure indication</t>

            <t>IGP hello timer</t>

            <t>IGP dead-interval or hold-timer</t>

            <t>LSA or LSP generation delay</t>

            <t>LSA or LSP flood packet pacing</t>

            <t>SPF delay</t>
          </list></t>
      </section>

      <section title="Interface Types">
        <t>All test cases in this methodology document MAY be executed with
        any interface type. The type of media may dictate which test cases may
        be executed. Each interface type has a unique mechanism for detecting
        link failures and the speed at which that mechanism operates will
        influence the measurement results. All interfaces MUST be the same
        media and Throughput <xref target="Br91"></xref><xref
        target="Br99"></xref> for each test case. All interfaces SHOULD be
        configured as point-to-point.</t>
      </section>

      <section title="Offered Load">
        <t>The Throughput of the device, as defined in <xref
        target="Br91"></xref> and benchmarked in <xref target="Br99"></xref>
        at a fixed packet size, needs to be determined over the preferred path
        and over the next-best path. The Offered Load SHOULD be the minimum of
        the measured Throughput of the device over the primary path and over
        the backup path. The packet size is selectable and MUST be recorded.
        Packet size is measured in bytes and includes the IP header and
        payload.</t>

        <t>The destination addresses for the Offered Load MUST be distributed
        such that all routes or a statistically representative subset of all
        routes are matched and each of these routes is offered an equal share
        of the Offered Load. It is RECOMMENDED to send traffic matching all
        routes, but a statistically representative subset of all routes can be
        used if required.</t>

        <t>In the Remote Interface failure testcases using topologies <xref
        format="counter" target="remote_nlb"></xref> and <xref
        format="counter" target="remote_lb"></xref> there is a possibility of
        a transient microloop between R1 and R2 during convergence. The TTL or
        Hop Limit value of the packets sent by the Tester may influence the
        benchmark measurements since it determines which device in the
        topology may send an ICMP Time Exceeded Message for looped
        packets.</t>

        <t>The duration of the Offered Load MUST be greater than the
        convergence time.</t>
      </section>

      <section title="Measurement Accuracy">
        <t>Since packet loss is observed to measure the Route Convergence
        Time, the time between two successive packets offered to each
        individual route is the highest possible accuracy of any packet loss
        based measurement. When packet jitter is much less than the
        convergence time, it is a negligible source of error and therefore it
        will be ignored here.</t>
      </section>

      <section title="Measurement Statistics">
        <t>The benchmark measurements may vary for each trial, due to the
        statistical nature of timer expirations, cpu scheduling, etc.
        Evaluation of the test data must be done with an understanding of
        generally accepted testing practices regarding repeatability, variance
        and statistical significance of a small number of trials.</t>
      </section>

      <section title="Tester Capabilities">
        <t>It is RECOMMENDED that the Tester used to execute each test case
        has the following capabilities:</t>

        <t><list style="numbers">
            <t>Ability to establish IGP adjacencies and advertise a single IGP
            topology to one or more peers.</t>

            <t>Ability to insert a timestamp in each data packet's IP
            payload.</t>

            <t>An internal time clock to control timestamping, time
            measurements, and time calculations.</t>

            <t>Ability to distinguish traffic load received on the Preferred
            and Next-Best Interfaces <xref target="Po09t"></xref>.</t>

            <t>Ability to disable or tune specific Layer-2 and Layer-3
            protocol functions on any interface(s).</t>
          </list></t>

        <t>The Tester MAY be capable to make non-data plane convergence
        observations and use those observations for measurements. The Tester
        MAY be capable to send and receive multiple traffic Streams <xref
        target="Po06"></xref>.</t>

        <t>Also see Section <xref format="counter"
        target="selection_metrics_methods"></xref> for method-specific
        capabilities.</t>
      </section>
    </section>

    <section anchor="selection_metrics_methods"
             title="Selection of Convergence Time Benchmark Metrics and Methods">
      <t>Different convergence time benchmark methods MAY be used to measure
      convergence time benchmark metrics. The Tester capabilities are
      important criteria to select a specific convergence time benchmark
      method. The criteria to select a specific benchmark method include, but
      are not limited to:</t>

      <texttable anchor="table_tester_capabilities" style="none"
                 suppress-title="true">
        <ttcol align="left" width="50%"></ttcol>

        <ttcol align="left"></ttcol>

        <c>Tester capabilities:</c>

        <c>Sampling Interval, number of Stream statistics to collect</c>

        <c>Measurement accuracy:</c>

        <c>Sampling Interval, Offered Load</c>

        <c>Test specification:</c>

        <c>number of routes</c>

        <c>DUT capabilities:</c>

        <c>Throughput</c>
      </texttable>

      <section title="Loss-Derived Method">
        <section title="Tester capabilities">
          <t>The Offered Load SHOULD consist of a single Stream <xref
          target="Po06"></xref>. If sending multiple Streams, the measured
          packet loss statistics for all Streams MUST be added together.</t>

          <t>In order to verify Full Convergence completion and the Sustained
          Convergence Validation Time, the Tester MUST measure Forwarding Rate
          each Packet Sampling Interval.</t>

          <t>The total number of packets lost between the start of the traffic
          and the end of the Sustained Convergence Validation Time is used to
          calculate the Loss-Derived Convergence Time.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Loss-Derived Method can be used to measure the Loss-Derived
          Convergence Time, which is the average convergence time over all
          routes, and to measure the Loss-Derived Loss of Connectivity Period,
          which is the average Route Loss of Connectivity Period over all
          routes.</t>
        </section>

        <section title="Measurement Accuracy">
          <t>The measurement accuracy of the Loss-Derived Method is equal to
          the time between two consecutive packets to the same route.</t>
        </section>
      </section>

      <section title="Rate-Derived Method">
        <section title="Tester Capabilities">
          <t>The Offered Load SHOULD consist of a single Stream. If sending
          multiple Streams, the measured traffic rate statistics for all
          Streams MUST be added together.</t>

          <t>The Tester measures Forwarding Rate each Sampling Interval. The
          Packet Sampling Interval influences the observation of the different
          convergence time instants. If the Packet Sampling Interval is large
          compared to the time between the convergence time instants, then the
          different time instants may not be easily identifiable from the
          Forwarding Rate observation. The requirements for the Packet
          Sampling Interval are specified in <xref target="Po09t"></xref>. The
          RECOMMENDED value for the Packet Sampling Interval is 10
          milliseconds. The Packet Sampling Interval MUST be reported.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Rate-Derived Method SHOULD be used to measure First Route
          Convergence Time and Full Convergence Time. It SHOULD NOT be used to
          measure Loss of Connectivity Period (see Section <xref
          format="counter" target="section_conv_and_loc"></xref>).</t>
        </section>

        <section title="Measurement Accuracy">
          <t>The measurement accuracy of the Rate-Derived Method for
          transitions that occur for all routes at the same instant is equal
          to the Packet Sampling Interval and for other transitions the
          measurement accuracy is equal to the Packet Sampling Interval plus
          the time between two consecutive packets to the same destination.
          The latter is the case since packets are sent in a particular order
          to all destinations in a stream and when part of the routes
          experience packet loss, it is unknown where in the transmit cycle
          packets to these routes are sent. This uncertainty adds to the
          error.</t>
        </section>
      </section>

      <section title="Route-Specific Loss-Derived Method">
        <section title="Tester Capabilities">
          <t>The Offered Load consists of multiple Streams. The Tester MUST
          measure packet loss for each Stream separately.</t>

          <t>In order to verify Full Convergence completion and the Sustained
          Convergence Validation Time, the Tester MUST measure packet loss
          each Packet Sampling Interval. This measurement at each Packet
          Sampling Interval MAY be per Stream.</t>

          <t>Only the total packet loss measured per Stream at the end of the
          Sustained Convergence Validation Time is used to calculate the
          benchmark metrics with this method.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Route-Specific Loss-Derived Method SHOULD be used to measure
          Route-Specific Convergence Times. It is the RECOMMENDED method to
          measure Route Loss of Connectivity Period.</t>

          <t>Under the conditions explained in Section <xref format="counter"
          target="section_conv_and_loc"></xref>, First Route Convergence Time
          and Full Convergence Time as benchmarked using Rate-Derived Method,
          may be equal to the minimum resp. maximum of the Route-Specific
          Convergence Times.</t>
        </section>

        <section title="Measurement Accuracy">
          <t>The measurement accuracy of the Route-Specific Loss-Derived
          Method is equal to the time between two consecutive packets to the
          same route.</t>
        </section>
      </section>
    </section>

    <section title="Reporting Format">
      <t>For each test case, it is recommended that the reporting tables below
      are completed and all time values SHOULD be reported with resolution as
      specified in <xref target="Po09t"></xref>.</t>

      <texttable anchor="table_report_config" style="headers"
                 suppress-title="true">
        <ttcol align="left" width="50%">Parameter</ttcol>

        <ttcol align="left">Units</ttcol>

        <c>Test Case</c>

        <c>test case number</c>

        <c>Test Topology</c>

        <c>(1, 2, 3, 4, or 5)</c>

        <c>IGP</c>

        <c>(ISIS, OSPF, other)</c>

        <c>Interface Type</c>

        <c>(GigE, POS, ATM, other)</c>

        <c>Packet Size offered to DUT</c>

        <c>bytes</c>

        <c>Offered Load</c>

        <c>packets per second</c>

        <c>IGP Routes advertised to DUT</c>

        <c>number of IGP routes</c>

        <c>Nodes in emulated network</c>

        <c>number of nodes</c>

        <c>Number of Routes measured</c>

        <c>number of routes</c>

        <c>Packet Sampling Interval on Tester</c>

        <c>seconds</c>

        <c>Forwarding Delay Threshold</c>

        <c>seconds</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Timer Values configured on DUT:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Interface failure indication delay</c>

        <c>seconds</c>

        <c>&nbsp;IGP Hello Timer</c>

        <c>seconds</c>

        <c>&nbsp;IGP Dead-Interval or hold-time</c>

        <c>seconds</c>

        <c>&nbsp;LSA Generation Delay</c>

        <c>seconds</c>

        <c>&nbsp;LSA Flood Packet Pacing</c>

        <c>seconds</c>

        <c>&nbsp;LSA Retransmission Packet Pacing</c>

        <c>seconds</c>

        <c>&nbsp;SPF Delay</c>

        <c>seconds</c>
      </texttable>

      <t>Test Details:<list style="empty">
          <t>If the Offered Load matches a subset of routes, describe how this
          subset is selected.</t>

          <t>Describe how the Convergence Event is applied; does it cause
          instantaneous traffic loss or not.</t>
        </list></t>

      <t>Complete the table below for the initial Convergence Event and the
      reversion Convergence Event.</t>

      <texttable anchor="table_report_measurements" style="headers"
                 suppress-title="true">
        <ttcol align="left" width="50%">Parameter</ttcol>

        <ttcol align="left">Units</ttcol>

        <c>Conversion Event</c>

        <c>(initial or reversion)</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Traffic Forwarding Metrics:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Total number of packets offered to DUT</c>

        <c>number of Packets</c>

        <c>&nbsp;Total number of packets forwarded by DUT</c>

        <c>number of Packets</c>

        <c>&nbsp;Connectivity Packet Loss</c>

        <c>number of Packets</c>

        <c>&nbsp;Convergence Packet Loss</c>

        <c>number of Packets</c>

        <c>&nbsp;Out-of-Order Packets</c>

        <c>number of Packets</c>

        <c>&nbsp;Duplicate Packets</c>

        <c>number of Packets</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Convergence Benchmarks:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Rate-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;First Route Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Full Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Loss-Derived Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;Route-Specific Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Route-Specific Convergence Time[n]</c>

        <c>array of seconds</c>

        <c>&nbsp;&nbsp;Minimum R-S Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Maximum R-S Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Median R-S Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Average R-S Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Loss of Connectivity Benchmarks:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Loss-Derived Loss of Connectivity Period</c>

        <c>seconds</c>

        <c>&nbsp;Route-Specific Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Route LoC Period[n]</c>

        <c>array of seconds</c>

        <c>&nbsp;&nbsp;Minimum Route LoC Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Maximum Route LoC Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Median Route LoC Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Average Route LoC Period</c>

        <c>seconds</c>
      </texttable>
    </section>

    <section anchor="test_cases" title="Test Cases">
      <t>It is RECOMMENDED that all applicable test cases be performed for
      best characterization of the DUT. The test cases follow a generic
      procedure tailored to the specific DUT configuration and Convergence
      Event <xref target="Po09t"></xref>. This generic procedure is as
      follows:</t>

      <t><list style="numbers">
          <t>Establish DUT and Tester configurations and advertise an IGP
          topology from Tester to DUT.</t>

          <t>Send Offered Load from Tester to DUT on ingress interface.</t>

          <t>Verify traffic is routed correctly.</t>

          <t>Introduce Convergence Event <xref target="Po09t"></xref>.</t>

          <t>Measure First Route Convergence Time <xref
          target="Po09t"></xref>.</t>

          <t>Measure Full Convergence Time <xref target="Po09t"></xref>.</t>

          <t>Stop Offered Load.</t>

          <t>Measure Route-Specific Convergence Times, Loss-Derived
          Convergence Time, Route LoC Periods, and Loss-Derived LoC Period
          <xref target="Po09t"></xref>.</t>

          <t>Wait sufficient time for queues to drain.</t>

          <t>Restart Offered Load.</t>

          <t>Reverse Convergence Event.</t>

          <t>Measure First Route Convergence Time.</t>

          <t>Measure Full Convergence Time.</t>

          <t>Stop Offered Load.</t>

          <t>Measure Route-Specific Convergence Times, Loss-Derived
          Convergence Time, Route LoC Periods, and Loss-Derived LoC
          Period.</t>
        </list></t>

      <section title="Interface failures">
        <section anchor="testcase_local_link_failure"
                 title="Convergence Due to Local Interface Failure">
          <t>Objective</t>

          <t>To obtain the IGP convergence times due to a Local Interface
          failure event.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is forwarded over Preferred Egress
              Interface.</t>

              <t>Remove link on DUT's Preferred Egress Interface. This is the
              Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on DUT's Preferred Egress Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP convergence time may be influenced by the link
          failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>
        </section>

        <section anchor="testcase_remote_link_failure"
                 title="Convergence Due to Remote Interface Failure">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to a Remote Interface
          failure event.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to SUT using the
              topology shown in Figure 2.</t>

              <t>Send Offered Load from Tester to SUT on ingress
              interface.</t>

              <t>Verify traffic is forwarded over Preferred Egress
              Interface.</t>

              <t>Remove link on Tester's interface <xref
              target="Po09t"></xref> connected to SUT's Preferred Egress
              Interface. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on Tester's interface connected to DUT's
              Preferred Egress Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP convergence time may be influenced by the link
          failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time. This test case may
          produce Stale Forwarding <xref target="Po09t"></xref> due to a
          transient microloop between R1 and R2 during convergence, which may
          increase the measured convergence times and loss of connectivity
          periods.</t>
        </section>

        <section anchor="testcase_local_lb_link_failure"
                 title="Convergence Due to ECMP Member Local Interface Failure">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to a Local Interface link
          failure event of an ECMP Member.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure 3.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is forwarded over the DUT's ECMP member
              interface that will be failed in the next step.</t>

              <t>Remove link on one of the DUT's ECMP member interfaces. This
              is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time measure Out-of-Order Packets
              <xref target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on DUT's ECMP member interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period. At the same time measure Out-of-Order Packets <xref
              target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP Convergence time may be influenced by link
          failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>
        </section>

        <section anchor="testcase_remote_lb_link_failure"
                 title="Convergence Due to ECMP Member Remote Interface Failure">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to a Remote Interface link
          failure event for an ECMP Member.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure 4.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is forwarded over the DUT's ECMP member
              interface that will be failed in the next step.</t>

              <t>Remove link on Tester's interface to R2. This is the
              Convergence Event Trigger.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time measure Out-of-Order Packets
              <xref target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on Tester's interface to R2.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period. At the same time measure Out-of-Order Packets <xref
              target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP convergence time may influenced by the link
          failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time. This test case may
          produce Stale Forwarding <xref target="Po09t"></xref> due to a
          transient microloop between R1 and R2 during convergence, which may
          increase the measured convergence times and loss of connectivity
          periods.</t>
        </section>

        <section anchor="testcase_parallel_link_failure"
                 title="Convergence Due to Parallel Link Interface Failure ">
          <t>Objective</t>

          <t>To obtain the IGP convergence due to a local link failure event
          for a member of a parallel link. The links can be used for data load
          balancing</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure 5.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is forwarded over the parallel link member
              that will be failed in the next step.</t>

              <t>Remove link on one of the DUT's parallel link member
              interfaces. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time measure Out-of-Order Packets
              <xref target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on DUT's Parallel Link member interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period. At the same time measure Out-of-Order Packets <xref
              target="Po06"></xref> and Duplicate Packets <xref
              target="Po06"></xref>.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP convergence time may be influenced by the link
          failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>
        </section>
      </section>

      <section title="Other failures">
        <section anchor="testcase_layer2_session_failure"
                 title="Convergence Due to Layer 2 Session Loss">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to a local layer 2
          loss.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure 1.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Remove Layer 2 session from DUT's Preferred Egress Interface.
              This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore Layer 2 session on DUT's Preferred Egress
              Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP Convergence time may be influenced by the Layer
          2 failure indication time, LSA/LSP delay, LSA/LSP generation time,
          LSA/LSP flood packet pacing, SPF delay, SPF execution time, and
          routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>

          <t>Discussion</t>

          <t>Configure IGP timers such that the IGP adjacency does not time
          out before layer 2 failure is detected.</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the layer 2 session is
          removed. Alternatively the Tester SHOULD record the time the instant
          layer 2 session is removed and traffic loss SHOULD only be measured
          on the Next-Best Egress Interface. For loss-derived benchmarks the
          time of the Start Traffic Instant SHOULD be recorded as well. See
          Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
        </section>

        <section anchor="testcase_igp_adj_failure"
                 title="Convergence Due to Loss of IGP Adjacency">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to loss of an IGP
          Adjacency.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure 1.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Remove IGP adjacency from the Preferred Egress Interface
              while the layer 2 session MUST be maintained. This is the
              Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore IGP session on DUT's Preferred Egress Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP Convergence time may be influenced by the IGP
          Hello Interval, IGP Dead Interval, LSA/LSP delay, LSA/LSP generation
          time, LSA/LSP flood packet pacing, SPF delay, SPF execution time,
          and routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>

          <t>Discussion</t>

          <t>Configure layer 2 such that layer 2 does not time out before IGP
          adjacency failure is detected.</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the IGP adjacency is
          removed. Alternatively the Tester SHOULD record the time the instant
          the IGP adjacency is removed and traffic loss SHOULD only be
          measured on the Next-Best Egress Interface. For loss-derived
          benchmarks the time of the Start Traffic Instant SHOULD be recorded
          as well. See Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
        </section>

        <section anchor="testcase_route_withdrawal"
                 title="Convergence Due to Route Withdrawal">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to route withdrawal.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure 1. The routes that will be withdrawn
              MUST be a set of leaf routes advertised by at least two nodes in
              the emulated topology. The topology SHOULD be such that before
              the withdrawal the DUT prefers the leaf routes advertised by a
              node "nodeA" via the Preferred Egress Interface, and after the
              withdrawal the DUT prefers the leaf routes advertised by a node
              "nodeB" via the Next-Best Egress Interface.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>The Tester withdraws the set of IGP leaf routes from nodeA.
              This is the Convergence Event. The withdrawal update message
              SHOULD be a single unfragmented packet. If the routes cannot be
              withdrawn by a single packet, the messages SHOULD be sent using
              the same pacing characteristics as the DUT. The Tester MAY
              record the time it sends the withdrawal message(s).</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Re-advertise the set of withdrawn IGP leaf routes from nodeA
              emulated by the Tester. The update message SHOULD be a single
              unfragmented packet. If the routes cannot be advertised by a
              single packet, the messages SHOULD be sent using the same pacing
              characteristics as the DUT. The Tester MAY record the time it
              sends the update message(s).</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP convergence time is influenced by SPF or route
          calculation delay, SPF or route calculation execution time, and
          routing and forwarding tables update time <xref
          target="Po09a"></xref>.</t>

          <t>Discussion</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the routes are withdrawn
          by the Tester. Alternatively the Tester SHOULD record the time the
          instant the routes are withdrawn and traffic loss SHOULD only be
          measured on the Next-Best Egress Interface. For loss-derived
          benchmarks the time of the Start Traffic Instant SHOULD be recorded
          as well. See Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
        </section>
      </section>

      <section title="Administrative changes">
        <section title="Convergence Due to Local Adminstrative Shutdown">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to taking the DUT's Local
          Interface administratively out of service.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure 1.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Take the DUT's Preferred Egress Interface administratively
              out of service. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore Preferred Egress Interface by administratively
              enabling the interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>It is possible that no measured packet loss will be observed
              for this test case.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP Convergence time may be influenced by LSA/LSP
          delay, LSA/LSP generation time, LSA/LSP flood packet pacing, SPF
          delay, SPF execution time, and routing and forwarding tables update
          time <xref target="Po09a"></xref>.</t>
        </section>

        <section anchor="testcase_route_cost_change"
                 title="Convergence Due to Cost Change">
          <t>Objective</t>

          <t>To obtain the IGP convergence time due to route cost change.</t>

          <t>Procedure</t>

          <t><list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure 1.</t>

              <t>Send Offered Load from Tester to DUT on ingress
              interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>The Tester, emulating the neighbor node, increases the cost
              for all IGP routes at DUT's Preferred Egress Interface so that
              the Next-Best Egress Interface becomes preferred path. The
              update message advertising the higher cost MUST be a single
              unfragmented packet. This is the Convergence Event. The Tester
              MAY record the time it sends the update message advertising the
              higher cost on the Preferred Egress Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>The Tester, emulating the neighbor node, decreases the cost
              for all IGP routes at DUT's Preferred Egress Interface so that
              the Preferred Egress Interface becomes preferred path. The
              update message advertising the lower cost MUST be a single
              unfragmented packet.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route LoC Periods, and Loss-Derived LoC
              Period.</t>
            </list></t>

          <t>Results</t>

          <t>The measured IGP Convergence time may be influenced by SPF delay,
          SPF execution time, and routing and forwarding tables update time
          <xref target="Po09a"></xref>.</t>

          <t>Discussion</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the cost is changed by the
          Tester. Alternatively the Tester SHOULD record the time the instant
          the cost is changed and traffic loss SHOULD only be measured on the
          Next-Best Egress Interface. For loss-derived benchmarks the time of
          the Start Traffic Instant SHOULD be recorded as well. See Section
          <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
        </section>
      </section>
    </section>

    <section title="Security Considerations">
      <t>Benchmarking activities as described in this memo are limited to
      technology characterization using controlled stimuli in a laboratory
      environment, with dedicated address space and the constraints specified
      in the sections above.</t>

      <t>The benchmarking network topology will be an independent test setup
      and MUST NOT be connected to devices that may forward the test traffic
      into a production network, or misroute traffic to the test management
      network.</t>

      <t>Further, benchmarking is performed on a "black-box" basis, relying
      solely on measurements observable external to the DUT/SUT.</t>

      <t>Special capabilities SHOULD NOT exist in the DUT/SUT specifically for
      benchmarking purposes. Any implications for network security arising
      from the DUT/SUT SHOULD be identical in the lab and in production
      networks.</t>
    </section>

    <section title="IANA Considerations">
      <t>This document requires no IANA considerations.</t>
    </section>

    <section title="Acknowledgements">
      <t>Thanks to Sue Hares, Al Morton, Kevin Dubray, Ron Bonica, David Ward,
      Peter De Vriendt and the BMWG for their contributions to this work.</t>
    </section>
  </middle>

  <back>
    <references title="Normative References">
      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.1242.xml"?>

      <reference anchor="Br91">
        <front>
          <title abbrev="Benchmarking Terminology">Benchmarking terminology
          for network interconnection devices</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>33 Kirkland Street</street>

                <street>William James Hall 1232</street>

                <city>Cambridge</city>

                <region>MA</region>

                <code>02138</code>

                <country>US</country>
              </postal>

              <phone>+1 617 495 3864</phone>

              <email>SOB@HARVARD.HARVARD.EDU</email>
            </address>
          </author>

          <date day="1" month="July" year="1991" />

          <abstract>
            <t>This memo discusses and defines a number of terms that are used
            in describing performance benchmarking tests and the results of
            such tests. The terms defined in this memo will be used in
            additional memos to define specific benchmarking tests and the
            suggested format to be used in reporting the results of each of
            the tests. This memo is a product of the Benchmarking Methodology
            Working Group (BMWG) of the Internet Engineering Task Force
            (IETF).</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="1242" />

        <format octets="22817" target="ftp://ftp.isi.edu/in-notes/rfc1242.txt"
                type="TXT" />
      </reference>

      <?rfc linefile="1555:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml"?>

      <reference anchor="Br97">
        <front>
          <title abbrev="RFC Key Words">Key words for use in RFCs to Indicate
          Requirement Levels</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>1350 Mass. Ave.</street>

                <street>Cambridge</street>

                <street>MA 02138</street>
              </postal>

              <phone>- +1 617 495 3864</phone>

              <email>sob@harvard.edu</email>
            </address>
          </author>

          <date month="March" year="1997" />

          <area>General</area>

          <keyword>keyword</keyword>

          <abstract>
            <t>In many standards track documents several words are used to
            signify the requirements in the specification. These words are
            often capitalized. This document defines these words as they
            should be interpreted in IETF documents. Authors who follow these
            guidelines should incorporate this phrase near the beginning of
            their document: <list>
                <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL",
                "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and
                "OPTIONAL" in this document are to be interpreted as described
                in RFC 2119.</t>
              </list></t>

            <t>Note that the force of these words is modified by the
            requirement level of the document in which they are used.</t>
          </abstract>
        </front>

        <seriesInfo name="BCP" value="14" />

        <seriesInfo name="RFC" value="2119" />

        <format octets="4723" target="ftp://ftp.isi.edu/in-notes/rfc2119.txt"
                type="TXT" />

        <format octets="17491"
                target="http://xml.resource.org/public/rfc/html/rfc2119.html"
                type="HTML" />

        <format octets="5777"
                target="http://xml.resource.org/public/rfc/xml/rfc2119.xml"
                type="XML" />
      </reference>

      <?rfc linefile="1557:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.2544.xml"?>

      <reference anchor="Br99">
        <front>
          <title abbrev="Benchmarking Methodology">Benchmarking Methodology
          for Network Interconnect Devices</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>1350 Mass. Ave</street>

                <street>Room 813</street>

                <city>Cambridge</city>

                <region>MA</region>

                <code>02138</code>

                <country>US</country>
              </postal>

              <phone>+1 617 495 3864</phone>

              <facsimile>+1 617 496 8500</facsimile>

              <email>sob@harvard.edu</email>
            </address>
          </author>

          <author fullname="Jim McQuaid" initials="J." surname="McQuaid">
            <organization>NetScout Systems</organization>

            <address>
              <postal>
                <street>4 Westford Tech Park Drive</street>

                <city>Westford</city>

                <region>MA</region>

                <code>01886</code>

                <country>US</country>
              </postal>

              <phone>+1 978 614 4116</phone>

              <facsimile>+1 978 614 4004</facsimile>

              <email>mcquaidj@netscout.com</email>
            </address>
          </author>

          <date month="March" year="1999" />

          <abstract>
            <t>This document discusses and defines a number of tests that may
            be used to describe the performance characteristics of a network
            interconnecting device. In addition to defining the tests this
            document also describes specific formats for reporting the results
            of the tests. Appendix A lists the tests and conditions that we
            believe should be included for specific cases and gives additional
            information about testing practices. Appendix B is a reference
            listing of maximum frame rates to be used with specific frame
            sizes on various media and Appendix C gives some examples of frame
            formats to be used in testing.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="2544" />

        <format octets="66688" target="ftp://ftp.isi.edu/in-notes/rfc2544.txt"
                type="TXT" />
      </reference>

      <?rfc linefile="1559:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.1195.xml"?>

      <reference anchor="Ca90">
        <front>
          <title abbrev="OSI ISIS for IP and Dual Environments">Use of OSI
          IS-IS for routing in TCP/IP and dual environments</title>

          <author fullname="Ross Callon" initials="R." surname="Callon">
            <organization>Digital Equipment Corporation (DEC)</organization>

            <address>
              <postal>
                <street>550 King Street</street>

                <street>LKG 1-2/A19</street>

                <city>Littleton</city>

                <region>MA</region>

                <code>01460-1289</code>

                <country>US</country>
              </postal>

              <phone>+1 508 486 5009</phone>
            </address>
          </author>

          <date day="1" month="December" year="1990" />

          <abstract>
            <t>This RFC specifies an integrated routing protocol, based on the
            OSI Intra-Domain IS-IS Routing Protocol, which may be used as an
            interior gateway protocol (IGP) to support TCP/IP as well as OSI.
            This allows a single routing protocol to be used to support pure
            IP environments, pure OSI environments, and dual environments.
            This specification was developed by the IS-IS working group of the
            Internet Engineering Task Force.</t>

            <t>The OSI IS-IS protocol has reached a mature state, and is ready
            for implementation and operational use. The most recent version of
            the OSI IS-IS protocol is contained in ISO DP 10589. The proposed
            standard for using IS-IS for support of TCP/IP will therefore make
            use of this version (with a minor bug correction, as discussed in
            Annex B). We expect that future versions of this proposed standard
            will upgrade to the final International Standard version of IS-IS
            when available.</t>

            <t>Comments should be sent to "isis@merit.edu".</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="1195" />

        <format octets="187866"
                target="ftp://ftp.isi.edu/in-notes/rfc1195.txt" type="TXT" />

        <format octets="362052" target="ftp://ftp.isi.edu/in-notes/rfc1195.ps"
                type="PS" />
      </reference>

      <?rfc linefile="1561:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.2285.xml"?>

      <reference anchor="Ma98">
        <front>
          <title abbrev="Benchmarking Terminology">Benchmarking Terminology
          for LAN Switching Devices</title>

          <author fullname="Robert Mandeville" initials="R."
                  surname="Mandeville">
            <organization>European Network Laboratories (ENL)</organization>

            <address>
              <postal>
                <street>2</street>

                <street>rue Helene Boucher</street>

                <street>78286 Guyancourt Cedex</street>

                <country>France</country>
              </postal>

              <phone>+ 33 1 39 44 12 05</phone>

              <facsimile>+ 33 1 39 44 12 06</facsimile>

              <email>bob.mandeville@eunet.fr</email>
            </address>
          </author>

          <date month="February" year="1998" />

          <area>Operations</area>

          <keyword>local area network</keyword>

          <keyword>Benchmarking</keyword>
        </front>

        <seriesInfo name="RFC" value="2285" />

        <format octets="43130" target="ftp://ftp.isi.edu/in-notes/rfc2285.txt"
                type="TXT" />

        <format octets="58104"
                target="http://xml.resource.org/public/rfc/html/rfc2285.html"
                type="HTML" />

        <format octets="42730"
                target="http://xml.resource.org/public/rfc/xml/rfc2285.xml"
                type="XML" />
      </reference>

      <?rfc linefile="1563:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.2328.xml"?>

      <reference anchor="Mo98">
        <front>
          <title>OSPF Version 2</title>

          <author fullname="John Moy" initials="J." surname="Moy">
            <organization>Ascend Communications, Inc.</organization>

            <address>
              <postal>
                <street>1 Robbins Road</street>

                <city>Westford</city>

                <region>MA</region>

                <code>01886</code>
              </postal>

              <phone>978-952-1367</phone>

              <facsimile>978-392-2075</facsimile>

              <email>jmoy@casc.com</email>
            </address>
          </author>

          <date month="April" year="1998" />

          <area>Routing</area>

          <keyword>open shortest-path first protocol</keyword>

          <keyword>routing</keyword>

          <keyword>OSPF</keyword>

          <abstract>
            <t>This memo documents version 2 of the OSPF protocol. OSPF is a
            link-state routing protocol. It is designed to be run internal to
            a single Autonomous System. Each OSPF router maintains an
            identical database describing the Autonomous System's topology.
            From this database, a routing table is calculated by constructing
            a shortest- path tree.</t>

            <t>OSPF recalculates routes quickly in the face of topological
            changes, utilizing a minimum of routing protocol traffic. OSPF
            provides support for equal-cost multipath. An area routing
            capability is provided, enabling an additional level of routing
            protection and a reduction in routing protocol traffic. In
            addition, all OSPF routing protocol exchanges are
            authenticated.</t>

            <t>The differences between this memo and RFC 2178 are explained in
            Appendix G. All differences are backward-compatible in nature.
            Implementations of this memo and of RFCs 2178, 1583, and 1247 will
            interoperate.</t>

            <t>Please send comments to ospf@gated.cornell.edu.</t>
          </abstract>
        </front>

        <seriesInfo name="STD" value="54" />

        <seriesInfo name="RFC" value="2328" />

        <format octets="447367"
                target="ftp://ftp.isi.edu/in-notes/rfc2328.txt" type="TXT" />

        <format octets="466915"
                target="http://xml.resource.org/public/rfc/html/rfc2328.html"
                type="HTML" />

        <format octets="446625"
                target="http://xml.resource.org/public/rfc/xml/rfc2328.xml"
                type="XML" />
      </reference>

      <?rfc linefile="1565:/tmp/CGI31441.1"?>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml/reference.RFC.4689.xml"?>

      <reference anchor="Po06">
        <front>
          <title>Terminology for Benchmarking Network-layer Traffic Control
          Mechanisms</title>

          <author fullname="S. Poretsky" initials="S." surname="Poretsky">
            <organization></organization>
          </author>

          <author fullname="J. Perser" initials="J." surname="Perser">
            <organization></organization>
          </author>

          <author fullname="S. Erramilli" initials="S." surname="Erramilli">
            <organization></organization>
          </author>

          <author fullname="S. Khurana" initials="S." surname="Khurana">
            <organization></organization>
          </author>

          <date month="October" year="2006" />

          <abstract>
            <t>This document describes terminology for the benchmarking of
            devices that implement traffic control using packet classification
            based on defined criteria. The terminology is to be applied to
            measurements made on the data plane to evaluate IP traffic control
            mechanisms. Rules for packet classification can be based on any
            field in the IP header, such as the Differentiated Services Code
            Point (DSCP), or any field in the packet payload, such as port
            number. This memo provides information for the Internet
            community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4689" />

        <format octets="62369" target="ftp://ftp.isi.edu/in-notes/rfc4689.txt"
                type="TXT" />
      </reference>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-bmwg-igp-dataplane-conv-term-18.xml"?>

      <reference anchor="Co08">
        <front>
          <title>OSPF for IPv6</title>

          <author fullname="R. Coltun" initials="R." surname="Coltun">
            <organization></organization>
          </author>

          <author fullname="D. Ferguson" initials="D." surname="Ferguson">
            <organization></organization>
          </author>

          <author fullname="J. Moy" initials="J." surname="Moy">
            <organization></organization>
          </author>

          <author fullname="A. Lindem" initials="A." surname="Lindem">
            <organization></organization>
          </author>

          <date month="July" year="2008" />

          <abstract>
            <t>This document describes the modifications to OSPF to support
            version 6 of the Internet Protocol (IPv6). The fundamental
            mechanisms of OSPF (flooding, Designated Router (DR) election,
            area support, Short Path First (SPF) calculations, etc.) remain
            unchanged. However, some changes have been necessary, either due
            to changes in protocol semantics between IPv4 and IPv6, or simply
            to handle the increased address size of IPv6. These modifications
            will necessitate incrementing the protocol version from version 2
            to version 3. OSPF for IPv6 is also referred to as OSPF version 3
            (OSPFv3).&lt;/t&gt;&lt;t&gt; Changes between OSPF for IPv4, OSPF
            Version 2, and OSPF for IPv6 as described herein include the
            following. Addressing semantics have been removed from OSPF
            packets and the basic Link State Advertisements (LSAs). New LSAs
            have been created to carry IPv6 addresses and prefixes. OSPF now
            runs on a per-link basis rather than on a per-IP-subnet basis.
            Flooding scope for LSAs has been generalized. Authentication has
            been removed from the OSPF protocol and instead relies on IPv6's
            Authentication Header and Encapsulating Security Payload
            (ESP).&lt;/t&gt;&lt;t&gt; Even with larger IPv6 addresses, most
            packets in OSPF for IPv6 are almost as compact as those in OSPF
            for IPv4. Most fields and packet- size limitations present in OSPF
            for IPv4 have been relaxed. In addition, option handling has been
            made more flexible.&lt;/t&gt;&lt;t&gt; All of OSPF for IPv4's
            optional capabilities, including demand circuit support and
            Not-So-Stubby Areas (NSSAs), are also supported in OSPF for IPv6.
            [STANDARDS TRACK]</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5340" />

        <format octets="225664"
                target="ftp://ftp.isi.edu/in-notes/rfc5340.txt" type="TXT" />
      </reference>

      <reference anchor="Ho08">
        <front>
          <title>Routing IPv6 with IS-IS</title>

          <author fullname="C. Hopps" initials="C." surname="Hopps">
            <organization></organization>
          </author>

          <date month="October" year="2008" />

          <abstract>
            <t>This document specifies a method for exchanging IPv6 routing
            information using the IS-IS routing protocol. The described method
            utilizes two new TLVs: a reachability TLV and an interface address
            TLV to distribute the necessary IPv6 information throughout a
            routing domain. Using this method, one can route IPv6 along with
            IPv4 and OSI using a single intra-domain routing protocol.
            [STANDARDS TRACK]</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5308" />

        <format octets="13324" target="ftp://ftp.isi.edu/in-notes/rfc5308.txt"
                type="TXT" />
      </reference>

      <reference anchor="Ko02">
        <front>
          <title>One-way Loss Pattern Sample Metrics</title>

          <author fullname="R. Koodli" initials="R." surname="Koodli">
            <organization></organization>
          </author>

          <author fullname="R. Ravikanth" initials="R." surname="Ravikanth">
            <organization></organization>
          </author>

          <date month="August" year="2002" />
        </front>

        <seriesInfo name="RFC" value="3357" />

        <format octets="30570" target="ftp://ftp.isi.edu/in-notes/rfc3357.txt"
                type="TXT" />
      </reference>

      <reference anchor="Po09t">
        <front>
          <title>Terminology for Benchmarking Link-State IGP Data Plane Route
          Convergence</title>

          <author fullname="Scott Poretsky" initials="S" surname="Poretsky">
            <organization></organization>
          </author>

          <author fullname="Brent Imhoff" initials="B" surname="Imhoff">
            <organization></organization>
          </author>

          <date day="8" month="July" year="2009" />

          <abstract>
            <t>This document describes the terminology for benchmarking
            Interior Gateway Protocol (IGP) Route Convergence. The terminology
            is to be used for benchmarking IGP convergence time through
            externally observable (black box) data plane measurements. The
            terminology can be applied to any link-state IGP, such as ISIS and
            OSPF. Link-State IGP Data Plane Route Convergence</t>
          </abstract>
        </front>

        <seriesInfo name="Internet-Draft"
                    value="draft-ietf-bmwg-igp-dataplane-conv-term-18" />

        <format target="http://www.ietf.org/internet-drafts/draft-ietf-bmwg-igp-dataplane-conv-term-18.txt"
                type="TXT" />
      </reference>

      <?rfc linefile="1:http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-bmwg-igp-dataplane-conv-app-17.xml"?>

      <reference anchor="Po09a">
        <front>
          <title>Considerations for Benchmarking Link-State IGP Data Plane
          Route Convergence</title>

          <author fullname="Scott Poretsky" initials="S" surname="Poretsky">
            <organization></organization>
          </author>

          <date day="8" month="March" year="2009" />

          <abstract>
            <t>This document discusses considerations for benchmarking
            Interior Gateway Protocol (IGP) Route Convergence for any
            link-state IGP, such as Intermediate System-Intermediate System
            (ISIS) and Open-Shorted Path first (OSPF). A companion methodology
            document is to be used for benchmarking IGP convergence time
            through externally observable (black box) data plane measurements.
            A companion</t>
          </abstract>
        </front>

        <seriesInfo name="Internet-Draft"
                    value="draft-ietf-bmwg-igp-dataplane-conv-app-17" />

        <format target="http://www.ietf.org/internet-drafts/draft-ietf-bmwg-igp-dataplane-conv-app-17.txt"
                type="TXT" />
      </reference>
    </references>
  </back>
</rfc>

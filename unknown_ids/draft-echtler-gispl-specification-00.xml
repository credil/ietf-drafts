<?xml version='1.0'?>
<!DOCTYPE rfc SYSTEM 'rfc2629.dtd'>
<?rfc symrefs='yes'?>
<rfc ipr='trust200902' category='info' docName='draft-echtler-gispl-specification-00'>

	<front>
		<title abbrev='GISpL'>GISpL: Gestural Interface Specification Language</title>

		<author initials='F.E.' surname='Echtler' fullname='Florian Echtler'> <!-- role='editor' -->
			<organization>Munich Univ. of Applied Sciences</organization>

			<address>
				<postal>
					<street>Lothstr. 64</street>
					<city>Munich</city>
					<code>80336</code>
					<country>Germany</country>
				</postal>

				<phone>+49 89 1265 3736</phone>
				<email>floe@butterbrot.org</email>
				<uri>http://floe.butterbrot.org/</uri>
			</address>
		</author>

		<date month='March' year='2011' />

		<area>Applications</area>
		<workgroup>Applications Area Working Group</workgroup>
		<keyword>Gestures</keyword>
		<keyword>Specification Language</keyword>
		<keyword>Internet-Draft</keyword>
		<keyword>JSON</keyword>
		<abstract>
			<t>This document introduces GISpL, the Gestural Interface Specification Language.
			GISpL enables the unambiguous description of gestures used in human-computer interfaces.
			This includes gestures on touch and multi-touch screens, with digital pens,
			with hand-held controllers or in free air. A matching engine analyzes motion data
			produced by the input device(s) and triggers registered gestures.
			</t>
		</abstract>
	</front>

	<middle>
		<section title="Introduction">
			<t>This document describes GISpL (Gestural Interface Specification Language), a formal language
			for describing human-computer interfaces which use gestures. The term "gesture" is used in a very 
			wide sense here, meaning any motion of the user which can be captured by an input device.</t>
		<section title="Motivation">
			<t>As novel types of human-computer interfaces such as multi-touch screens, digital pens, hand-held
			controllers or even free-air gestures become more and more common, so does the number of special-purpose 
			applications built to interact with these devices.</t>
			<t>Using a dedicated formal language to describe the various types of gestural interaction which are
			possible with an application has advantages for various distinct groups:
			<list hangIndent='8' style='hanging'>
				<t hangText='Users'>End users of gestural applications benefit in two ways: the behavior of such applications
				becomes more consistent across different platforms and the users are given the option to modify the behavior
				to suit their needs.</t>
				<t hangText='Researchers'>When the behavior of a gestural interface is specified formally, it is easier for 
				user interface researchers to compare different interfaces. Also, prototypical implementations of such interfaces
				are easier to modify and adapt during the design, development and evaluation cycle.</t>
				<t hangText='Developers'>Since the formal specification enables a dedicated matching engine to perform
				the actual detection and matching of gestures, developers are spared tedious tasks such as constant
				reimplementation of basic gesture matching algorithms.</t>
			</list>
			</t>
		</section>
		<section title="Design Considerations">
			<t>GISpL serves two purposes:
			<list style="symbols">
				<t>describe what motions the users have to execute in order to trigger a certain response</t>
				<t>deliver information about the actual execution of these motions to the application</t>
			</list></t>
			<t>Moreover, GISpL should be usable across a very wide range of platforms, even unconventional ones. One important
			example is the Firefox web browser which has recently acquired the capability to deliver multi-touch events to web
			applications.</t>
			<t>Consequently, the requirements regarding GISpL are:
			<list style="symbols">
				<t>must be human-readable</t>
				<t>must be machine-readable</t>
				<t>must be supported on a wide range of platforms</t>
				<t>must be lightweight for low-latency network transmission</t>
			</list></t>
			<t>Therefore, the decision was made to base GISpL on <xref target='RFC4627'>JSON</xref>. Compared to XML, JSON gives a better
			balance between readability for humans and code size. It is supported in nearly all programming languages and platforms.
			</t>
		</section>
		</section>

		<section title="Specification">
			<t>GISpL consists of three core elements: regions, gestures and features.</t>
			<t>For the full formal ABNF specifictation, please see the appendix. This section will use a relaxed syntax for easier readability,
			with rule names enclosed by angle braces &lt; &gt;. JSON-related markup ( { } , [ ] "" ) will be reproduced verbatim.</t>

			<section title="Definitions">
				<t>In the context of GISpL, the terms <spanx style="emph">input object</spanx> and <spanx style="emph">input event</spanx> will be used. 
				An <spanx style="emph">input object</spanx> is any physical object which is detectable by the input hardware, such as a mouse, a Wiimote
				or the user's hand. Input objects are classified into one of 32 categories as given in the appendix. Every input object is given a unique
				ID number according to the capabilities of the hardware. I.e., a uniquely identifiable tangible object always has the same ID, while anonymous
				touch points on an interactive surface will have IDs so that each touch point is uniquely identifiable during the duration of the touch.
				An <spanx style="emph">input event</spanx> is a single spatial measurement regarding the 
				position (and optionally, orientation, dimensions etc.) of an input object as captured by one or more of the sensors used.</t>
			</section>

			<section title="Region" anchor="sec_region">
				<t>Regions define spatial areas in which a certain set of gestures is valid and which capture motion data
				that falls within their boundaries. Regions are defined in one of two reference coordinate systems:
					<list style="symbols">
						<t>A 2D screen coordinate system where the x and y coordinates range from 0.0 to 1.0 across the
						physical extension of the user's screen, with the origin being in the lower left corner. This is indicated by the region flag "poly".
						The list of boundary points is interpreted as a closed polygon in the x-y plane. The z coordinate is not used.</t>
						<t>A 3D metric coordinate system with an arbitrary reference point as origin (usually defined by the input device). This is indicated
						by the region flag "hull". The list of boundary points is interpreted as defining the convex hull of the region.</t>
					</list>
				</t>
				<t>Regions are managed in an ordered list. Arriving input events are checked against this list, starting from the first element. If the
				position of the input event falls within the boundaries of the region <spanx style="emph">and</spanx> if the bit in the filter bitmask
				which corresponds to the category of the input event is set, the the input event is captured by the region. Captured events and their 
				history are subsequently used to check whether one or more of the gestures attached to this region have been triggered.</t>

				<figure><artwork><![CDATA[
point     = [ <number>, <number>, <number> ]
pointlist = [ null/<point> *( , <point> ) ]

regionflags = "poly" / "hull"

region = {
  "id":<string>,
  "flags":<regionflags>,
  "filters":<number>,
  "points":<pointlist>,
  "gestures":<gesturelist>
}
				]]></artwork></figure>

			</section>

			<section title="Gesture" anchor="sec_gesture">
				<t>Gestures appear in two variants: either as gesture specifications (describing what motions
				the user has to execute for a certain effect) or as gesture events (describing a) the fact that a matching
				motion event just took place and b) the motion data which triggered the event).</t>
				<t>Whether a gesture object is a gesture event can be determined from looking at the flags. Should they contain the string "result",
				then it is a gesture event and the features composing this gesture will contain valid data in their "result" array. Otherwise,
				the object is a gesture specification and the features will contain valid data in their "constraints" array.</t>
				<t>When a gesture has the "oneshot" flag, then it can only be triggered once by a given set of input IDs. Repeated triggering is
				only possible when the set of IDs captured by the containing region changes.</t>
				<t>When a gesture has the "default" flag set, it is added to a pool of default gestures. When a gesture specification with an empty
				feature list is encountered, then the name is looked up in the default gesture pool and if a match is found, the feature list is copied
				from the default gesture. Otherwise, the empty gesture is ignored.</t>

				<figure><artwork><![CDATA[
gesturelist  = [ null/<gesture> *( , <gesture> ) ]
gestureflag  = "oneshot" / "default" / "result"
gestureflags = [ null/<gestureflag> (* , <gestureflag> ) ]

gesture = {
  "name":<string>,
  "flags":<gestureflags>,
  "features":<featurelist>
}
				]]></artwork></figure>

			</section>

			<section title="Feature">
				<t> are the building blocks of gestures and are atomic mathematical properties of the raw motion data,
				such as the average motion vector or the diameter of the convex hull of all motion points.</t>

				<figure><artwork><![CDATA[
featurelist = [ null/<feature> *( , <feature> ) ]
featureitem = <point>/<number>
itemlist    = [ null/<featureitem>  *( , <featureitem>  ) ]

feature = {
  "type":<string>,
  "filters":<number>,
  "constraints":<itemlist>,
  "result":<itemlist>
}
				]]></artwork></figure>

				<t>A feature is classified by its type, a filter bitmask for input events as described in <xref target="sec_region"/>, a
				type-specific list of constraints and a type-specific list of results. Depending on whether the containing gesture is a
				"result" gesture or not (see <xref target="sec_gesture"/>), either the constraint list or the result list will be empty.
				All temporal relations (such as motion vector) are expressed relative to a time unit of one sensor frame, i.e. for a sensor
				setup running at 60 Hz, the temporal unit will be 16.66 ms.</t>

				<t>Features are divided into two groups. The first one are <spanx style="emph">single-match</spanx> features, which will only
				generate a single result instance regardless of the number of input objects.</t>

				<texttable anchor='table_features_single'>
					<ttcol align='center'>Feature Type</ttcol>
					<ttcol align='center'>Constraint Types</ttcol>
					<ttcol align='center'>Result Types</ttcol>
					<ttcol align='center'>Description</ttcol>
					<c>Motion</c> <c>&lt;point&gt;, &lt;point&gt;</c> <c>&lt;point&gt;</c> <c>Average motion vector, lower and upper limits per component</c>
					<c>Rotation</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Relative rotation angle in rad, lower and upper limits</c>
					<c>Scale</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Relative size change, lower and upper limits</c>
					<c>Path</c> <c>&lt;point&gt; *(, &lt;point&gt;)</c> <c>&lt;number&gt;</c> <c>Match for template path</c>
					<c>Count</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Number of input objects, lower and upper limits</c>
					<c>Delay</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Number of frames since first object entered, lower and upper limits</c>
					<postamble>Single-Match Features</postamble>
				</texttable>

				<t>The second group of features are <spanx style="emph">multi-match</spanx> features, which will potentially generate one result
				instance for each input object (depending on the constraint values).</t>

				<texttable anchor='table_features_multi'>
					<ttcol align='center'>Feature Type</ttcol>
					<ttcol align='center'>Constraint Types</ttcol>
					<ttcol align='center'>Result Types</ttcol>
					<ttcol align='center'>Description</ttcol>
					<c>ObjectID</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Match range of unique IDs (e.g. for tangible objects)</c>
					<c>ObjectParent</c> <c>&lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;</c> <c>Match range of unique parent IDs (e.g. for fingers belonging to specific user)</c>
					<c>ObjectPosition</c> <c>&lt;point&gt;, &lt;point&gt;</c> <c>&lt;point&gt;</c> <c>Position, lower and upper limits per component</c>
					<c>ObjectDimension</c> <c>&lt;point&gt;, &lt;point&gt;, &lt;point&gt;, &lt;point&gt;, &lt;number&gt;, &lt;number&gt;</c> <c>&lt;point&gt;, &lt;point&gt;, &lt;number&gt;</c> <c>Object dimensions, lower and upper limits per component (major axis, minor axis, size)</c>
					<c>ObjectGroup</c> <c>&lt;number&gt;, &lt;number&gt;, &lt;number&gt;</c> <c>&lt;number&gt;, &lt;point&gt;</c> <c>Match group of input objects (min. count, max. count, max. radius). Result = count/centroid</c>
					<postamble>Multi-Match Features</postamble>
				</texttable>

			</section>
		</section>

		<section title="Runtime Behavior">
			<t>This section details the behavior of the gesture matching engine.</t>
			<t>The following steps are performed every time an input event has been received:
			<list style="numbers">
				<t>...</t>
				<t>...</t>
			</list></t>
			<t>The following steps are performed every time after a full set of input events has been received:
			<list style="numbers">
				<t>...</t>
				<t>...</t>
			</list></t>
		</section>

		<section title="Examples">
			<t>For illustration purposes, this section gives some usage examples of GISpL.</t>
			<t>TBD</t>
		</section>

		<section title="Security Considerations">
			<t>Gesture events, if intercepted while in transit over the network, may disclose private
			data about the users' actions. Consequently, they should preferably be transmitted over 
			secure channels such as private networks, VPNs or SSL-encrypted links.</t>
		</section>

		<section title="IANA Considerations">
			<t>This document has no actions for IANA.</t>
		</section>
	</middle>

	<back>

		<references title='Normative References'>
			<?rfc include="reference.RFC.5234.xml"?>
		</references>
		<references title='Informative References'>
			<?rfc include="reference.RFC.4627.xml"?>
		</references>

		<section title="Formal GISpL Specification">
			<t>The following formal specification of GISpL is given in <xref target='RFC5234'>ABNF</xref>.
			JSON-related rule definitions (e.g. &lt;number&gt;, &lt;string&gt; ...) are to be found in <xref target='RFC4627'/>.</t>
			<figure anchor='gispl_spec'>
				<!-- preamble>foo</preamble-->
				<artwork type="abnf">
quote = quotation-mark

point = begin-array
        number value-separator number value-separator number
        end-array

item  = point / number

itemlist    = begin-array item    *( value-separator item    ) end-array 
pointlist   = begin-array point   *( value-separator point   ) end-array
gesturelist = begin-array gesture *( value-separator gesture ) end-array
featurelist = begin-array feature *( value-separator feature ) end-array

filters     = quote "filter" quote name-separator number value-separator
regionid    = quote "id" quote name-separator string value-separator 
regionflags = quote "flags" quote name-separator
              ( ( quote "poly" quote ) / ( quote "hull" quote ) )
              value-separator

gesturename  = quote "name" quote name-separator string value-separator
gestureflag  = ( quote "oneshot" quote ) / ( quote "default" quote )
gestureflags = quote "flags" quote name-separator
               begin-array gestureflag
               *( value-separator gestureflag )
               end-array value-separator

featuretype  = quote "type" quote name-separator string value-separator
results      = quote "result" quote name-separator itemlist
constraints  = quote "constraints" quote name-separator
               itemlist value-separator

feature      = begin-object
               featuretype constraints results
               end-object

gesture      = begin-object
               gesturename gestureflags featurelist
               end-object

region       = begin-object
               regionid regionflags filters
               pointlist value-separator gesturelist
               end-object
				</artwork>
				<!--postamble>Formal ABNF specification for GISpL.</postamble-->
			</figure>
		</section>

		<section title="Input Event Types">
			<t>TBD - see TUIO 2.0 spec</t>
		</section>

	</back>

</rfc>


<?xml version="1.0"?>

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
	  <!ENTITY RFC0793 SYSTEM
		   "http://xml.resource.org/public/rfc/bibxml/reference.RFC.0793">
	  <!ENTITY RFC3465 SYSTEM
		   "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3465">
	  <!ENTITY RFC3484 SYSTEM
		   "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3484">
	  <!ENTITY I-D.ietf-mptcp-multiaddressed SYSTEM
		   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-mptcp-multiaddressed-02">
	  <!ENTITY I-D.ietf-mptcp-architecture SYSTEM 
		   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-mptcp-architecture-05">
	  <!ENTITY I-D.ietf-mptcp-congestion SYSTEM 
		   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-mptcp-congestion-01">
	  <!ENTITY I-D.ietf-mptcp-api SYSTEM 
		   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-mptcp-api-00">
          <!ENTITY I-D.ietf-mif-problem-statement SYSTEM
                   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-ietf-mif-problem-statement-09">
          <!ENTITY I-D.tuexen-tsvwg-sctp-multipath SYSTEM
                   "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.draft-tuexen-tsvwg-sctp-multipath-01">
	  ]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<?rfc toc="yes"?>
<?rfc tocdepth="4"?>
<?rfc symrefs="no"?>
<?rfc sortrefs="yes" ?>
<?rfc compact="no" ?>
<?rfc subcompact="no" ?>

<rfc ipr="trust200902" docName='draft-barre-mptcp-impl-00'>
  <front>
    <title abbrev="MPTCP Impl. guidelines">MultiPath TCP - Guidelines for implementers</title>

    <author initials="S." surname="Barré" 
	    fullname="Sébastien Barré">
      <organization abbrev="UCLouvain, Belgium">Université catholique de Louvain
      </organization>
      <address>
        <postal>
          <street>Place Ste Barbe, 2</street>
          <city>Louvain-la-Neuve</city> <code>1348</code>
          <country>BE</country>
        </postal>
        <email>sebastien.barre@uclouvain.be</email>
        <uri>http://inl.info.ucl.ac.be/sbarre</uri>
      </address>      
    </author>

    <author initials="C." surname="Paasch" fullname="Christoph Paasch">
      <organization abbrev="UCLouvain, Belgium">Université catholique de Louvain
      </organization>
      <address>
        <postal>
          <street>Place Ste Barbe, 2</street>
          <city>Louvain-la-Neuve</city> <code>1348</code>
          <country>BE</country>
        </postal>
        <email>christoph.paasch@uclouvain.be</email>
        <uri>http://inl.info.ucl.ac.be/cpaasch</uri>
      </address>
    </author>

    <author initials="O." surname="Bonaventure" fullname="Olivier Bonaventure">
      <organization abbrev="UCLouvain, Belgium">Université catholique de Louvain
      </organization>
      <address>
        <postal>
          <street>Place Ste Barbe, 2</street>
          <city>Louvain-la-Neuve</city> <code>1348</code>
          <country>BE</country>
        </postal>
        <uri>http://inl.info.ucl.ac.be/obo</uri>
      </address>
    </author>

    <date month="March" year="2011" />

    <abstract>
      <t>
	Multipath TCP is a major extension to TCP that allows improving the
	resource usage in the current Internet by transmitting data
	over several TCP subflows, while still showing one single regular
	TCP socket to the application.
        
	This document describes our experience in writing a MultiPath TCP
        implementation in the Linux kernel and discusses implementation
        guidelines that could be useful for other developers who are planning
        to add MultiPath TCP to their networking stack.
      </t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>
	The MultiPath TCP protocol
	<xref target="I-D.ietf-mptcp-multiaddressed"/> is a major TCP
	extension that allows for simultaneous use of multiple paths,
	while being transparent to the applications, fair to regular
	TCP flows <xref target="I-D.ietf-mptcp-congestion"/> and
	deployable in the current Internet. The MPTCP design goals
	and the protocol architecture that allow reaching
	them are described in
	<xref target="I-D.ietf-mptcp-architecture"/>. 
	Besides the protocol architecture, a number of non-trivial 
	design choices need to be made in order to extend an existing TCP
        implementation to support MultiPath TCP. This document gathers a set of
        guidelines
	that should help implementers writing an efficient and modular
	MPTCP stack. The guidelines are expected to be applicable regardless of
        the Operating System (although the MPTCP
	implementation described here is done in Linux
	<xref target="Barre_Multipath"/>). Another goal is to
	achieve the greatest level of modularity without impacting
	efficiency, hence allowing other multipath protocols to nicely
	co-exist in the same stack. In order for the reader to clearly
	disambiguate "useful hints" from "important requirements", we
	write the latter in their own paragraphs, starting with the
	keyword "IMPORTANT". By important requirements, we mean design
	options that, if not followed, would lead to an 
	under-performing MPTCP stack, maybe even slower than regular TCP.
      </t>
      <t>
        This draft presents implementation guidelines that are based on the code
        which has been implemented in our MultiPath TCP aware Linux
        kernel (the version covered here is 0.6) which
        is available
        from <eref target="http://inl.info.ucl.ac.be/mptcp">
	  http://inl.info.ucl.ac.be/mptcp</eref>. We also list
        configuration guidelines that have proven to be useful in practice. In
        some cases, we discuss some mechanisms that have not yet been
        implemented. These mechanisms are clearly listed. During our work in
        implementing MultiPath TCP, we evaluated other designs. Some of them are
        not used anymore in our implementation. However, we explain in the
        appendix the reason why these particular designs have not been
        considered further.
      </t>
      <t>
	This document is structured as follows. First we propose an
	architecture that allows supporting MPTCP in a protocol stack residing
        in an operating system. Then we consider a range of problems
	that must be solved by an MPTCP stack (compared to a regular
	TCP stack). In <xref target="sec_conf"/>, we propose recommendations on
        how a system administrator could correctly configure an MPTCP-enabled
        host. Finally, we discuss future work, in particular in the
	area of MPTCP optimization.
      </t>
      <section title="Terminology">
	<t>
	  In this document we use the same terminology as in
	  <xref target="I-D.ietf-mptcp-architecture"/> and 
	  <xref target="I-D.ietf-mptcp-multiaddressed"/>. In addition,
	  we will use the following implementation-specific terms:
	  <list style="symbols">
	    <t>
	      Meta-socket: A socket structure used to reorder incoming
	      data at the connection level and schedule outgoing data
	      to subflows.
	    </t>
	    <t>
	      Master subsocket: The socket structure that is visible
	      from the application. If regular TCP is in use, this is
	      the only active socket structure. If MPTCP is used, this
	      is the socket corresponding to the first subflow.
	    </t>
	    <t>
	      Slave subsocket: Any socket created by the kernel to
	      provide an additional subflow. Those sockets are not
	      visible to the application (unless a specific API
	      <xref target="I-D.ietf-mptcp-api"/> is used).
	      The meta-socket, master and slave subsocket are explained
	      in more details in <xref target="sec_structure_mt"/>.
	    </t>
	    <t>
	      Endpoint id: Endpoint identifier. It is the tuple (saddr,
	      sport, daddr, dport) that identifies a particular
	      subflow, hence a particular subsocket.
	    </t>
	    <t>
	      Fendpoint id: First Endpoint identifier. It is the
	      endpoint identifier of the Master subsocket.
	    </t>
	    <t>
	      Connection id or token: It is a locally
	      unique number, defined in Section 2 of 
	      <xref target="I-D.ietf-mptcp-multiaddressed"/>, that
	      allows finding a connection during the establishment of
	      new subflows.
	    </t>	    
	  </list>
	</t>
      </section>
    </section>
    
    <section title="An architecture for Multipath transport" anchor="sec_arch">
      <t>
        Section 4 of the MPTCP architecture document
        <xref target="I-D.ietf-mptcp-architecture"/> describes the functional
        decomposition of MPTCP. It lists four entities, namely Path
        Management, 
	Packet Scheduling,
	Subflow Interface and Congestion Control. These entities can be further
        grouped based on the layer at which they operate:
	<list style="symbols">
	  <t>Transport layer: This includes Packet Scheduling,
	    Subflow Interface and Congestion Control, and is grouped
	    under the 
	    term "Multipath Transport (MT)". From an
	    implementation point of view, they all will involve
	    modifications to TCP. </t>
	  <t>Any layer: Path Management. Path management can be done
	    in the transport layer, as is the case of the built-in path
	    manager (PM)
	    described in
	    <xref target="I-D.ietf-mptcp-multiaddressed"/>. That PM
	    discovers paths through the exchange of TCP options of type
	    ADD_ADDR or the reception of a SYN on a new address pair, 
	    and defines a path as an endpoint_id 
	    (saddr, sport, daddr, dport). But, more generally, a PM could be any
            module able to
	    expose multiple paths to MPTCP, located either in kernel
	    or user space, and acting on any OSI layer (e.g. a bonding
	    driver that would expose its multiple links to the
	    Multipath Transport).</t>
	</list>
      </t>
      
      <t>Because of the fundamental independence of Path Management
	compared to the three other entities, we draw a clear line  between
	both, and define a simple interface that allows MPTCP to benefit
	easily from any appropriately interfaced multipath
	technology. In this document, we stick to describing how
	the functional elements of MPTCP are defined, using the
	built-in Path Manager described in 
	<xref target="I-D.ietf-mptcp-multiaddressed"/>, and we leave for
	future separate documents the description of other path
	managers.
	We describe in the first subsection the precise roles of the Multipath
	Transport and the Path Manager. Then we detail how they are
	interfaced with each other.
      </t>

      <section title="MPTCP architecture">
	  <t>Although, when using the built-in PM, MPTCP is fully
	  contained in the transport layer, it can still be organized as a Path
          Manager and a Multipath Transport Layer as shown in
	  <xref target="fig_func_sep_default" />. The Path Manager
	  announces to the MultiPath Transport what paths can be used
	  through path indices for an MPTCP connection, identified by the
          fendpoint_id (first endpoint id). The fendpoint_id is the 
	  tuple (saddr, sport, daddr, dport) seen by
          the application and uniquely identifies the MPTCP connection
	  (an alternate way to identify the MPTCP connection being the
	  conn_id, which is a token as described in Section 2 of
	  <xref target="I-D.ietf-mptcp-multiaddressed"/>). The
          Path Manager maintains the mapping between the path_index
	  and an endpoint_id. The endpoint_id is
	  the tuple (saddr, sport, daddr, dport) that is to be used for
	  the corresponding path index. 
	  </t>
	  <t>
	  Note that the fendpoint_id itself represents
	  a path and is thus a particular endpoint_id. By convention,
	  the fendpoint_id is always represented as path index 1. As
	  explained in <xref target="I-D.ietf-mptcp-architecture"/>,
	  Section 5.6, it is not yet clear how an implementation
	  should behave in the event of a failure in the first
	  subflow. We expect, however, that the Master subsocket should
	  be kept in use as an interface with the application, even if
	  no data is transmitted anymore over it. It also allows the
	  fendpoint_id to remain meaningful throughout the life of the
	  connection. This behavior has yet to be
	  tested and refined with Linux MPTCP.
	  </t>
	  <t>
	    <xref target="fig_func_sep_default"/> shows an example
	    sequence of MT-PM interactions happening at the beginning
	    of an exchange. When the MT starts a new connection
	    (through an application connect() or accept()), it can
	    request the PM to be updated about possible alternate
	    paths for this new connection. The PM can also
	    spontaneously update the MT at any time (normally when the
	    path set changes). This is step 1 in
	    <xref target="fig_func_sep_default"/>. In the example, 4
	    paths can be used, hence 3 new ones. Based on the update,
	    the MT can decide whether to establish new subflows, and
	    how many of them. Here, the MT decides to
	    establish one subflow only, and sends a request for
	    endpoint_id to the PM. This is step 2. In step 3, the
	    answer is given: &lt;A2,B2,0,pB2&gt;. The source port is
	    unspecified to allow the MT ensure the unicity of the new
	    endpoint_id, thanks to the new_port() primitive
	    (present in regular TCP as well). Note that messages 1,2,3 need
	    not be real messages and can be function calls instead
	    (as is the case in Linux MPTCP). 
	  </t>

	  <figure align="center" anchor="fig_func_sep_default" title="Functional separation of MPTCP in the transport layer">
            <artwork align="left"><![CDATA[

                            Control plane    
+---------------------------------------------------------------+
|                     Multipath Transport (MT)                  |
+----------------------------------------------------|----------+
  ^                  |           ^                   v  
  |                  |           |          [Build new subsocket,
  | 1.For fendpt_id  |2.endpt_id |                 with endpt_ids
  |<A1,B1,pA1,pB1>   | for path  | 3.<A2,B2,   <A2,B2,new_port(),pB2]
  |Paths 1->4 can be | index 2 ? |   0,pB2>
  |used.             |           |
  |                  |           |
  |                  |           |
  |                  v           |
+---------------------------------------------------------------+
|                         Path Manager (PM)                     |
+---------------------------------------------------------------+
   /                                     \  
  /---------------------------------------\ 
  | mapping table:                        |
  |   Subflow   <--> endpoint_id          |
  |  path index                           |
  |                                       |
  |    [see table below]                  |
  |                                       |
  +---------------------------------------+

            ]]></artwork>
          </figure>

	  <t>
	    The following options, described in
	    <xref target="I-D.ietf-mptcp-multiaddressed"/> , are
	    managed by the Multipath Transport:
	      
	    <list style="symbols">
	      <t>MULTIPATH CAPABLE (MP_CAPABLE): Tells the peer that we
		support MPTCP and announces our local token.
	      </t>
	      <t>MP_JOIN/MP_AUTH: Initiates a new subflow (Note that
		MP_AUTH
		is not yet part of our Linux implementation at the moment)</t>
	      <t>DATA SEQUENCE NUMBER (DSN_MAP): Identifies the position of
		a set of bytes in the meta-flow.</t>
	      <t>DATA_ACK: Acknowledge data at the connection
		level (subflow level acknowledgments are contained in
		the normal TCP header).</t>
	      <t>DATA FIN (DFIN): Terminates a connection.</t>
	      <t>MP_PRIO: Asks the peer to revise the backup status of
	      the subflow on which the option is sent. Although the
	      option is sent by the Multipath Transport (because this
	      allows using the TCP option space), it may be triggered
	      by the Path Manager. This option is not yet supported by our MPTCP
              implementation.</t>
	      <t>MP_FAIL: Checksum failed at
		connection-level. Currently the Linux implementation
		does not implement the checksum in option DSN_MAP, and
		hence does not implement either the MP_FAIL option.</t>
	    </list>
	  </t>

	  <t>
	    The Path manager applies a particular technology to give
	    the MT the possibility to use several paths. The built-in
	    MPTCP Path Manager uses multiple IPv4/v6 addresses as its
	    mean to influence the forwarding of packets through the
	    Internet.

	    When the MT starts a new connection, it chooses a
	    token that will be used to identify the connection. This
	    is necessary to allow future subflow-establishment SYNs
	    (that is, containing the MP_JOIN option) to be attached to
	    the correct connection.
	    
	    An example mapping table is
	    given hereafter:
	  </t>
	  <texttable anchor="table_pm" title="Example mapping table for built-in PM">
	    <ttcol align="center">token</ttcol>
            <ttcol align="center">path index</ttcol>
	    <ttcol align="center">Endpoint id</ttcol>
	    <c>token_1</c>
	    <c>1</c>
	    <c>&lt;A1,B1,0,pB1&gt;</c>
	    
	    <c>token_1</c>
	    <c>2</c>
	    <c>&lt;A2,B2,0,pB1&gt;</c>
	    
	    <c>token_1</c>
	    <c>3</c>
	    <c>&lt;A1,B2,0,pB1&gt;</c>
	    
	    <c>token_1</c>
	    <c>4</c>
	    <c>&lt;A2,B1,0,pB1&gt;</c>

	    <c></c>
	    <c></c>
	    <c></c>
	    
	    <c>token_2</c>
	    <c>1</c>
	    <c>&lt;A1,B1,0,pB2&gt;</c>
	    
	    <c>token_2</c>
	    <c>2</c>
	    <c>&lt;A2,B1,0,pB2&gt;</c>
	    
	  </texttable>
	  
	  <t>
	    <xref target="table_pm" /> shows an example where two
	    MPTCP 
	    connections are active.
            One is identified by token_1, the
	    other one with token_2. As per
	    <xref target="I-D.ietf-mptcp-multiaddressed"/>, the tokens
	    must be unique locally.
	    
	    Since the endpoint identifier may change from one subflow
	    to another, the attachment of incoming new subflows
	    (identified by a SYN + MP_JOIN option) to the right connection
	    is achieved thanks to the locally unique token.
	    
	    The built-in path manager currently implements the following
	    options
	    The following options (defined in 
	    <xref target="I-D.ietf-mptcp-multiaddressed"/>) are
	    intended to be part of the built-in path manager:
	    <list style="symbols">
	      <t>Add Address (ADD_ADDR): Announces a new address we own</t>
	      <t>Remove Address (REMOVE_ADDR): Withdraws a previously
		announced address</t>
	    </list>
	  </t>
	  
          <t> Those options form the built-in MPTCP Path Manager, based
            on declaring IP addresses, and carries control information
            in TCP options. An implementation of Multipath TCP can use
            any Path Manager, but it must be able to fallback to the
            default PM in case the other end does not support the custom
            PM. Alternative Path Managers may be specified in separate
            documents in the future.
	  </t>
      </section>
      <section title="Structure of the Multipath Transport"
	       anchor="sec_structure_mt">
	<t>
	  The Multipath Transport handles three kinds of sockets. We
	  define them here and use this notation throughout the entire document:
	  <list style="symbols">
	    <t>
	      Master subsocket: This is the first socket in use when a
	      connection (TCP or MPTCP) starts. 
	      It is also the only one in use if we
	      need to fall back to regular TCP. This socket is
	      initiated by the application through the socket() system
	      call. Immediately after a new master subsocket is
	      created, MPTCP capability is enabled by the creation of
	      the meta-socket.	      
	    </t>
	    <t>
	      Meta-socket: It holds the multipath control block, and
	      acts as the connection level socket. As data source, it
	      holds the main send buffer. As data sink, it holds the
	      connection-level receive queue and out-of-order queue
	      (used for reordering). We represent it as a normal
	      (extended) socket structure
	      in Linux MPTCP because this allows reusing much of the
	      existing TCP code with few modifications. In particular,
	      the regular socket structure already holds pointers to
	      SND.UNA, SND.NXT, SND.WND, RCV.NXT, RCV.WND (as defined
	      in <xref target="RFC0793"/>). It also holds all the
	      necessary queues for sending/receiving data.
	    </t>
	    <t>
	      Slave subsocket: Any subflow created by MPTCP, in
	      addition to the first one (the master subsocket is
	      always considered as a subflow even though it may be in
	      failed state at some point in the communication). The
	      slave subsockets are created by the kernel (not
	      visible from the application) The
	      master subsocket and the slave subsockets together form
	      the pool of available subflows that the MPTCP Packet
	      Scheduler (called from the meta-socket) can use to send packets.
	    </t>
	  </list>
        </t>
      </section>
      <section title="Structure of the Path Manager">
	<t>	  
	  In contrast to the multipath transport, which is more complex
	  and divided in sub-entities (namely Packet Scheduler,
	  Subflow Interface and Congestion Control, see
	  <xref target="sec_arch" />),
	  the Path Manager just maintains the mapping table and
	  updates the Multipath Transport when the mapping table changes.
	  The mapping table has been described
	  above (<xref target="table_pm" />). 
	  We detail in <xref target="table_pm_events"/> 
	  the set of (event,action) pairs
	  that are implemented in the Linux MPTCP built-in path
	  manager. For reference, an earlier architecture for the Path
	  Management is discussed in <xref target="app_other_pm"/>.
	  Also, <xref target="app_pms"/>
          proposes a small extension to this current architecture to
          allow supporting other path managers.
	</t>
	<!--Inserting big vspace, to force page break,
	  because the array does not fit in the same page otherwise-->
	<t>
	<vspace blankLines="100" />
	</t>
	<texttable anchor="table_pm_events" 
		   title="(event,action) pairs implemented in the 
			  built-in PM">
	  <ttcol align="left">event</ttcol>
          <ttcol align="left">action</ttcol>
	  <c>master_sk bound: This event is triggered upon
	    either a bind(), connect(), or when a new server-side
	    socket becomes established.</c>
	  <c>Discovers the set of local addresses and stores them in
	    local_addr_table</c>
	  
	  <c>ADD_ADDR option received or SYN+MP_JOIN received on 
	    new address</c>
	  <c>Updates remote_addr_table correspondingly</c>
	  
	  <c>local/remote_addr_table updated</c>
	  <c>Updates mapping_table by adding any new address
	    combinations, or removing the ones that have
	    disappeared. Each address pair is given a path
	    index. Once allocated to an address pair, a path index
	    cannot be reallocated to another one, to ensure
	    consistency of the mapping table.</c>	  
	  <c>Mapping_table updated</c>
	  <c>Sends notification to the Multipath Transport. The
	    notification contains the new set of path indices that the
	    MT is allowed to use. This is shown in 
	    <xref target="fig_func_sep_default"/>, msg 1.</c>
	  <c>Endpoint_id(path_index) request received from MT
	    (<xref target="fig_func_sep_default"/>, msg 2)</c>
	  <c>Retrieves the endpoint_ids for the corresponding path
	    index from the mapping table and returns them to the
	    MT. One such request/response is illustrated in 
	    <xref target="fig_func_sep_default"/>, msg 3. Note that
	    in that msg 3, the local port is set to zero. This is
	    to let the operating system choose a unique local port
	    for the new socket.</c>
	</texttable>
      </section>
    </section>
    <section title="MPTCP challenges for the OS">
      <t>
	MPTCP is a major modification to the MPTCP stack. We have
    described above an architecture that
    separates Multipath Transport from Path Management. Path
    Management can be implemented rather simply. But Multipath
    Transport involves a set of new challenges, that do not exist
    in regular TCP. We first describe how an MPTCP client or server
    can start a new connection, or a new subflow within a
    connection. Then we propose techniques (a concrete
    implementation of which is done in Linux MPTCP) to efficiently implement
    data reception (at the data sink) and data sending
    (at the data source).
    </t>
      <section title="Charging the application for its CPU cycles"
	       anchor="sec_cpu_cycles">
	<t>
	  As this document is about implementation, it is important not
	  only to ensure that MPTCP is fast, but also that it is fair to
	  other applications that share the same CPU. Otherwise one could
	  have an extremely fast file transfer, while the rest of the
	  system is just hanging. CPU fairness is ensured by the scheduler of
          the Operating System when things are implemented in user space. But in the
	kernel, we can choose to run code in "user context", that is,
	in a mode where each CPU cycle is charged to a particular
	application. Or we can (and must in some cases) run code in
	"interrupt context", that is, interrupting everything else
	until the task has finished.
	In Linux (probably a similar thing is true in other systems), the
	arrival of a new packet on a NIC triggers a hardware
	interrupt, which in turn schedules a software interrupt that
	will pull the packet from the NIC and perform the initial
	processing. The challenge is to stop the processing of the incoming
        packet in software interrupt as soon as it can be attached to
	a socket, and wake up the application.  With TCP, an
	additional constraint is that incoming data should be
	acknowledged as soon as possible, which requires
	reordering. Van Jacobson has proposed a solution for this
	<xref target="VJ_prequeues"/>: If an application is waiting on
	a recv() system call, incoming packets can be put into a
	special queue (called prequeue in Linux) and the application is
	woken up. Reordering and acknowledgement are then performed in
	user context. The execution path for outgoing packets is less
	  critical from that point of view, because the vast majority
	  of processing can be done very easily in user context.
	</t>
	<t>
	  In this document, when discussing CPU fairness, we
	  will use the following terms:
	  <list style="symbols">
	    <t>
	      User context: Execution environment that is under
	      control of the OS scheduler. CPU cycles are charged to
	      the associated application, which allows to ensure fairness with
              other applications.
	    </t>
	    <t>
	      Interrupt context: Execution environment that runs with
	      higher priority than any process. Although it is impossible to
              completely avoid running code in interrupt context, it is
              important to minimize the amount of code running in such a context.
	    </t>
	    <t>
	      VJ prequeues: This refers to Van Jacobson prequeues, as
              explained above<xref target="VJ_prequeues"/>.
	    </t>
	  </list>
	</t>
      </section>
    <section title="At connection/subflow establishment">
      <t>
	As described in <xref target="I-D.ietf-mptcp-multiaddressed"/>,
	the establishment of an MPTCP connection is quite simple, being
	just a regular three-way exchange with additional options. As
	shown in <xref target="sec_structure_mt"/> this is done in the
	master subsocket. Currently Linux MPTCP attaches a meta-socket
	to a socket as soon as it is created, that is, upon a socket()
	system call (client side), or when a server side socket enters
	the ESTABLISHED state. An alternate solution is described in 
	<xref target="app_new_metasock"/>.
      </t>
      <t>
	An implementation can choose the best moment,
	maybe depending on the OS, to instantiate the
	meta-socket. However, if this meta-socket is needed to
	accept new subflows (like it is in Linux MPTCP), it should
	be attached at the latest when the MP_CAPABLE option is
	received. Otherwise incoming new subflow requests
	(SYN + MP_JOIN) may be lost, requiring retransmissions by the
	peer and delaying the subflow establishment.
      </t>
      <t>
	The establishment of subflows, on the other hand, is more
	tricky. The problem is that new SYNs (with the MP_JOIN option)
	must be accepted by a socket (the meta-socket in the proposed
	design) as if it was in LISTEN state, while its state is
	actually ESTABLISHED. There is the following in common with a
	LISTEN socket:
	<list style="symbols">
	  <t>Temporary structure: Between the reception of the SYN and the final ACK, a
	    mini-socket is used as a temporary structure.</t>
	  <t>Queue of connection requests: The meta-socket, like a
	    LISTEN socket, maintains
	    a list of pending connection requests. There are two such
	    lists. One contains mini-sockets, because the final ACK
	    has not yet been received. The second list contains sockets in the
            ESTABLISHED state that have not yet been accepted. "Accepted" means,
            for regular TCP, returned to
	    the application as a result of an accept() system
	    call. For MPTCP it means that the new subflow has been
	    integrated in the set of active subflows.</t>
	</list>
	We can list the following differences with a normal LISTEN
	socket.
	<list style="symbols">
	  <t>Socket lookup for a SYN: When a SYN is received, 
	    the corresponding
	    LISTEN socket is found by using the endpoint_id. This is
	    not possible with MPTCP, since we can receive a SYN on any
            endpoint_id. Instead, the token must be used to retrieve
	    the meta-socket to which the SYN must be attached. A new
	    hashtable must be defined, with tokens as keys.</t>
	  <t>Lookup for connection request: In regular TCP, this
	    lookup is quite similar to the previous one (in Linux at
	    least). The 5-tuple is used, first to find the LISTEN
	    socket, next to retrieve the corresponding mini-socket, stored
	    in a private hashtable inside the LISTEN socket. With
	    MPTCP, we cannot do that, because there is no way to
	    retrieve the meta-socket from the final ACK. The
	    5-tuple can be anything, and the token is only present in the SYN.
            There is no token in the final ACK. Our Linux MPTCP implementation
            uses a global
	    hashtable for pending connection requests, where the key
	    is the 5-tuple of the connection request.
	  </t>
	</list>
      </t>
      <t>
	An implementation must carefully check the
	presence of the MP_JOIN option in incoming SYNs before performing the
        usual socket lookup. If it is present, only the
	token-based lookup must be done. If this lookup does not return a
        meta-socket, the SYN must be discarded. Failing to do
	that could lead to mistakenly attach the incoming SYN to a
	LISTEN socket instead of attaching it to a meta-socket. 
      </t>
    </section>
    <section title="Subflow management" anchor="sec_subflow_mgmnt">
      <t>
        Further research is needed to define the appropriate heuristics to solve
        these problems. Initial thoughts are provided in
        <xref target="app_subflow_mgmnt"/>.
      </t>
      <t>
	Currently, in a Linux MPTCP client, 
	the Multipath Transport tries to
	open all subflows advertised by the Path Manager. On the
	other hand, the server only accepts new subflows, but does
	not try to establish new ones. The rationale for this is
	that the client is the connection initiator. New subflows are only
        established if the initiator requests them. This is subject to change in
        future releases of our MPTCP implementation.
      </t>
    </section>
    <section title="At the data sink">
      <t>
	There is a symmetry between the behavior of the data source
	and the data sink. Yet, the specific requirements are
	different. The data sink is described in this section while the data
        source is described in the next section.
      </t>
      <section title="Receive buffer tuning" anchor="sec_rcv_buf">
	<t>
	  The MPTCP required receive buffer is larger than the sum
	  of the buffers required by the individual subflows. The
	  reason for this and proper values for the buffer are
	  explained in <xref target="I-D.ietf-mptcp-architecture"/>
	  Section 5.3. Not following this could result in the MPTCP speed
	  being capped at the bandwidth of the slowest subflow.
	</t>
	<t>
	  An interesting way to dynamically tune the receive buffer
	  according the bandwidth/delay product (BDP) of a path, 
	  for regular TCP, is
	  described in <xref target="Fisk_Dynamic"/> and implemented
	  in recent Linux kernels. It uses the COPIED_SEQ sequence
	  variable (sequence number of the next byte to copy to the
	  app buffer) to count, every RTT, the number of bytes received during
	  that RTT. This number of bytes is precisely the BDP. The
	  accuracy of this technique is directly dependent on the
	  accuracy of the RTT estimation. Unfortunately, the data sink does not
          have a reliable estimate of the SRTT. To solve this,
	  <xref target="Fisk_Dynamic"/> proposes two techniques:
	  <list style="numbers">
	    <t>Using the timestamp option (quite accurate).
	    </t>
	    <t>
	      Computing the time needed to receive one RCV.WND
	      <xref target="RFC0793"/> worth of data. It is less
	      precise and is used only to
	      compute an upper bound on the required receive buffer.
	    </t>
	  </list>

	  As described in
	  <xref target="I-D.ietf-mptcp-multiaddressed"/>, section
	  3.3.3, the MPTCP advertised receive window is shared by all
	  subflows. Hence, no per-subflow information can be deduced
	  from it, and the second technique from
	  <xref target="Fisk_Dynamic"/> cannot be used.

	  <xref target="I-D.ietf-mptcp-architecture"/> mentions that
	  the allocated connection-level receive buffer should be
          2*sum(BW_i)*RTT_max, where BW_i is the bandwidth seen by subflow i and
          RTT_max is the maximum RTT estimated among all the subflows.
          This is achieved in Linux MPTCP by slightly
	  modifying the first tuning algorithm from
	  <xref target="Fisk_Dynamic"/>, and disabling the second one.
	  The modification consists in counting on each subflow, every RTT_max
          the number of bytes received during that time on this subflow.
	  Per subflow, this provides its contribution to the total
	  receive buffer of the connection. This computes the contribution of
          each subflow to the total receive buffer of the connection.
	</t>
      </section>
      <section title="Receive queue management">
        <t>	  
	  As advised in
	  <xref target="I-D.ietf-mptcp-multiaddressed"/>, Section 3.3.1,
	  "subflow-level processing should be undertaken separately 
	  from that at connection-level". This also has the
	  side-effect of allowing much code reuse from the regular TCP stack.

	  A regular TCP stack (in Linux at least) maintains a receive
	  queue (for storing incoming segments until the application
	  asks for them) and an out-of-order queue (to allow reordering).

	  In Linux MPTCP, the subflow-level
	  receive-queue is not used. Incoming segments
	  are reordered at the subflow-level, just as if they were
	  plain TCP data. But once the data is in-order at the subflow level, it
          can be immediately handed to MPTCP (See Figure 7 of
	  <xref target="I-D.ietf-mptcp-architecture"/>) for
	  connection-level reordering. The role of the subflow-level
	  receive queue is now taken by the MPTCP-level receive queue.
	  In order to maximize the CPU cycles spent in user context
	  (see <xref target="sec_cpu_cycles"/>), VJ prequeues can be
	  used just as in regular TCP (they are not yet supported in
	  Linux MPTCP, though).
        </t>

          <t>
	  An alternate design, where the subflow-level receive
	  queue is kept active and the MPTCP receive queue is not
	  used, is discussed in <xref target="app_recv_queue"/>.
          </t>
      </section>
      <section title="Scheduling data ACKs">
	<t>
	  As specified in
	  <xref target="I-D.ietf-mptcp-multiaddressed"/>, Section
	  3.3.2, data ACKs not only help the sender in having a
	  consistent view of what data has been correctly received at
	  the connection level. They are also used as the left edge of
	  the advertised receive window.
	</t>
	<t>
	  In regular TCP, if a receive buffer becomes full,
	  the receiver announces a receive window. When finally some
	  bytes are given to the application, freeing space in the
	  receive buffer, a duplicate ACK is sent to act as a window
	  upate, so that the sender knows it can transmit again.
	  Likewise, when the MPTCP shared receive buffer becomes full,
	  a zero window is advertised. When some bytes are delivered to
	  the application, a duplicate DATA_ACK must be sent to act as a
	  window update. 
	  Such an important DATA_ACK should be sent on all subflows,
	  to maximize the probability that at least one of them
	  reaches the peer. If, however, all DATA_ACKs are lost, there
	  is no other option than relying on the window probes
	  periodically sent by the data source, as in regular TCP.
	</t>
	<t>
	  In theory a DATA_ACK can be sent on any subflow, or even on
	  all subflows, simultaneously. As of version 0.5,
	  Linux MPTCP simply adds the
	  DATA_ACK option to any outgoing segment (regardless of
	  whether it is data or a pure ACK). There is thus no
	  particular DATA_ACK scheduling policy. The only exception is
	  for a window update that follows a zero-window. In this case, the
          behavior is as described in the previous paragraph.
        </t>
      </section>
    </section>
    <section title="At the data source">
      <t>
      In this section we mirror the topics of the previous section, in
      the case of a data sender. The sender does not have the same view of
      the communication, because one has information that the other
      can only estimate. Also, the data source sends data and receives
      acknowledgements, while the data sink does the reverse. This results in a
      different set of problems to be dealt with by the data source.
      </t>
      <section title="Send buffer tuning" anchor="sec_sndbuf_tuning">
	<t>
	  As explained in <xref target="I-D.ietf-mptcp-architecture"/>,
	  end of Section 5.3, the send buffer should have the same size
	  as the receive buffer. At the sender, we don't have the RTT
	  estimation problem described in <xref target="sec_rcv_buf"/>,
	  because we can reuse the built-in TCP SRTT (smoothed RTT).
	  Moreover, the sender has the congestion window,
	  which is itself an estimate of the BDP, and is used in Linux
	  to tune the send buffer of regular TCP.
	  Unfortunately, we cannot use the congestion window with MPTCP,
	  because the buffer equation does not involve the
	  product BW_i*delay_i for the subflows (which is what the congestion
	  window estimates), but it involves BW_i*delay_max, where
	  delay_max is the maximum observed delay across all subflows.

	  An obvious way to compute the contribution of each subflow
	  to the receive buffer would be: 2*(cwnd_i/SRTT_i)*SRTT_max.
	  However, some care is needed because of the variability of
	  the SRTT (measurements show that, even smoothed, the SRTT is
	  not quite stable). Currently Linux MPTCP estimates the
	  bandwidth periodically by checking the sequence number
	  progress. This however introduces new mechanisms in the
	  kernel, that could probably be avoided. Future experience
	  will tell what is appropriate.
	</t>
      </section>
      <section title="Send queue management" anchor="sec_send_queue">
	<t>
	  As MultiPath TCP involves the use of several TCP subflows, a
	  scheduler must be added to decide where to send
	  each byte of data. Two possible places for the scheduler
	  have been evaluated for Linux MPTCP. 
	  One option is to schedule data as soon
	  as it arrives from the application buffer. This option,
	  consisting in <spanx style="emph">pushing</spanx> 
	  data to subflows as soon as it is
	  available,  was
	  implemented in older versions of Linux MPTCP and is now
	  abandoned. We keep a description of it (and why it has been
	  abandoned) in 
	  <xref target="app_sep_send_queues"/>. Another option is to
	  store all data centrally in the Multipath Transport, inside
	  a shared send buffer (see <xref target="fig_arch_shared_sndbuf"/>). 
	  Scheduling is then done at
	  transmission time, whenever any subflow becomes ready to
	  send more data (usually due to acknowledgements having
	  opened space in the congestion window). In that scenario,
	  the subflows <spanx style="emph">pull</spanx> segments from
	  the shared send queue whenever they are ready. Note that
	  several subflows can become ready simultaneously, if an
	  acknowledgement advertises a new receive window, that opens
	  more space in the shared send window. For that reason, when
	  a subflow pulls data, the Packet Scheduler is run and other
	  subflows may be fed by the Packet Scheduler in the same time.
	  <figure align="center" anchor="fig_arch_shared_sndbuf"
	  title="Send queue configuration">
            <artwork align="left"><![CDATA[
                            Application
                                |
                                v
                              | * |
 Next segment to send (A)  -> | * |
                              |---| <- Shared send queue
Sent, but not DATA-acked(B)-> |_*_|
                                |
                                v
                          Packet Scheduler
                               /  \
                              /    \
                             |      |
                             v      v
Sent, but not acked(B)  ->  |_|    |_| <- Subflow level congestion
                             |      |     window
                             v      v
                            NIC    NIC
            ]]></artwork>
          </figure>
	  This
	  approach, similar to the one proposed in <xref target="Hsieh_ptcp"/>,
	  presents several advantages:
	  <list style="symbols">
	    <t>
	      Each subflow can easily fill its pipe. (As long as there is
	      data to pull from the shared send buffer, and the
	      scheduler is not applying a policy that restricts the subflow).
	    </t>
	    <t>
	      If a subflow fails, it will no longer receive
	      acknowledgements, and hence will naturally stop
	      pulling from the shared send buffer. This removes the
	      need for an explicit "failed state", to ensure that a
	      failed subflow does not receive data (As opposed to
	      e.g. SCTP-CMT, that needs an explicit marking of failed
	      subflows by design, because it uses a single sequence
	      number
	      space <xref target="I-D.tuexen-tsvwg-sctp-multipath"/>).
	    </t>
	    <t>
	      Similarly, when a failed subflow becomes active again,
	      the pending segments of its congestion window are
	      finally acknowledged, allowing it to pull again from the
	      shared send buffer. Note that in such a case, the
	      acknowledged data is normally just dropped by the
	      receiver, because the corresponding segments have been
	      retransmitted on another subflow during the failure
	      time.
	    </t>
	  </list>
	  Despite the adoption of that approach in Linux MPTCP, there
	  are still two drawbacks:
	  <list style="symbols">
	    <t>
	      There is one single queue, in the Multipath Transport,
	      from which all subflows pull segments. In Linux, queue
	      processing is optimized for handling segments, not
	      bytes. This implies that the shared send queue must
	      contain pre-built segments, hence requiring the 
	      <spanx style="emph">same</spanx> MSS to be used for
	      all subflows. We note however that today, the most
	      commonly negotiated MSS is around 1380 bytes
	      <xref target="Barre_Multipath"/>, so this approach
	      sounds reasonable. Should this requirement become too
	      constraining in the future, a more flexible approach
	      could be devised (e.g., supporting a few Maximum Segment Sizes).
	    </t>
	    <t>
	      Because the subflows pull data whenever they get new
	      free space in their congestion window, the Packet
	      Scheduler must run at that time. But that time most
	      often corresponds to the reception of an
	      acknowledgement, which happens in interrupt context
	      (see <xref target="sec_cpu_cycles"/>). This is both
	      unfair to other system processes, and slightly
	      inefficient for high speed communications. The problem
	      is that the packet scheduler performs more operations that the
              usual "copy
	      packet to NIC". One way to solve this problem would be
	      to have a small subflow-specific send queue, which would
	      actually lead to a hybrid architecture between the pull
	      approach (described here) and the push approach
	      (described in
	      <xref target="app_sep_send_queues"/>). Doing that would
	      require solving non-trivial problems, though, and
	      requires further study.
	    </t>
	  </list>
	</t>
	<t>
	  As shown, in <xref target="fig_arch_shared_sndbuf"/>, a
	  segment first enters the shared send queue, then, when
	  reaching the bottom of that queue, it is pulled by some
	  subflow. But to support failures, we need to be able to move
	  segments from one subflow to another, so that the failure is
	  invisible from the application. In Linux
	  MPTCP, the segment data is kept in the Shared send queue (B
	  portion of the queue). When a subflow pulls a segment, it
	  actually only copies the control structure (struct sk_buff)
	  (which Linux calls packet cloning) and increments its
	  reference count. The following event/action table summarizes these operations:
	</t>
	<texttable anchor="table_send_queues" 
		   title="(event,action) pairs implemented in the 
			  Multipath Transport queue management">
	  <ttcol align="left">event</ttcol>
          <ttcol align="left">action</ttcol>
	  <c>Segment acknowledged at subflow level</c>
	  <c>Remove references to the segment from the subflow-level queue</c>
	  
	  <c>Segment acknowledged at connection level</c>
	  <c>Remove references to the segment from the connection-level queue</c>
	  
	  <c>Timeout (subflow-level)</c>
	  <c>Push the segment to the best running subflow (according
	    to the Packet Scheduler). If no subflow is available, push
	    it to a temporary retransmit queue (not represented in 
	    <xref target="fig_arch_shared_sndbuf"/>) for future pulling by an
	    available subflow. The retransmit queue is parallel to
	    the connection level queue and is read with higher priority.</c>
	  <c>Ready to put new data on the wire 
	    (normally triggered by an incoming ack)</c>
	  <c>If the retransmit queue is not empty, first pull from
	    there. Otherwise, then take new segment(s) from the
	    connection level send queue (A portion). The pulling
	    operation is a bit special in that it can result in
	    sending a segment over a different subflow than the one
	    which initiated the pull. This is because the Packet
	    Scheduler is run as part of the pull, which can result in
	    selecting any subflow. In most cases, though, the subflow which
	    originated the pull will get fresh data, given it has
	    space for that in the congestion window. Note that the
	    subflows have no A portion in
	    <xref target="fig_arch_shared_sndbuf"/>, because they
	    immediately send the data they pull. </c>
	</texttable>		  	  
	<t>
	  IMPORTANT: A subflow can be stopped from transmitting by the
	  congestion window, but also by the send window (that is, the
	  receive window announced by the peer). Given that the receive
	  window has a connection level meaning, a DATA_ACK arriving
	  on one subflow could unblock another subflow. Implementations
	  should be aware of this to avoid stalling part of the
	  subflows in such situations. In the case of Linux MPTCP, that
	  follows the above architecture, this is ensured by running
	  the Packet Scheduler at each pull operation. This is not
	  completely optimal, though, and may be revised when more
	  experience is gained.
	</t>
      </section>
      <section title="Scheduling data">
	<t>
          As several subflows may be used to transmit data, MPTCP must select a
          subflow to send each data. First, we need to know
	  which subflows are available for sending data. The mechanism
	  that controls this is the congestion controller, which
	  maintains a per-subflow congestion window. The aim of a
	  Multipath congestion controller is to move data away from congested
	  links, and ensure fairness when there is a shared bottleneck. The handling of
	  the congestion window is explained in <xref target="sec_ccc"/>.
	  Given a set of available subflows (according to the
	  congestion window), one of these has to be selected by the
	  Packet Scheduler. The role of the Packet Scheduler is to
	  implement a particular policy, as will be explained in
	  <xref target="sec_pkt_sched"/>.
	</t>
	<section title="The congestion controller" anchor="sec_ccc">
            <t>
	      The Coupled Congestion Control provided in Linux MPTCP
	      implements the algorithm defined in <xref target="I-D.ietf-mptcp-congestion"/>.

	      Operating System kernels (Linux at least) do not support
	      floating-point numbers for efficiency reasons. 
	      <xref target="I-D.ietf-mptcp-congestion"/> makes
	      an extensive use of them, which must be worked around.
	      Linux MPTCP solves that by performing fixed-point
	      operations using a minimum number of fractions and
	      performs scaling when divisions are necessary.
	    </t>
	    <t>
	      Linux already includes a work-around for floating point
	      operations in the Reno congestion avoidance implementation. Upon
              reception of an ack, the congestion window (counted in
	      segments, not in bytes as proposed in 
	      <xref target="I-D.ietf-mptcp-congestion"/> does) should be
	      updated as cwnd+=1/cwnd. Instead, Linux increments the
	      separate variable snd_cwnd_cnt, until
	      snd_cwnd_cnt>=cwnd. When this happens, snd_cwnd_cnt is
	      reset, and cwnd is incremented. Linux MPTCP reuses this
	      to update the window in the CCC (Coupled Congestion
	      Control) congestion avoidance phase: snd_cwnd_cnt is
	      incremented as previously explained, and cwnd is
	      incremented when 
	      snd_cwnd_cnt >= max(tot_cwnd / alpha, cwnd)
	      (see <xref target="I-D.ietf-mptcp-congestion"/>). Note
	      that the bytes_acked variable, present in
	      <xref target="I-D.ietf-mptcp-congestion"/>, is not
	      included here because Linux MPTCP does not currently
	      support ABC <xref target="RFC3465"/>,
	      but instead considers acknowledgements in MSS
	      units. Linux uses for ABC, in Reno, the bytes_acked
	      variable instead of snd_cwnd_cnt. For Reno, cwnd is
	      incremented by one if bytes_acked>=cwnd*MSS. Hence,
	      in the case of a CCC with ABC, one would increment cwnd
	      when bytes_acked>=max(tot_cwnd*MSS / alpha, cwnd*MSS).
            </t>
            <t>
	      Unfortunately, the alpha parameter mentioned above
	      involves many fractions. 	      
	      The current implementation of MPTCP uses a rewritten version of the alpha
	      formula from <xref target="I-D.ietf-mptcp-congestion"/>:
            </t>

   <figure>
    <artwork align="left">
                          cwnd_max * scale_num
alpha = tot_cwnd * ----------------------------------
                  /     rtt_max * cwnd_i * scale_den \ 2
                  | sum -----------------------------|
                  \  i              rtt_i            /
    </artwork>
</figure>

            <t>
This computation assumes that the MSS is shared by all
subflows, which is true under the architecture described in
<xref target="sec_send_queue" /> but implies that implementations
choosing to support several MSS cannot use the above simplified equation.
The variables cwnd_max and rtt_max in the above equation are NOT
resp. the maximum congestion
window and RTT across all subflows. Instead, they are the values of
subflow i such that
cwnd_i / rtt_i² is maximum. This corresponds to the numerator of the
equation provided in <xref target="I-D.ietf-mptcp-congestion"/>.
            </t>

            <t>
scale_num and scale_den have to be selected in such a way that scale_num >
scale_den^2. A good choice is to use scale_num=2^32 (using 64 bits
arithmetic) and scale_den=2^10.
In that case the final alpha value is scaled by 2^12, which gives a reasonable precision. Due to the scaling, it is
necessary to also scale later in the formula that decides whether an increase
of the congestion window is necessary or not: snd_cwnd_cnt >=
max((tot_cwnd&lt;&lt;12) / alpha,cwnd).
            </t>
	  </section>
	  <section title="The Packet Scheduler" anchor="sec_pkt_sched">
            <t>
	      Whenever the Congestion Controller (described above)
	      allows new data for at least one subflow, the Packet
	      Scheduler is run. When only one subflow is available
	      the Packet Scheduler just decides which packet to pick
	      from the A section of the shared send buffer (see
	      <xref target="fig_arch_shared_sndbuf"/>). Currently
	      Linux MPTCP picks the bottom most segment. If more than
	      one subflow is available, there are three decisions to
	      take:
	      <list style="symbols">
		<t>
		  Which of the subflows to feed with fresh data:
		  As the only Packet Scheduler currently supported in
		  Linux MPTCP aims at filling all pipes, it always
		  feeds data to all subflows as long as there is data
		  to send. 
		</t>
		<t>
		  In what order to feed selected subflows: 
		  when several subflows become
		  available simultaneously, they are fed by order of
		  time-distance to the client. We define the
		  time-distance as the time needed for the packet to
		  reach the peer if given to a particular
		  subflow. This time depends on the RTT, bandwidth and
		  queue size (in bytes), as follows: 
		  time_distance_i = queue_size_i/bw_i+RTT_i.
		  Given that with the architecture described in
		  <xref target="sec_send_queue"/>, the
		  subflow-specific queue size cannot exceed a
		  congestion window, the time_distance becomes
		  time_distance_i~=RTT_i. This scheduling policy
		  favors fast subflows for application-limited
		  communications (where all subflows need not be
		  used). However, for network-limited communications,
		  this scheduling policy has little effect because all
		  subflows will be used at some point, even the slow
		  ones, to try minimizing the connection-level completion time.
		</t>
		<t>
		  How much data to allocate to a single subflow: this
		  question concerns the granularity of the
		  allocation. Using big allocation units allows
		  for better support of TCP Segmentation Offload
		  (TSO). TSO allows the system to aggregate several
		  times the MSS into one single segment, sparing
		  memory and CPU cycles, by leaving the fragmentation
		  task to the NIC. However, this is only possible if
		  the large single segment is made of contiguous data,
		  at the subflow level and the connection level (see
		  also important note below).
		</t>
	      </list>
	      </t>
	      <t>
		IMPORTANT: When scheduling data to subflows, an
		implementation must be careful that if two segments are
		contiguous at the subflow-level, but non-contiguous at
		the connection level, they cannot be aggregated into
		one. As Linux (and probably other systems) merges
		segments when it is under memory pressure, it could
		easily decide to merge non-contiguous MPTCP segments,
		simply because they look contiguous from the subflow
		viewpoint. This must be avoided, because the DATA_SEQ
		mapping option would loose its meaning in such a case,
		leading to all possible kinds of misbehaviors.
	      </t>
	  </section>
      </section>
    </section>
    <section title="At connection/subflow termination">
      <t>
	In Linux MPTCP, subflows are terminated only when the whole
	connection terminates, because the heuristic for terminating
	subflows (without closing the connection) is not yet mature,
	as explained in <xref target="sec_subflow_mgmnt"/>. 	
      </t>
      <t>
	At connection termination, an implementation must ensure that
	all subflows plus the meta-socket are cleanly removed. The
	obvious choice to propagate the close() system call on all
	subflows does not work. The problem is that a close() on a
	subflow appends a FIN at the end of the send queue. If we
	transpose this to the meta-socket, we would append a DATA_FIN
	on the shared send queue (see
	<xref target="sec_send_queue"/>). 
	That operation results in the shared
	send queue not accepting any more data from the application,
	which is correct. It also results in the subflow-specific
	queues not accepting any more data from the shared send
	queue. The shared send queue 
	may however still be full of
	segments, which will never be sent because all gates are
	closed.
      </t>
      <t>
	IMPORTANT: Upon a close() system call, an implementation must
	refrain from sending a FIN on all subflows, unless the
	implementation uses an architecture with no connection-level
	send queue (like the one described in
	<xref target="app_sep_send_queues"/>). Even in that case, it
	makes sense to keep all subflows open until the last byte is
	sent, to allow retransmission on any path, should any one of
	them fail.
      </t>
      <t>
	Currently, upon a close() system call, Linux MPTCP appends a
	DATA_FIN to the connection-level send queue. Only when that
	DATA_FIN reaches the bottom of the send queue is the regular
	FIN sent on all subflows.
      </t>
      <t>
	DISCUSSION: In the Linux MPTCP behavior described above, a
	connection could still stall near its end if one path fails
	while transmitting its last congestion window of data (because
	the maximum size of the subflow-specific send queue is cwnd).
	This can be avoided by waiting just a bit more before to
	trigger the subflow-FIN: Instead of sending the FIN together
	with the DATA_FIN, send the DATA_FIN alone and wait for the
	corresponding DATA_ACK to trigger a FIN on all subflows. This
	however augments by one RTT the duration of the overall
	connection termination.
      </t>
    </section>
    </section>
    <section title="Configuring the OS for MPTCP" anchor="sec_conf">
      <t>
Previous sections concentrated on implementations. In this section, we
try to gather guidelines that help getting the full potential from
MPCTP through appropriate system configuration. Currently those
guidelines apply especially to Linux, but the principles can be
applied to other systems.
      </t>
    <section title="Source address based routing" anchor="sec_policyrouting">
        <t>
As already pointed out by <xref target="I-D.ietf-mif-problem-statement"/>, the
default behavior of most operating systems is not appropriate for the use of
multiple interfaces. Most operating systems are typically configured
to use at most one IP address at a time. It is more and more
common to maintain several links in up state (e.g. using the wired
interface as main link, but maintaining a ready-to-use wireless
link in the background, to facilitate fallback when the wired link
fails). But MPTCP is not about that. MPTCP is
about <spanx style="emph">simultaneously</spanx> using several
interfaces (when available). It is expected that one of the mostly
used MPTCP configurations will be through two or more NICs, each being
assigned a different address. Another possible configuration would be
to assign several IP addresses to the same interface, in which case
the path diverges later in the network, based on the particular
address that is used in the packet.
        </t>

        <t>
Usually an operating system has a single default route, with a single source IP
address. If the host has several IP addresses and we want to do MultiPath TCP,
it is necessary to configure source address based routing. This means that
based on the source address, selected by the MultiPath TCP-module in the
operating system, the routing-decision is based on a different routing table.
Each of these routing tables defines a default route to the
Internet. This is different from defining several default routes in
the same routing table (which is also supported in Linux), because in
that case only the first one is used. Any additional default route is
considered as a fallback route, used only in case the main one fails.
        </t>

        <t>
It is easier to understand the necessary configuration by means of an
example. Let a host have two interfaces,I1 and I2, both connected to
the public Internet and being assigned addresses resp. A1 and A2. 
Such a host needs 3 routing tables.
One of them is the classical default routing table, present in all
systems.
The default routing table is used to find a route based on the
destination address only, when a segment is issued with the
undetermined source address. The undetermined source address is
typically used by applications that initiate a TCP connect() system
call, specifying the destination address but letting the system choose
the source address. In that case, after the default routing table has
been consulted, an address is assigned to the socket by the system by
applying <xref target="RFC3484"/>.

The additional routing tables are used when the source address is
specified. If the source address has no impact on the route that
should be chosen, then the default routing table is sufficient. But this
is a particular case (e.g., a host connected to one network only, but
using two addresses to exploit ECMP paths later in the network). In
most cases, a source address is attached to a specific interface, or
at least a specific gateway. Both of those cases require defining a
separate routing table, one per (gateway, outgoing interface) pair.

To select the proper routing
table based on the source address, an additional indirection level
must be configured. It is called "policy routing" in Linux and is
illustrated at the bottom of <xref target="fig_policy"/>.
</t>

<figure align="center" anchor="fig_policy" title="Routing table
configuration for MultiPath TCP">
<artwork align="left"><![CDATA[
+----------------------------------------------------+
|                   Default Table                    |
+----------------------------------------------------+
| Dst: 0.0.0.0/0  Via: Gateway-IP1 Dev: I1           |
| Dst: 0.0.0.0/0  Via: Gateway-IP2 Dev: I2           |
| Dst: Gateway1-Subnet Dev: I1 Src: A1  Scope: Link  |
| Dst: Gateway2-Subnet Dev: I2 Src: A2  Scope: Link  |
+----------------------------------------------------+

+----------------------------------------------------+
|                      Table 1                       |
+----------------------------------------------------+
| Dst: 0.0.0.0/0  Via: Gateway-IP1 Dev: I1           |
| Dst: Gateway1-Subnet Dev: I1 Src: A1 Scope: Link   |
+----------------------------------------------------+

+----------------------------------------------------+
|                      Table 2                       |
+----------------------------------------------------+
| Dst: 0.0.0.0/0  Via: Gateway-IP2 Dev: I2           |
| Dst: Gateway2-Subnet Dev: I2 Src: A2 Scope: Link   |
+----------------------------------------------------+

+----------------------------------------------------+
|                 Policy Table                       |
+----------------------------------------------------+
|   If src == A1 , Table 1                           |
|   If src == A2 , Table 2                           |
+----------------------------------------------------+
]]></artwork>
</figure>

<t>
If only the default routing table were used, only the first default
route would be used, regardless of the source address. For example, a
packet with source address A2, would leave the host through interface
I1, which is incorrect.
</t>
    </section>
    <section title="Buffer configuration">
      <t>
<xref target="I-D.ietf-mptcp-architecture"/>, Section 5.3 describes in
details the new, higher buffer requirements of MPTCP.
<xref target="sec_rcv_buf"/> and <xref target="sec_sndbuf_tuning"/>
describe how the MPTCP buffers can be tuned dynamically. However, it
is important to note that even the best tuning is capped by a maximum
configured at the system level.

When using MultiPath TCP, the
maximum receive and send buffer should be configured to a higher value
than for regular TCP. There is no universal guideline on what
value is best there. Instead the most appropriate action, for an
administrator, is probably to roughly estimate the maximum bandwidth and delay
that can be observed on a particular connectivity setup,
and apply the equation from
<xref target="I-D.ietf-mptcp-architecture"/>, Section 5.3 to find a
reasonable tradeoff. This exercise could lead an administrator to
decide to disable MPTCP on some interfaces, because it allows
consuming less memory while still achieving reasonable performance.
      </t>
    </section>
  </section>
  <section title="Future work" anchor="sec_future">
    <t>
A lot of work has yet to be done, and there is much space for
improvements. In this section we try to assemble a list of future
improvements that would complete this guidelines.
    </t>
    <t>
      <list style="symbols">
	<t>
Today's host processors have more and more CPU cores. Given Multipath
TCP tries to exploit another form of parallelism, there is a challenge
in finding how those they can work together optimally. An important
question is how to work with hardware that behaves intelligently with
TCP (e.g. flow to core affinity). This problem is discussed in more details in
<xref target="Watson_offload"/>.
	</t>
        <t>
	  An evaluation of Linux MPTCP exists <xref target="Barre_Multipath"/>.
	  But many optimizations are still possible and should be evaluated. Examples of them VJ prequeues
	  (<xref target="sec_cpu_cycles"/>), MPTCP fast path (that is,
	  a translation of the existing TCP fast path to MPTCP) or DMA support.

          VJ prequeues, described in <xref target="sec_cpu_cycles"/>,
          are intended to defer segment processing until the
          application is awoken, when possible.
        </t>
        <t> Currently, support for TCP Segmentation Offload remains a 
	  challenge because it plays with the Maximum Segment
	  Size. Linux MPTCP currently works with a single MSS across
	  all subflows (see <xref target="sec_send_queue"/>). Adding TSO support to MPTCP
	  is certainly possible, but requires further work
	  (<xref target="sec_send_queue"/>). Also, support for Large
	  Receive Offload has not been investigated yet.
	</t>
	<t>
	  There are ongoing discussions on heuristics that would be
	  used to decide when to start new subflows. Those discussions
	  are summarized in <xref target="app_subflow_mgmnt"/>, but
	  none of the proposed heuristics have been evaluated yet.
	</t>
      </list>
    </t>
  </section>
  <section title="Acknowledgements">
    <t>
      Sébastien Barré, Christoph Paasch and Olivier Bonaventure are
      supported by Trilogy (http://www.trilogy-project.org), a research
      project (ICT-216372) partially funded by the European Community under
      its Seventh Framework Program.  The views expressed here are those of
      the author(s) only.  The European Commission is not liable for any
      use that may be made of the information in this document.
    </t>
    <t>
      The authors gratefully acknowledge Costin Raiciu, who wrote a
      userland implementation of MPTCP and provided insight on
      implementation matters during
      several fruitful debates. Discussions with Janardhan Iyengar
      also helped understanding the specificities of MPTCP compared to
      SCTP-CMT.
    </t>
    <t>
      The authors would also like to thank the following people for
      useful discussions on the mailing list and/or reviews: Alan Ford, 
      Bob Briscoe,  Mark
      Handley, Michael Scharf.
    </t>
  </section>
  </middle>
  
  <back>
    <references>
      &RFC0793;
      &RFC3465;
      &RFC3484;
      &I-D.ietf-mptcp-multiaddressed;
      &I-D.ietf-mptcp-architecture;
      &I-D.ietf-mptcp-congestion;
      &I-D.ietf-mptcp-api;
      &I-D.ietf-mif-problem-statement;
      &I-D.tuexen-tsvwg-sctp-multipath;

      <reference anchor="Barre_Multipath"
		 target="http://inl.info.ucl.ac.be/publications/mptcp-nw">
	<front>
	  <title>Multipath TCP: From Theory to Practice</title>
	  <author initials="S." surname="Barré">
	    <organization>Université catholique de Louvain</organization>
	  </author>
	  <author initials="C." surname="Paasch">
	    <organization>Université catholique de Louvain</organization>
	  </author>
	  <author initials="O." surname="Bonaventure">
	    <organization>Université catholique de Louvain</organization>
	  </author>
	  <date month="May" year="2011"/>
	</front>
	<seriesInfo name="IFIP Networking,Valencia" value=""/>
	</reference>

      <reference anchor="Fisk_Dynamic"
		 target="http://woozle.org/~mfisk/papers/tcpwindow-lacsi.pdf">
	<front>
	  <title>Dynamic right-sizing in TCP</title>
	  <author initials="M." surname="Fisk">
	    <organization>Los Alamos National Laboratory</organization>
	  </author>
	  <author initials="W." surname="Feng">
	    <organization>Los Alamos National Laboratory</organization>
	  </author>
	  <date month="" year="2001"/>
	</front>
	<seriesInfo name="Los Alamos Computer Science Institute Symposium" value=""/>
	</reference>

      <reference anchor="Hsieh_ptcp"
		 target="http://www.ece.gatech.edu/research/GNAN/archive/2002/icnp02h.html">
	<front>
	  <title>pTCP: An End-to-End Transport Layer Protocol for Striped Connections</title>
	  <author initials="H." surname="Hsieh">
	    <organization>Los Alamos National Laboratory</organization>
	  </author>
	  <author initials="R." surname="Sivakumar">
	    <organization>Georgia Institue of Technology</organization>
	  </author>
	  <date month="" year="2002"/>
	</front>
	<seriesInfo name="ICNP" value=""/>
      </reference>

      <reference anchor="Watson_offload" 
		 target="http://www.informatics.sussex.ac.uk/research/projects/ngn/slides/msn10talks/watson-stack.pdf">
        <front>
          <title>Protocol stacks and multicore scalability
          </title>
          <author initials="R." surname="Watson">
            <organization>University of Cambridge</organization>
          </author>
          <date month="Jul" year="2010"/>
        </front>    
        <seriesInfo name="Presentation at Maastricht MPTCP workshop" 
                    value=""/>
      </reference>

      <reference anchor="VJ_prequeues" 
		 target="ftp://ftp.ee.lbl.gov/email/vanj.93sep07.txt">
        <front>
          <title>Re: query about tcp header on tcp-ip
          </title>
          <author initials="V." surname="Jacobson">
            <organization>PARC</organization>
          </author>
          <date month="Sep" year="1993"/>
        </front>            
      </reference>

    </references>
    <section title="Design alternatives">
      <t>
      In this appendix, we describe alternate designs that have been
      considered previously, and abandoned for various reasons
      (detailed as well). We keep them here for the archive and
      possible discussion. We also describe some potential designs 
      that have not been explored yet but could reveal to be better in
      the future, in which case that would be moved to the draft body.
      </t>
      <section title="Another way to consider Path Management"
	       anchor="app_other_pm">
	<t>
	In a previous implementation of MPTCP, it was proposed that
	the multipath transport had an even more abstract view of the
	paths in use
	than what is described in <xref target="sec_arch"/>. 
	In that design, the sub-sockets all shared the same
	tuple (saddr,sport,daddr,dport), and was disambiguated only by
	the path index. The advantage is that the Multipath Transport
	needs only to worry about how to efficiently spread data among
	multiple paths, without any knowledge about the addresses or
	ports used by each particular subflow. 
	</t>
	<t>
	  That design was particularly well suited for using Shim6 as a
	  Path Manager, because Shim6 is already designed to work in
	  the network layer and rewrite addresses. The first version
	  of the Linux MPTCP implementation was using Shim6 as path
	  manager. It looks also well suited to path managers that
	  don't use addresses (e.g. path managers that write a label
	  in the packet header, later interpreted by the
	  network). Finally, it removes the need for the token in the
	  multipath transport (connection identification is done
	  naturally with the tuple, shared by all subflows). The token
	  hence becomes specific to the built-in path manager, and can
	  be just ignored with other path managers (the context tag
	  plays a similar role in shim6, nothing is needed if the path
	  manager just sets labels to the packets).
	</t>
	<t>
	  However, this cleaner separation between Multipath Transport
	  and Path Management suffers from three drawbacks:
	  <list style="symbols">
	    <t>It requires a heavy modification to the existing
	    stacks, because it modifies the current way to identify
	    sockets in the stack. They are currently unambiguously
	    identified with the usual 5-tuple. This architecture would
	    require extending the 5-tuple with the path index, given
	    all subflows would share the same 5-tuple.
	    </t>
	    <t>
	      Although correctly implemented stacks could handle that
	      new endpoint identifier (5-tuple+path index), having
	      several flows with same 5-tuple could confuse
	      middleboxes.
	    </t>
	    <t>
	      When the path manager involves using several addresses,
	      forcing the same 5-tuple for all subflows at the
	      Multipath Transport level implies that the Path Manager
	      needs to rewrite the address fields of each packet. That
	      rewriting operation is simply avoided if the sockets
	      are bound to the addresses actually used to send the
	      packets. Hence, this alternate design would involve
	      avoidable costs for path managers that belong to the
	      "multi-address" category.
	    </t>
	  </list>
	</t>
      </section>
      <section title="Implementing alternate Path Managers"
	       anchor="app_pms">
	<t>
	  In <xref target="sec_arch"/>, the Path Manager is defined as
	  an entity that maintains a (path_index&lt;-&gt;endpoint_id) mapping.
	  This is enough in the case of the built-in path manager,
	  because the segments are associated to a path within the
	  socket itself, thanks to its endpoint_id. However, it is
	  expected that most other path managers will need to apply a
	  particular action, on a per-packet basis, to associate them
	  with a path. Example actions could be writing a number in a
	  field of the segment or choosing a different gateway than
	  the default one in the routing table. In an earlier version
	  of Linux MPTCP, based on a Shim6 Path Manager, the action
	  was used and consisted in rewriting the addresses of the
	  packets.
	</t>
	<t>
	  To reflect the need for a per-packet action, the PM mapping
	  table (an example of which is given in
	  <xref target="table_pm"/>) only needs to be extended with an
	  action field. As an example of this, we show hereafter an
	  example mapping table for a Path Manager based on writing the
	  path index into a field of the packets.	  
	</t>
	<texttable anchor="table_pm_ext" 
		   title="Example mapping table for a label-based PM">
	  <ttcol align="center">token</ttcol>
          <ttcol align="center">path index</ttcol>
	  <ttcol align="center">Endpoint id</ttcol>
	  <ttcol align="center">Action (Write x in DSCP)</ttcol>
	  <c>token_1</c>
	  <c>1</c>
	  <c>&lt;A1,B1,0,pB1&gt;</c>
	  <c>1</c>
	  
	  <c>token_1</c>
	  <c>2</c>
	  <c>&lt;A1,B1,0,pB1&gt;</c>
	  <c>2</c>
	  
	  <c>token_1</c>
	  <c>3</c>
	  <c>&lt;A1,B1,0,pB1&gt;</c>
	  <c>3</c>
	  
	  <c>token_1</c>
	  <c>4</c>
	  <c>&lt;A1,B1,0,pB1&gt;</c>
	  <c>4</c>
	  
	  <c></c>
	  <c></c>
	  <c></c>
	  <c></c>
	  
	  <c>token_2</c>
	  <c>1</c>
	  <c>&lt;A1,B1,0,pB2&gt;</c>
	  <c>1</c>
	  
	  <c>token_2</c>
	  <c>2</c>
	  <c>&lt;A1,B1,0,pB2&gt;</c>
	  <c>2</c>
	  
	</texttable>
      </section>
      <section title="When to instantiate a new meta-socket ?"
	       anchor="app_new_metasock">
	<t>
	The meta-socket is responsible only for MPTCP-related
	operations. This includes connection-level reordering for incoming
	data, scheduling for outgoing data, and subflow management. A
	natural choice then would be to instantiate a new meta-socket
	only when the peer has told us that it supports MPTCP. In the
	server it is naturally the case since the master subsocket is
	created upon the reception of a SYN+MP_CAPABLE.

	The client, however, instantiates its master subsocket when
	the application issues a socket() system call, but needs to
	wait until the SYN+ACK to know whether its peer supports
	MPTCP. Yet, it must already provide its token in the SYN.
	</t>
	<t>
	Linux MPTCP currently instantiates its client-side meta-socket
	when the master-socket is created (just like the
	server-side). The drawback of this is that if after socket(),
	the application subsequently issues a listen(), we have built
	a useless meta-socket. The same happens if the peer SYN+ACK
	does not carry the MP_CAPABLE option.
	To avoid that, one may want to instantiate the meta-socket
	upon reception of an MP_CAPABLE option. But this implies that
	the token (sent in the SYN), must be stored in some temporary
	place or in the master subsocket until the meta-socket is
	built.
	</t>
      </section>
      <section title="Forcing more processing in user context" 
	       anchor="app_recv_queue">
	<t>
	  The implementation architecture proposed in this draft uses
	the following queue configuration:
	<list style="symbols">
	  <t>
	    Subflow level: out-of-order queue. Used for subflow-level
	    reordering.
	  </t>
	  <t>
	    Connection level: out-of-order queue. Used for
	    connection-level reordering.
	  </t>
	  <t>
	    Connection level: receive queue. Used for storing the
	    ordered data until the application asks for it through a
	    recvmsg() system call or similar.
	  </t>
	</list>
	In a previous version of Linux MPTCP, another queue
	configuration has been examined:
	<list style="symbols">
	  <t>
	    Subflow level: out-of-order queue. Used for subflow-level
	    reordering.
	  </t>
	  <t>
	    Subflow level: receive queue. Used for storing the data
	    until the application asks for it through a
	    recvmsg() system call or similar.
	  </t>
	  <t>
	    Connection level: out-of-order queue. Used for
	    connection-level reordering.
	  </t>
	</list>
	In this alternate architecture, the connection-level data is
	lazily reordered as the application asks for it. The main goal
	for this was to ensure that as many CPU cycles as possible
	were spent in user context (See
	<xref target="sec_cpu_cycles"/>). VJ prequeues allow forcing
	user context processing when the application is waiting on a
	recv() system call. Otherwise the subflow-level reordering
	must be done in interrupt context. This remains true with
	MPTCP because the subflow-level implementation is left
	unmodified when possible. With MPTCP, the question is: "Where
	do we perform connection-level reordering ?". This alternate
	architecture answer is: "Do
	it <spanx style="emph">always</spanx> in user context". This
	was the strength of that architecture. Technically, the task
	of each subflow was to reorder its own segments and put them
	in their own receive queue, until the application asks for
	data. When the application wants to eat more data, MPTCP
	searches all subflow-level receive queue for the next bytes to
	receive, and reorder them as appropriate by using its own
	reordering queue. As soon as the number of requested bytes are
	handed to the application buffer, the MPTCP reordering task finishes.
	</t>
	<t>
	  Unfortunately, there are two major drawbacks about doing it
	  that way:
	  <list style="symbols">
	    <t>
	      The socket API supports the SO_RCVLOWAT option, which
	      allows an application to ask not being woken up until n
	      bytes have been received. Counting those bytes requires
	      reordering at least n bytes at the connection level in
	      interrupt context.
	    </t>
	    <t>
	      The
	      DATA_ACK <xref target="I-D.ietf-mptcp-multiaddressed"/>
	      should report the latest byte received in order at the
	      connection level. In this architecture, the best we can
	      do is report the latest byte that has been copied to the
	      application buffers, which would slightly change the
	      DATA_ACK semantic described in section 3.3.2
	      of  <xref target="I-D.ietf-mptcp-multiaddressed"/>. This
	      change could confuse peers that try to derive
	      information from the received DATA_ACK.
	    </t>
	  </list>
	</t>
      </section>
      <section title="Buffering data on a per-subflow basis" 
	       anchor="app_sep_send_queues">
	<t>
	In previous versions of Linux MPTCP, the configuration of the
	send queues was as shown in
	<xref target="fig_arch_sep_sndbuf"/>.
	</t>
	<figure align="center" anchor="fig_arch_sep_sndbuf"
		title="Send queue configuration">
          <artwork align="left"><![CDATA[
                               Application
                                   |
                                   v
                             Packet Scheduler
                                  /  \
                                 /    \
                                |      |
                                v      v
                              | * |  |   |
 Next segment to send (A)  -> | * |  | * |
                              |---|  |---|  <- Separate send queue
 Sent, but not acked (B)   -> |_*_|  |_*_|
                                |      |
                                v      v
                               NIC    NIC
            ]]></artwork>
          </figure>
	<t>
	  In contrast to the architecture presented in
	  <xref target="sec_send_queue"/>, there is no shared send
	  queue. The Packet Scheduler is run each time data is
	  produced by the application.
	  Compared to <xref target="fig_arch_sep_sndbuf"/>, 
	  the advantages and drawbacks
	  are basically reversed. Here are the advantages:
	  <list style="symbols">
	    <t> This architecture supports subflow-specific Maximum
	      Segment Sizes, because the subflow is selected before the
	      segment is built.
	    </t>
	    <t>
	      The segments are stored in their
	      final form in the subflow-specific send queues, and there is
	      no need to run the Packet Scheduler at transmission
	      time. The result is more fairness with other
	      applications (because the Packet Scheduler runs in user
	      context only), and faster data transmission when
	      acknowledgements open the congestion window (because
	      segments are buffered in their final form and no call
	      to the Packet Scheduler is needed.
	    </t>
	  </list>
	  The drawback, which motivated the architecture change in
	  Linux MPTCP is the complexity of the data allocation  (hence
	  the Packet Scheduler), and the computing cost involved.

	      Given that there is no shared send buffer, the send
	      buffer auto-tuning must be divided into its subflow
	      contributions. This buffer size can be easily derived
	      from <xref target="sec_sndbuf_tuning"/>. However,
	      when scheduling in advance a full send buffer of data,
	      we may be allocating a segment hundreds of milliseconds before it
	      actually goes to the wire. The task of the Packet
	      Scheduler is then complicated because it
	      must <spanx style="emph">predict</spanx> the path
	      properties. If the prediction is incorrect, two subflows
	      may try to put on the wire segments that are very
	      distant in terms of DATA_SEQ numbers. This can
	      eventually result in stalling some subflows, because the
	      DATA_SEQ gap between two subflows exceeds the receive
	      window announced by the receiver. The Packet Scheduler
	      can relatively easily compute a correct allocation of
	      segments if the path properties do not vary (just
	      because it is easy to predict a constant value), but the
	      implementation was very sensitive to variations in delay
	      or bandwidth. The previous implementation of Linux MPTCP
	      solved this allocation problem by verifying, upon each
	      failed transmission attempt, if it was blocked by the
	      receive window due to a gap in DATA_SEQ with other
	      subflows. If this was the case, a full reallocation of
	      segments was conducted. However, the cost of such a
	      reallocation is very high, because it involves
	      reconsidering the allocation of any single segment, and
	      do this for all the subflows. Worse, this costly
	      reallocation sometimes needed to happen in interrupt
	      context, which removed one of the advantages of this architecture.
	</t>
	<t>
	  Yet, under the assumption that the subflow-specific queue
	  size is small, the above drawback almost disappears. For
	  this reason the abandoned design described here could be
	  used to feed a future hybrid architecture, as explained in 
	  <xref target="sec_send_queue"/>.  For the sake of comparison
	  with <xref target="table_send_queues"/>, we provide
	  hereafter the action/table implemented by this architecture.
	</t>
	<texttable anchor="table_sep_send_queues" 
		   title="(event,action) pairs implemented in a
			  queue management based on separate send queues">
	  <ttcol align="left">event</ttcol>
          <ttcol align="left">action</ttcol>
	  <c>Segment acknowledged at subflow level</c>
	  <c>Remove references to it from the subflow-level queue</c>
	  
	  <c>Segment acknowledged at connection level</c>
	  <c>No queue-related action.</c>
	  
	  <c>Timeout (subflow-level)</c>
	  <c>Push the segment to the best subflow (according
	    to the Packet Scheduler). In contrast with the solution
	    of <xref target="sec_send_queue"/>, there is no need for
	    a connection-level retransmit queue, because there is no
	    requirement to be available immediately for a subflow 
	    to accept new data.</c>
	  <c>Ready to put new data on the wire 
	    (normally triggered by an incoming ack)</c>
	  <c>Just send the next segment from the A portion of the
	    subflow-specific send queue, if any. Note that the
	    "IMPORTANT" note from <xref target="sec_send_queue"/>
	    still applies with this architecture.</c>
	</texttable>		  	  
      </section>
      </section>
    <section title="Ongoing discussions on implementation improvements">
      <t>
      This appendix collects information on features that have been
      currently implemented nowhere, but can still be useful as hints
      for implementers to test. Feedback from implementers will help
      converging on those topics and propose solid guidelines for
      future versions of this memo.
      </t>
      <section title="Heuristics for subflow management"
	       anchor="app_subflow_mgmnt">
      <t>
	Some heuristic should determine when it would be
	beneficial to add a new subflow. Linux MPTCP has no such
	heuristic at the moment, but the topic has been discussed on
	the MPTCP mailing list, so this section summarizes the input
	from many individuals. MPTCP is not useful for very short
	flows, so three questions appear: 
	<list style="symbols">
	  <t>How long is a "too short flow"</t>
	  <t>How to predict that a flow will be short ?</t>
	  <t>When to decide to add/remove subflows ?</t>
	</list>
	To answer the third question, it has been proposed to use
	hints from the application. On the other hand the experience
	shows that socket options are quite often poorly or not used,
	which motivates the parallel use of a good default heuristic.
	This default heuristic may be influenced in particular by the
	particular set of options that are enabled for MPTCP (e.g. an
	administrator can decide that some security mechanisms for
	subflow initiation are not needed in his environment, and
	disable them, which would change the cost of establishing new 
	subflows).
	The following elements have been proposed to feed the
	heuristic, none of them tested yet:
	<list style="symbols">
	  <t>Check the size of the write
	    operations from the applications. Initiate a new subflow
	    if the write size exceeds some threshold. This information
	    can be taken only as a hint because applications could
	    send big chunks of data split in many small writes. 
	    A particular case of checking the size of write operations
	    is when the application uses the sendfile() system
	    call. In that situation MPTCP can know very precisely how
	    many bytes will be transferred.
	  </t>
	  <t>Check if the flow is network limited or application
	    limited. Initiate a new subflow only if it is network
	    limited.
	  </t>
	  <t>
	    It may be useful to establish new
	    subflows even for application-limited communications,
	    to provide failure survivability. A way to do that would
	    be to initiate a new subflow (if not done before by
	    another trigger) after some time has elapsed, regardless
	    of whether the communication is network or application limited.
	  </t>
	  <t>
	    Wait until slow start is done before to establish a new
	    subflow. Measurements with Linux MPTCP suggest that slow start
	    could be a reasonable tool for determining when it is
	    worth starting a new subflow (without increasing the
	    overall completion time). More analysis is needed in that
	    area, however. Also, this should be taken as a hint only
	    if the slow start is actually progressing (otherwise a
	    stalled subflow could prevent the establishment of another
	    one, precisely when a new one would be useful).
	  </t>
	  <t>
	    Use information from the application-layer protocol. Some
	    of them (e.g. HTTP) carry flow length information in their headers,
	    which can be used to decide how many subflows are useful.
	  </t>
	  <t>
	    Allow the administrator to configure subflow policies on a
	    per-port basis. The host stack could learn as well for
	    what ports MPTCP turns out to be useful.
	  </t>
	  <t>
	    Check the underlying medium of each potential subflow. For
	    example, if the initial subflow is initiated over 3G, and
	    WiFi is available, it probably makes sense to immediately
	    negotiate an additional subflow over WiFi.
	  </t>
	</list>

	It is not only useful to determine when to start new subflows,
	one should also sometimes decide to abandon some of its subflows.
	An MPTCP implementation should be able to
	determine when removing a subflow would increase the
	aggregate bandwidth. This can happen, for example, when the
	subflow has a significantly higher delay compared to other
	subflows, and the maximum buffer size allowed by the
	administrator has been reached (Linux MPTCP currently has
	no such heuristic yet).
      </t>
      </section>
    </section>
  </back>
</rfc>

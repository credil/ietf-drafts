<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?rfc toc="yes"?>
<?rfc tocompact="yes"?>
<?rfc tocdepth="3"?>
<?rfc tocindent="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<?rfc compact="yes"?>
<?rfc subcompact="no"?>
<rfc category="info" docName="draft-waehlisch-sam-common-api-00"
     ipr="trust200811">
  <front>
    <title abbrev="Common Mcast API">A Common API for Transparent Hybrid
    Multicast</title>

    <author fullname="Matthias Waehlisch" initials="M." surname="Waehlisch">
      <organization>link-lab &amp; FU Berlin</organization>

      <address>
        <postal>
          <street>Hoenower Str. 35</street>

          <city>Berlin</city>

          <code>10318</code>

          <country>Germany</country>
        </postal>

        <email>mw@link-lab.net</email>

        <uri>http://www.inf.fu-berlin.de/~waehl</uri>
      </address>
    </author>

    <author fullname="Thomas C. Schmidt" initials="TC." surname="Schmidt">
      <organization>HAW Hamburg</organization>

      <address>
        <postal>
          <street>Berliner Tor 7</street>

          <city>Hamburg</city>

          <code>20099</code>

          <country>Germany</country>
        </postal>

        <email>schmidt@informatik.haw-hamburg.de</email>

        <uri>http://inet.cpt.haw-hamburg.de/members/schmidt</uri>
      </address>
    </author>

    <date day="19" month="October" year="2009" />

    <abstract>
      <t>Group communication services are most efficiently implemented on the
      lowest layer available. This document describes a common multicast API
      that serves the requirements of data distribution and maintenance for
      multicast and broadcast on a middleware abstraction layer, suitable for
      transparent underlay and overlay communication. Additionally, it
      describes the application of this API in current Internet multicast
      routing protocols.</t>
    </abstract>

    <note title="Requirements Language">
      <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
      "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
      document are to be interpreted as described in <xref
      target="RFC2119">RFC 2119</xref>.</t>
    </note>
  </front>

  <middle>
    <section title="Introduction">
      <t>Group communication is implemented on different layers. To allow for
      a comprehensive development of applications and group services a common
      API is required, which provides calls to transmit and receive multicast
      data as well as a consistent view on multicast states. This document
      describes an abstract group communication API. A specific implementation
      description with respect to operating systems or programming languages
      is out-of-scope of this document.</t>

      <t>The aim of this API is twofold: An application programmer should be
      able to implement group-oriented data distribution independent of the
      underlying delivery mechanisms (e.g. native IP multicast or application
      layer multicast). Receivers require an interface to subscribe and leave
      a multicast group. Multicast sources send data to a group address. This
      group address relies on a specific namespace. The API should reflect the
      flexiblity in choosing an appropriate namespace.</t>

      <t>Additionally, the multicast API should provide internal interfaces to
      request current multicast states at the host. Multiple multicast
      protocols may run in parallel on a single host. These protocols may
      interact together.</t>
    </section>

    <section title="Terminology">
      <t><list style="hanging">
          <t hangText="Inter-domain Multicast Gateway:">An Inter-domain
          Multicast Gateway (IMG) transparently forwards data between IP layer
          and overlay multicast.</t>
        </list></t>
    </section>

    <section title="Overview">
      <t></t>

      <t>The API consists of up and down calls. The down calls can be
      implemented in a middleware that delegates those to overlay or
      underlay:</t>

      <t><figure>
          <artwork><![CDATA[*-------*     *-------*     
| App 1 |     | App 2 |
*-------*     *-------*
    |             |
*---------------------*
|   Middleware        |
*---------------------*
     |          |
*---------*     |
| Overlay |     |
*---------*     |
     |          |
     |          |
*---------------------*
|   Underlay          |
*---------------------*]]></artwork>
        </figure></t>
    </section>

    <section title="Hybrid Multicast API">
      <t></t>

      <section title="Abstract Data Types">
        <t><list style="hanging">
            <t hangText="Namespace">describes the domain-specific context in
            which the applications operate.</t>

            <t hangText="Address">is any kind of address in underlay (e.g.
            IPv4, IPv6) or overlay (e.g. SIP, hash-based ID).</t>

            <t hangText="Mode">denotes the layer on which the corresponding
            call will be effective. This may be unspecified to leave the
            decicision at the group communication stack.</t>
          </list></t>
      </section>

      <section title="Send/Receive Calls">
        <t><list style="hanging">
            <t hangText="init(in Namespace n)">This call is implemented</t>

            <t hangText="join(in Address a, in Mode m)">This operation
            initiates a group subscription. Depending on the mode, this may
            result in an IGMP/MLD report.</t>

            <t hangText="leave(in Address a, in Mode m)">This operation
            results in an unsubscription for the given address.</t>

            <t hangText="send(in Address a, in Mode m, out Message msg)"></t>

            <t
            hangText="receive(in Address a, in Mode m, out Message msg)"></t>
          </list></t>
      </section>

      <section title="Service Calls">
        <t><list style="hanging">
            <t hangText="groupSet(out Address[] g, in Mode m)">This operation
            returns all registered multicast groups. The information can be
            provided by group management or routing protocols. The return
            values distinguish between sender and listener states.</t>

            <t hangText="neighborSet(out Address[] a, in Mode m)">This
            function can be invoked to get the set of multicast routing
            neighbors.</t>

            <t hangText="designatedHost(out Bool b, in Address a)">This
            function returns true, if the host has the role of a designated
            forwarder or querier. Such an information is provided by almost
            all multicast protocols to handle packet duplication, if multiple
            multicast instances serve on the same subnet.</t>

            <t hangText="updateListener(out Address g, in Mode m)">This upcall
            is invoked to inform a group service about a change of listener
            states for a group. This is the result of receiver new
            subscriptions or leaves. The group service may call groupSet to
            get updated information.</t>

            <t hangText="updateSender(out Address g, in Mode m)">This upcall
            should be implemented to inform the application about source state
            changes. Analog to the updateListener case, the group service may
            call thereupon groupSet.</t>
          </list></t>
      </section>
    </section>

    <section title="Deployment Use Cases">
      <t>This section describes the application of the defined API to
      implement an IMG.</t>

      <section title="DVMRP">
        <t>An arbitrary DVMRP <xref target="RFC1075"></xref> router will not
        be informed about new receivers, but will learn about new sources
        immediately. The concept of DVMRP does not provide any central
        multicast instance. Thus, the IMG can be placed anywhere inside the
        multicast region, but requires a DVMRP neighbor connectivity. The
        group communication stack used by the IMG is enhanced by a DVMRP
        implementation. New sources in the underlay will be advertised based
        on the DVMRP flooding mechanism and received by the IMG. Based on this
        the updateSender() call is triggered. The relay agent initiates a
        corresponding join in the native network and forwards the received
        source data towards the overlay routing protocol. Depending on the
        group states, the data will be distributed to overlay peers.</t>

        <t>DVMRP establishes source specific multicast trees. Therefore, a
        graft message is only visible for DVMRP routers on the path from the
        new receiver subnet to the source, but in general not for an IMG. To
        overcome this problem, data of multicast senders will be flooded in
        the overlay as well as in the underlay. Hence, an IMG has to initiate
        an all-group join to the overlay using the namespace extension of the
        API. Each IMG is initially required to forward the received overlay
        data to the underlay, independent of native multicast receivers.
        Subsequent prunes may limit unwanted data distribution thereafter.</t>
      </section>

      <section title="PIM-SM">
        <t>The Protocol Independent Multicast Sparse Mode (PIM-SM) <xref
        target="RFC4601"></xref> establishes rendezvous points (RP). These
        entities receive listener and source subscriptions of a domain. To be
        continuously updated, an IMG has to be co-located with a RP. Whenever
        PIM register messages are received, the IMG must signal internally a
        new multicast source using updateSender(). Subsequently, the IMG joins
        the group and a shared tree between the RP and the sources will be
        established, which may change to a source specific tree after a
        sufficient number of data has been delivered. Source traffic will be
        forwarded to the RP based on the IMG join, even if there are no
        further receivers in the native multicast domain. Designated routers
        of a PIM-domain send receiver subscriptions towards the PIM-SM RP. The
        reception of such messages invokes the updateListener() call at the
        IMG, which initiates a join towards the overlay routing protocol.
        Overlay multicast data arriving at the IMG will then transparently be
        forwarded in the underlay network and distributed through the RP
        instance.</t>
      </section>

      <section title="PIM-SSM">
        <t>PIM Source Specific Multicast (PIM-SSM) is defined as part of
        PIM-SM and admits source specific joins (S,G) according to the source
        specific host group model <xref target="RFC4604"></xref>. A multicast
        distribution tree can be established without the assistance of a
        rendezvous point.</t>

        <t>Sources are not advertised within a PIM-SSM domain. Consequently,
        an IMG cannot anticipate the local join inside a sender domain and
        deliver a priori the multicast data to the overlay instance. If an IMG
        of a receiver domain initiates a group subscription via the overlay
        routing protocol, relaying multicast data fails, as data are not
        available at the overlay instance. The IMG instance of the receiver
        domain, thus, has to locate the IMG instance of the source domain to
        trigger the corresponding join. In the sense of PIM-SSM, the signaling
        should not be flooded in underlay and overlay.</t>

        <t>One solution could be to intercept the subscription at both, source
        and receiver sites: To monitor multicast receiver subscriptions
        (updateListener()) in the underlay, the IMG is placed on path towards
        the source, e.g., at a domain border router. This router intercepts
        join messages and extracts the unicast source address S, initializing
        an IMG specific join to S via regular unicast. Multicast data arriving
        at the IMG of the sender domain can be distributed via the overlay.
        Discovering the IMG of a multicast sender domain may be implemented
        analogously to AMT <xref
        target="I-D.ietf-mboned-auto-multicast"></xref> by anycast.
        Consequently, the source address S of the group (S,G) should be built
        based on an anycast prefix. The corresponding IMG anycast address for
        a source domain is then derived from the prefix of S.</t>
      </section>

      <section title="BIDIR-PIM">
        <t>Bidirectional PIM <xref target="RFC5015"></xref> is a variant of
        PIM-SM. In contrast to PIM-SM, the protocol pre-establishes
        bidirectional shared trees per group, connecting multicast sources and
        receivers. The rendezvous points are virtualized in BIDIR-PIM as an
        address to identify on-tree directions (up and down). However, routers
        with the best link towards the (virtualized) rendezvous point address
        are selected as designated forwarders for a link-local domain and
        represent the actual distribution tree. The IMG is to be placed at the
        RP-link, where the rendezvous point address is located. As source data
        in either cases will be transmitted to the rendezvous point address,
        the BIDIR-PIM instance of the IMG receives the data and can internally
        signal new senders towards the stack via updateSender(). The first
        receiver subscription for a new group within a BIDIR-PIM domain needs
        to be transmitted to the RP to establish the first branching point.
        Using the updateListener() invocation, an IMG will thereby be informed
        about group requests from its domain, which are then delegated to the
        overlay.</t>
      </section>
    </section>

    <section anchor="IANA" title="IANA Considerations">
      <t>This document makes no request of IANA.</t>
    </section>

    <section anchor="Security" title="Security Considerations">
      <t>This draft does neither introduce additional messages nor novel
      protocol operations. TODO</t>
    </section>

    <section anchor="Acknowledgements" title="Acknowledgements">
      <t>TODO</t>
    </section>
  </middle>

  <back>
    <references title="Normative References">
      <?rfc include="reference.RFC.2119"?>

      <?rfc include="reference.RFC.1075"?>

      <?rfc include="reference.RFC.5015"?>

      <?rfc include="reference.RFC.3810"?>

      <?rfc include="reference.RFC.4604"?>

      <?rfc include="reference.RFC.2710"?>

      <?rfc include="reference.RFC.4601"?>

      <?rfc include="reference.RFC.3376"?>
    </references>

    <references title="Informative References">
      <?rfc include="reference.I-D.ietf-mboned-auto-multicast"?>
    </references>
  </back>
</rfc>
